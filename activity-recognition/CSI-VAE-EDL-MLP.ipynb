{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"]= '1' # Use legacy keras for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTENNAS = 1\n",
    "antenna = 0  # if ANTENNAS==1, this value selects the antenna ID (from 0 to 3)\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "latent_dim = 2\n",
    "num_activities = 5\n",
    "folder_name = f'models/single_antenna_{antenna}'\n",
    "\n",
    "base_directory = './models'\n",
    "os.makedirs(base_directory, exist_ok=True)\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "os.makedirs('./results_csv', exist_ok=True)\n",
    "os.makedirs('./results_graphs', exist_ok=True)\n",
    "\n",
    "saveGraph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state) # predictable random numbers, for demonstration only\n",
    "tf.random.set_seed(random_state) # reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # make operations deterministic\n",
    "os.environ['PYTHONHASHSEED'] = str(random_state) # reproducibility\n",
    "\n",
    "# computes golden ratio for figures\n",
    "def goldenrect(h):\n",
    "    return (h * 1.618, h)\n",
    "\n",
    "def summary_clf(y_test, predicted, y_score, _labels = None):\n",
    "    print(classification_report(y_test, predicted, labels= _labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSI data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsiData(tf_keras.utils.Sequence):\n",
    "    def __init__(self, csi, labels, indices, batch_size=25, window_size=450, antennas=1):\n",
    "        self.csi = csi\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.antennas = antennas\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.indices.shape[-1] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        first_idx = batch_idx * self.batch_size\n",
    "        last_idx = (batch_idx + 1) * self.batch_size\n",
    "        \n",
    "        data_batch = [self.csi[x:x + self.window_size, ...] for x in range(first_idx, last_idx)]\n",
    "        labels_batch = np.transpose([self.labels[first_idx:last_idx]])\n",
    "\n",
    "        data_batch = tf.convert_to_tensor(data_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "\n",
    "        if self.antennas == 1:\n",
    "            data_batch = tf.expand_dims(data_batch, 3)\n",
    "            labels_batch = tf.expand_dims(labels_batch, 2)\n",
    "\n",
    "        return data_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test_CSI_data(file_list, num_samples=12000, window_size=450, batch_size=25, antennas=1, random_state=42, verbose=False):\n",
    "    csi_per_sec = 150\n",
    "    windows_per_activity = 5\n",
    "    train_seconds = 9\n",
    "    test_seconds = 1\n",
    "    ignore_seconds = 3\n",
    "\n",
    "    if antennas == 1:\n",
    "        train_data = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "        test_data = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "    else:\n",
    "        train_data = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "        test_data = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "\n",
    "    train_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    test_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    train_indices = tf.zeros([0], dtype=tf.int32)\n",
    "    test_indices = tf.zeros([0], dtype=tf.int32)\n",
    "\n",
    "    for file in file_list:\n",
    "        if verbose: print(f\"\\n*************** activity {file_list.index(file)} ***************\")\n",
    "        # Load CSI data from MATLAB file\n",
    "        mat = sio.loadmat(file)      # WARNING This code does not handle exceptions for simplicity...\n",
    "        data = np.array(mat['csi'])  # ...exceptions would require keeping track of indices\n",
    "        if antennas == 1:\n",
    "            data = data[range(num_samples), ..., int(antenna)]\n",
    "        data = np.round(np.abs(data))\n",
    "        train_index_offset = train_data.shape[0]\n",
    "        test_index_offset = test_data.shape[0]\n",
    "        activity_label = file_list.index(file)  # Labels depend on file index \n",
    "\n",
    "        n_samples_activity = data.shape[0]//windows_per_activity\n",
    "\n",
    "        tmp_train_data = np.zeros((0, 2048))\n",
    "        tmp_test_data = np.zeros((0, 2048))\n",
    "\n",
    "        for iter in range(windows_per_activity):\n",
    "            if verbose: print(f\"\\n-------- {iter} ---------\")\n",
    "            start_train_idx = 0 + (n_samples_activity) * iter\n",
    "            end_train_idx = start_train_idx + csi_per_sec * train_seconds\n",
    "            if verbose: print(f\"train idx {start_train_idx} to {end_train_idx}\")\n",
    "            tmp_train_data = np.append(tmp_train_data, data[start_train_idx : end_train_idx], axis=0)\n",
    "            \n",
    "            if verbose: print(\"train shape\", tmp_train_data.shape)\n",
    "\n",
    "            start_test_idx = end_train_idx + csi_per_sec * test_seconds\n",
    "            end_test_idx =  start_test_idx + csi_per_sec * ignore_seconds\n",
    "            if verbose: print(f\"ignoring idx from {end_train_idx} to {start_test_idx}\")\n",
    "\n",
    "            if verbose: print(f\"test idx {start_test_idx} to {end_test_idx}\")\n",
    "            tmp_test_data = np.append(tmp_test_data, data[start_test_idx : end_test_idx], axis=0)\n",
    "            \n",
    "            if verbose: print(\"test shape\", tmp_test_data.shape)\n",
    "\n",
    "        train_num_samples = tmp_train_data.shape[0]\n",
    "        tmp_train_data = tf.convert_to_tensor(tmp_train_data, dtype=tf.float32)\n",
    "        tmp_train_labels = tf.convert_to_tensor(activity_label * np.ones(train_num_samples - window_size), dtype=tf.int32)\n",
    "        tmp_train_indices = tf.convert_to_tensor(tf.range(train_index_offset, train_index_offset + train_num_samples - window_size), dtype=tf.int32)\n",
    "\n",
    "        test_num_samples = tmp_test_data.shape[0]\n",
    "        tmp_test_data = tf.convert_to_tensor(tmp_test_data, dtype=tf.float32)\n",
    "        tmp_test_labels = tf.convert_to_tensor(activity_label * np.ones(test_num_samples - window_size), dtype=tf.int32)\n",
    "        tmp_test_indices = tf.convert_to_tensor(tf.range(test_index_offset, test_index_offset + test_num_samples - window_size), dtype=tf.int32)\n",
    "\n",
    "        train_data = tf.concat([train_data, tmp_train_data], axis=0)\n",
    "        train_labels = tf.concat([train_labels, tmp_train_labels], axis=0)\n",
    "        train_indices = tf.concat([train_indices, tmp_train_indices], axis=0)\n",
    "\n",
    "        test_data = tf.concat([test_data, tmp_test_data], axis=0)\n",
    "        test_labels = tf.concat([test_labels, tmp_test_labels], axis=0)\n",
    "        test_indices = tf.concat([test_indices, tmp_test_indices], axis=0)\n",
    "\n",
    "        if verbose: print(train_data.shape, train_labels.shape)\n",
    "        if verbose: print(test_data.shape, test_labels.shape)\n",
    "\n",
    "    # Normalize the CSI dataset\n",
    "    if antennas == 1:\n",
    "        train_data = tf.math.divide(train_data, tf.math.reduce_max(train_data, axis=(0, 1)))\n",
    "        test_data = tf.math.divide(test_data, tf.math.reduce_max(test_data, axis=(0, 1)))\n",
    "    else:\n",
    "        train_data = tf.math.divide(train_data, tf.math.reduce_max(train_data, axis=(0, 1, 2)))\n",
    "        test_data = tf.math.divide(test_data, tf.math.reduce_max(test_data, axis=(0, 1, 2)))\n",
    "\n",
    "    train_data = CsiData(train_data, train_labels, train_indices, batch_size=batch_size, window_size=window_size, antennas=antennas)\n",
    "    test_data = CsiData(test_data, test_labels, test_indices, batch_size=batch_size, window_size=window_size, antennas=antennas)\n",
    "\n",
    "    return train_data, test_data\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf_keras.layers.Layer):\n",
    "    \"\"\"Takes a couple (z_mean, z_log_var) to draw a sample z from the latent space.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(encoder_inputs)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(x)\n",
    "    x = tf_keras.layers.Conv2D(32, (2, 4), activation='relu', strides=(2, 4), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (2, 4), activation='relu', strides=(2, 4), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class Sampling(tf_keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(encoder_inputs)\n",
    "    x = tf_keras.layers.BatchNormalization()(x)\n",
    "    x = tf_keras.layers.Conv2D(64, (5, 8), activation='relu', strides=(5, 8), padding='valid')(x)\n",
    "    x = tf_keras.layers.BatchNormalization()(x)\n",
    "    x = tf_keras.layers.Conv2D(128, (2, 4), activation='relu', strides=(2, 4), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf_keras.layers.Dropout(0.3)(x)  # Dropout added to regularize\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(128, (2, 4), activation='relu', strides=(2, 4), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(64, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf_keras.Model):\n",
    "    def __init__(self, enc_input_shape=(450, 2048, 1), dec_input_shape=(9, 8, 32), latent_dim=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_csi_encoder(enc_input_shape, latent_dim)\n",
    "        self.decoder = create_csi_decoder(dec_input_shape, latent_dim, enc_input_shape[-1])\n",
    "        self.total_loss_tracker = tf_keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = tf_keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = tf_keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf_keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vae_encoder(vae, source):\n",
    "    #Use the VAE to process CSI data\n",
    "    z_data = np.zeros([0, 4])\n",
    "    z_labels = np.zeros([0])\n",
    "\n",
    "    for (data, labels) in source:\n",
    "        labels = tf.squeeze(labels)\n",
    "        z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "        z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "        z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "        z_labels = np.concatenate([z_labels, labels.numpy().ravel()], axis=0)\n",
    "        \n",
    "    return z_data, z_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./{folder_name}/' + 'cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = tf_keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)\n",
    "early_stopping_cb = tf_keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "csv_logger_cb = tf_keras.callbacks.CSVLogger(f'./{folder_name}/model_history_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components(data, target, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    #Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "\n",
    "    var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "    #finds PCs that explain 95% of the variance\n",
    "    num_components = np.argmax(var_cumulative > target) + 1\n",
    "    print(f\"Number of components explaining {target}% variance: \"+ str(num_components))\n",
    "\n",
    "    if plotGraph:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title('Cumulative Explained Variance explained by the components')\n",
    "        plt.ylabel('Cumulative Explained variance')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.axvline(x=num_components, color=\"r\", linestyle=\"--\")\n",
    "        plt.axhline(y=target, color=\"r\", linestyle=\"--\")\n",
    "        plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', linestyle='--')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'var_cumulative_x_component.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "\n",
    "    return num_components\n",
    "\n",
    "def analyze_PCA(data, n_components, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    if (plotGraph):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "        plt.title('Cumulative Explained Variance by PCA Components')\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'cumulative_explained_variance.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "    \n",
    "    return reduced_df, pca\n",
    "\n",
    "def reconstruct_data(df, pca, columns):\n",
    "    df_reconstructed = pca.inverse_transform(df.values)\n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=columns)    \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    clusters = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (clusters[:-1] + clusters[1:]) / 2 #Defines intervals of clusters\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
    "        \n",
    "        empty_clusters = np.isnan(new_clusters) #Restore previous cluster if empty\n",
    "        new_clusters[empty_clusters] = clusters[empty_clusters] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_clusters - clusters)) < delta:\n",
    "            break\n",
    "\n",
    "        clusters = new_clusters\n",
    "\n",
    "    #Quantize the data based on the final clusters\n",
    "    quantized_data = clusters[indices]\n",
    "\n",
    "    return quantized_data, clusters, thresholds\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, clusters, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return clusters[indices]\n",
    "\n",
    "def apply_quantization(reduced_df, lvls):\n",
    "    df_quantized = reduced_df.apply(lambda col: lloyd_max_quantization(col.values, num_levels=lvls)[0])\n",
    "    return df_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bits_needed(source, verbose=True):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    bits_needed_window = {}\n",
    "    total_bits = 0\n",
    "    total_symbols = 0\n",
    "    \n",
    "    for index in range(0, len(data), window_size):\n",
    "        bits_needed = {}\n",
    "        data_window = data.iloc[index : index + window_size] \n",
    "        window_total_bits = 0\n",
    "        window_total_symbols = 0\n",
    "        \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            total_num_symbols = len(data_window[col])\n",
    "            \n",
    "            if num_symbols > 1:\n",
    "                bits_needed[col] = np.ceil(np.log2(num_symbols)).astype(int)  # Number of bits to represent each symbol\n",
    "            else:\n",
    "                bits_needed[col] = 1  # If only one unique symbol\n",
    "            if verbose: print(f\"Column: {col}, Bits needed: {bits_needed[col]} bits\")\n",
    "            \n",
    "            # bits this column in the window\n",
    "            column_bits = bits_needed[col] * total_num_symbols\n",
    "            window_total_bits += column_bits\n",
    "            window_total_symbols += total_num_symbols\n",
    "\n",
    "        bits_needed_window[index] = window_total_bits\n",
    "        if verbose: print(f\"Window: {index}, Average bits needed: {window_total_bits:.2f} bits\")\n",
    "    \n",
    "        total_bits += window_total_bits\n",
    "        total_symbols += window_total_symbols\n",
    "\n",
    "    average_bits_per_symbol = total_bits / total_symbols if total_symbols > 0 else 0\n",
    "    average_bits_per_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    #print(f\"\\nGlobal metrics:\")\n",
    "    print(f\"Average bits per symbol: {average_bits_per_symbol:.2f} bits\")\n",
    "    print(f\"Average bits per window: {average_bits_per_window:.2f} bits\")\n",
    "    print(f\"Bits for the whole dataset: {total_bits:.2f} bits\")\n",
    "\n",
    "    return average_bits_per_symbol.round(2), average_bits_per_window, total_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_needed(source, num_lvls=-1):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    num_features = len(data.columns)\n",
    "    bits_needed_unique = {}\n",
    "    avg_bits_needed = {}\n",
    "    bits_needed_window = {}\n",
    "    total_bits_needed_dataset = 0\n",
    "\n",
    "    for index in range(0, len(data), window_size):\n",
    "        data_window = data.iloc[index : index + window_size] \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            if num_lvls > 0:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_lvls)).astype(int)\n",
    "                #print(f\"Column: {col}, Bits needed: {bits_needed_unique[col]} bits (num levels: {num_lvls})\")\n",
    "            else:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_symbols)).astype(int)\n",
    "                \n",
    "        avg_bits_needed[index] = np.mean(list(bits_needed_unique.values())).round(2)\n",
    "        bits_needed_window[index] = sum(bits_needed_unique.values())\n",
    "        total_bits_needed_dataset += sum(bits_needed_unique.values())\n",
    "\n",
    "    bits_needed = np.mean(list(avg_bits_needed.values())).round(2)\n",
    "    bits_needed_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    return bits_needed, bits_needed_window, total_bits_needed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "The VAE has been trained without any information about the target classes; it just tries to minimize reconstruction loss + KL loss.\n",
    "\n",
    "The Encoder in the VAE maps sequences of CSI into **2 Gaussian variables** with parameters (z_mean, z_log_var).\n",
    "\n",
    "More in detail, from the dataset we load `data` and `labels`.\n",
    "- `data`: every element is a 4-tuple with the values (z1_mean, z2_mean, z1_log_var, z2_log_var)\n",
    "- `labels`: 5 different classes, labelled with integers from 0 to 4 (0 = walk, 1 = run, 2 = jump, 3 = sit, 4 = empty)\n",
    "\n",
    "Available datasets:\n",
    "- `single_antenna`: data of just antenna 1, normalized wrt to the maximum value over the entire dataset (four antennas are available, numbered from 0 to 3)\n",
    "- `four_antennas`: data of the four antennas fused together, normalized wrt to the maximum value over the entire dataset\n",
    "- `four_antennas_latent_space_3`: same as `four_antennas`, but the CSI is mapped onto 3 Gaussian variables; hence, every element in `data` is a 6-tuple with the values (z1_mean, z2_mean, z3_mean, z1_log_var, z2_log_var, z3_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_classes = [\"Walk\", \"Run\", \"Jump\", \"Sit\", \"Empty\"]\n",
    "#base_directory = './NEW_results'\n",
    "base_directory = './results'\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(directory, scaler=None):\n",
    "    data = None\n",
    "    labels = None\n",
    "\n",
    "    # check which experiments we wants to load\n",
    "    with open(directory, 'rb') as f:\n",
    "        data, labels = pickle.load(f)\n",
    "    \n",
    "    # features columns\n",
    "    fcolumns = ['mu1','mu2','sigma1','sigma2']\n",
    "\n",
    "    # labels are categoricals\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    \n",
    "    # let's load into a dataframe\n",
    "    df = pd.DataFrame(data, columns=fcolumns)\n",
    "    df['signal'] = labels\n",
    "    \n",
    "    if scaler is None:\n",
    "        # Fit scaler on training data\n",
    "        scaler = StandardScaler().fit(df[fcolumns])\n",
    "    df[fcolumns] = scaler.transform(df[fcolumns])\n",
    "    \n",
    "    X = df[fcolumns]\n",
    "    y = df['signal']\n",
    "\n",
    "    # one-hot-encoding\n",
    "    y_dummy = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, y_dummy, scaler, fcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_annealing = 1\n",
    "num_classes = 5\n",
    "\n",
    "ep = 1.0\n",
    "class GetEpochs(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global ep\n",
    "        ep += 1\n",
    "\n",
    "def res_to_mean(ev, dim = 5):\n",
    "    return np.max(dirichlet.mean(ev.reshape(dim,)+1))\n",
    "\n",
    "def res_to_dirichlet(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    S = np.sum(alpha)\n",
    "    K = 2.0\n",
    "    return dirichlet.mean(alpha), K/S\n",
    "\n",
    "def edl_accuracy(yTrue, yPred):\n",
    "    pred = K.argmax(yPred, axis=1)\n",
    "    truth = K.argmax(yTrue, axis=1)\n",
    "    match = K.reshape(K.cast(K.equal(pred, truth), \"float32\"),(-1,1))\n",
    "    return K.mean(match)\n",
    "\n",
    "def load_edl_experiment(name):\n",
    "    keras.models.load_model(name)\n",
    "\n",
    "def plot_res_beta(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    plt.figure(figsize=(16,9))\n",
    "    x = np.linspace(0,1,1000)\n",
    "    plt.plot(x, beta.pdf(x, alpha[1], alpha[0]))\n",
    "    x1, x2 = beta.interval(0.95, alpha[1], alpha[0])\n",
    "    areaplot = np.multiply(beta.pdf(x, alpha[1],alpha[0]), rect(x,x1, x2))\n",
    "    plt.fill_between(x, 0, areaplot, alpha=0.5)\n",
    "\n",
    "def results_test (train_dir, test_dir, num_components=0, num_levels=0, default=False):\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dir, scaler)\n",
    "    if default:\n",
    "        model_directory = os.path.join(base_directory, f'0_components/models/0components_0lvls_Keras_Model.keras')\n",
    "    else:\n",
    "        model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    \n",
    "    mlp_edl = keras.models.load_model(model_directory, compile=False)\n",
    "    mlp_edl_scores = np.array([res_to_mean(r, dim=5) for r in mlp_edl.predict(X_test)])\n",
    "    y_predictions_edl = np.array(tf.argmax(mlp_edl.predict(X_test), axis=1))\n",
    "\n",
    "    print(summary_clf(y_test, y_predictions_edl, mlp_edl_scores))\n",
    "    accuracy = accuracy_score(y_test, y_predictions_edl)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predictions_edl)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=semantic_classes)\n",
    "    cmdisp = disp.plot(cmap=\"cividis\")\n",
    "    #CM_directory = os.path.join(base_directory, f'{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    CM_directory = os.path.join(base_directory, f'PCA_ONLY/{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    #CM_directory = os.path.join(base_directory, f'{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    os.makedirs(os.path.dirname(CM_directory), exist_ok=True)\n",
    "    cmdisp.figure_.savefig(CM_directory, bbox_inches='tight')\n",
    "\n",
    "    return round(accuracy, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edl_experiment(name, _X_train, _y_train_dummy, num_components=0, num_levels=0):\n",
    "\n",
    "    model_edl = None\n",
    "    num_classes = 5\n",
    "    \n",
    "    if name == \"Delayed-Fusing\":\n",
    "        num_epochs_annealing = 3\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(16,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5, activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.001\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing3\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(6,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 64\n",
    "        lr = 0.001\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu', input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.5))\n",
    "        model_edl.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.5))\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(5, activation='softplus'))\n",
    "        \"\"\"\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(4, activation='relu', kernel_initializer=keras.initializers.GlorotUniform(seed=random_state), input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.GlorotUniform(seed=random_state)))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    def KL(alpha):\n",
    "        beta=K.constant(np.ones((1,num_classes)),dtype=\"float32\")\n",
    "        S_alpha = K.sum(alpha,axis=1,keepdims=True)\n",
    "        S_beta = K.sum(beta,axis=1,keepdims=True)\n",
    "        lnB = tf.math.lgamma(S_alpha) - K.sum(tf.math.lgamma(alpha),axis=1,keepdims=True)\n",
    "        lnB_uni = K.sum(tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n",
    "\n",
    "        dg0 = tf.math.digamma(S_alpha)\n",
    "        dg1 = tf.math.digamma(alpha)\n",
    "\n",
    "        return K.sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "\n",
    "    # Loss function considering the expected squared error and the KL divergence\n",
    "    def mse_loss(yTrue,yPred):\n",
    "        alpha = yPred + 1\n",
    "        S = K.sum(alpha, axis=1, keepdims=True)\n",
    "        m = alpha / S\n",
    "\n",
    "        # A + B minimises the sum of squared loss, see discussion in EDL paper for the derivation\n",
    "        A = K.sum((yTrue-m)**2, axis=1, keepdims=True)\n",
    "        B = K.sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
    "\n",
    "        # the lambda_t parameter, in this case min{1, t/10} with t the number of epochs\n",
    "        ll = min(1.0, float(ep/float(num_epochs_annealing)))\n",
    "        \n",
    "        alp = yPred*(1-yTrue) + 1 \n",
    "        C =  ll * KL(alp)\n",
    "\n",
    "        return A + B + C\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model_edl.compile(loss=mse_loss, optimizer=optimizer, metrics=[edl_accuracy])\n",
    "\n",
    "    model_edl.fit(_X_train, _y_train_dummy,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    os.makedirs(os.path.dirname(model_directory), exist_ok=True)\n",
    "    model_edl.save(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS, verbose=False)\n",
    "\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_models = True\n",
    "\n",
    "if load_pretrained_models:\n",
    "    print('Loading pretrained models...')\n",
    "else:\n",
    "    # Train from scratch\n",
    "    vae = VAE()\n",
    "    vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "    vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "    vae.fit(train_data, epochs=20, shuffle=True, callbacks=[checkpoint_cb, csv_logger_cb])\n",
    "    vae.save_weights(f'./{folder_name}/train_weights_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"-------------- 0 components --------------\")\n",
    "directory = './dumps/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "print(\"Encoding train data...\")\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "\n",
    "print(\"Encoding test data...\")\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_train, z_labels_train], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_test, z_labels_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler=scaler)\n",
    "\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "directory = './dumps/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir, default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Output Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VAE to process CSI data\n",
    "z_data = np.zeros([0, 4])\n",
    "z_labels = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/VAE_QNTZD/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "bit_results = []\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "\n",
    "df_z_data_train = pd.DataFrame(z_data_train, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "df_z_data_test = pd.DataFrame(z_data_test, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "\n",
    "for lvl in levels:\n",
    "    print(f\"-------------- {lvl} lvls --------------\")\n",
    "    df_train_quantized = apply_quantization(df_z_data_train, lvl)\n",
    "    df_test_quantized = apply_quantization(df_z_data_test, lvl)\n",
    "    print (f\"DF_QUANTIZED\")\n",
    "        #QT_avg_bits_per_symbol, QT_avg_bits_per_window, QT_total_bits = compute_bits_needed(df_quantized, verbose=False)\n",
    "    QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_test_quantized, lvl)\n",
    "    print(f\"Bits needed: {QT_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "    z_data_train = df_train_quantized.to_numpy()\n",
    "    z_data_test = df_test_quantized.to_numpy()\n",
    "\n",
    "    sub_dir=os.path.join(directory, f'training/{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "    os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "    with open(sub_dir, 'wb') as f:\n",
    "       pickle.dump([z_data_train, z_labels_train], f)\n",
    "\n",
    "    sub_dir=os.path.join(directory, f'test/{lvl}lvls_single_antenna_{antenna}_test.pkl')\n",
    "    os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "    with open(sub_dir, 'wb') as f:\n",
    "       pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "    bit_results.append({\n",
    "            'num_levels': lvl,\n",
    "            'QT_bits': QT_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/VAE_bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/VAE_QNTZD/0_components'\n",
    "for num_levels in levels:  \n",
    "    print(f\"-------------- {num_levels} lvls --------------\")\n",
    "    filename = f'{lvl}lvls_single_antenna_{antenna}'\n",
    "    train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler)\n",
    "\n",
    "    name = \"No-Fused-1\"\n",
    "    run_edl_experiment(name, X_train, y_train_dummy, num_components, num_levels)\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_components, num_levels)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": num_levels,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.title('Accuracy and Bits Comparison (quantized data post VAE)')\n",
    "plt.axhline(y=0.68, color=\"r\", linestyle=\":\", label=\"max accuracy\")\n",
    "plt.xlabel('Bits per symbol')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/accuracy_bit_POST_VAE[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Comprehenisve Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        #Quantize the data\n",
    "        df_train_quantized = apply_quantization(df_train_reduced, num_levels)\n",
    "        df_test_quantized = apply_quantization(df_test_reduced, num_levels)\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "        train_data.csi = reconstructed_train_data\n",
    "\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "        vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "        \n",
    "        print(\"Encoding train data...\")\n",
    "        z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "        \n",
    "        print(\"Encoding test data...\")\n",
    "        z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "        train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "        os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "        os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "        print(\"Saving data...\")\n",
    "        with open(train_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_train, z_labels_train], f)\n",
    "        with open(test_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/{num_components}_components'\n",
    "    for num_levels in levels:  \n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        train_dump_dir = './dumps/single_antenna_0.pkl'\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "        # Test model\n",
    "        accuracy = results_test(train_dump_dir, test_dump_dir, num_components, num_levels, default=True)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/results.csv', index=False)\n",
    "\n",
    "\"\"\"results_df = pd.DataFrame(results)\n",
    "os.makedirs('NEW_results_csv', exist_ok=True)\n",
    "results_df.to_csv('NEW_results_csv/results.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_bits, PCA_win_bits,total_PCA_bits = bits_needed(df_test_reduced)\n",
    "    print(f\"Bits needed: {PCA_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {PCA_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_PCA_bits} bits\")\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        df_test_quantized = apply_quantization(df_test_reduced, num_levels)\n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_test_quantized, num_levels)\n",
    "        print(f\"Bits needed: {QT_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        REC_bits, REC_win_bits, total_REC_bits = bits_needed(df_test_reconstructed)\n",
    "        print(f\"Bits needed: {REC_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {REC_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_REC_bits} bits\")\n",
    "        \n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'num_components': num_components,\n",
    "            'num_levels': num_levels,\n",
    "            'PCA_bits': PCA_bits,\n",
    "            'QT_bits': QT_bits,\n",
    "            'REC_bits': REC_bits,\n",
    "            'PCA_win_bits': PCA_win_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'REC_win_bits': REC_win_bits,\n",
    "            'total_PCA_bits': total_PCA_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "            'total_REC_bits': total_REC_bits\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/bit_results_single_antenna_0.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "\n",
    "\"\"\"df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\"\"\"\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "#plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.axhline(y=0.56, color=\"r\", linestyle=\":\", label=\"max accuracy no PCA\")\n",
    "plt.title('Accuracy and Bits Comparison (PCA and Quantized data)')\n",
    "plt.xlabel('Bits per symbol')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/accuracy_bit[BxS][filtered].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/bit_results_single_antenna_0.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "\n",
    "\"\"\"df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\"\"\"\n",
    "\n",
    "components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_win_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "#plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.axhline(y=0.56, color=\"r\", linestyle=\":\", label=\"max accuracy no PCA\")\n",
    "plt.title('Accuracy and Bits Comparison (PCA and Quantized data)')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.savefig(os.path.join('results_graphs/accuracy_bit[BxW][1-20].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "filename = f'single_antenna_{antenna}'\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_train_reconstructed = reconstruct_data(df_train_reduced, pca, csi_subcarriers)\n",
    "    df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "    reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "    train_data.csi = reconstructed_train_data\n",
    "\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "    vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "    vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "    \n",
    "    print(\"Encoding train data...\")\n",
    "    z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "    \n",
    "    print(\"Encoding test data...\")\n",
    "    z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "    train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "    os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "    os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "    print(\"Saving data...\")\n",
    "    with open(train_dump_dir, 'wb') as f:\n",
    "        pickle.dump([z_data_train, z_labels_train], f)\n",
    "    with open(test_dump_dir, 'wb') as f:\n",
    "        pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'single_antenna_{antenna}'\n",
    "results = []\n",
    "\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/PCA_ONLY/{num_components}_components'\n",
    "\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    train_dump_dir = './dumps/single_antenna_0.pkl'\n",
    "    test_dump_dir = os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_components, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_components\": num_components,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "\"\"\"results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/PCA_ONLY_results.csv', index=False)\"\"\"\n",
    "\n",
    "\"\"\"results_df = pd.DataFrame(results)\n",
    "os.makedirs('NEW_results_csv', exist_ok=True)\n",
    "results_df.to_csv('NEW_results_csv/PCA_ONLY_results.csv', index=False)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs PCA only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_bits, PCA_win_bits,total_PCA_bits = bits_needed(df_test_reduced)\n",
    "    print(f\"Bits needed: {PCA_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {PCA_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_PCA_bits} bits\")\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "    print (f\"DF_RECONSTRUCTED\")\n",
    "    REC_bits, REC_win_bits, total_REC_bits = bits_needed(df_test_reconstructed)\n",
    "    print(f\"Bits needed: {REC_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {REC_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_REC_bits} bits\")\n",
    "    \n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    bit_results.append({\n",
    "        'num_components': num_components,\n",
    "        'PCA_bits': PCA_bits,\n",
    "        'REC_bits': REC_bits,\n",
    "        'PCA_win_bits': PCA_win_bits,\n",
    "        'REC_win_bits': REC_win_bits,\n",
    "        'total_PCA_bits': total_PCA_bits,\n",
    "        'total_REC_bits': total_REC_bits\n",
    "    })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/PCA_ONLY_bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/PCA_ONLY_results.csv')\n",
    "df_bits = pd.read_csv('results_csv/PCA_ONLY_bit_results_single_antenna_0.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_acc_bit['PCA_win_bits'], df_acc_bit['accuracy'], marker='o', linestyle='--', label='')\n",
    "plt.axhline(y=0.56, color=\"r\", linestyle=\":\", label=\"Max accuracy no PCA\")\n",
    "plt.title('Accuracy and Bits Comparison (Per PCA Component: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100])')\n",
    "plt.xlabel('avg bits per window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/accuracy_bit_PCA_Only[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/NEW'\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf_keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(encoder_inputs)\n",
    "    x = tf_keras.layers.BatchNormalization()(x)\n",
    "    x = tf_keras.layers.Conv2D(64, (5, 8), activation='relu', strides=(5, 8), padding='valid')(x)\n",
    "    x = tf_keras.layers.BatchNormalization()(x)\n",
    "    x = tf_keras.layers.Conv2D(128, (2, 4), activation='relu', strides=(2, 4), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf_keras.layers.Dropout(0.3)(x)  # Dropout added to regularize\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(128, (2, 4), activation='relu', strides=(2, 4), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(64, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_annealing = 1\n",
    "num_classes = 5\n",
    "\n",
    "ep = 1.0\n",
    "class GetEpochs(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global ep\n",
    "        ep += 1\n",
    "\n",
    "def res_to_mean(ev, dim = 5):\n",
    "    return np.max(dirichlet.mean(ev.reshape(dim,)+1))\n",
    "\n",
    "def res_to_dirichlet(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    S = np.sum(alpha)\n",
    "    K = 2.0\n",
    "    return dirichlet.mean(alpha), K/S\n",
    "\n",
    "def edl_accuracy(yTrue, yPred):\n",
    "    pred = K.argmax(yPred, axis=1)\n",
    "    truth = K.argmax(yTrue, axis=1)\n",
    "    match = K.reshape(K.cast(K.equal(pred, truth), \"float32\"),(-1,1))\n",
    "    return K.mean(match)\n",
    "\n",
    "def load_edl_experiment(name):\n",
    "    keras.models.load_model(name)\n",
    "\n",
    "def plot_res_beta(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    plt.figure(figsize=(16,9))\n",
    "    x = np.linspace(0,1,1000)\n",
    "    plt.plot(x, beta.pdf(x, alpha[1], alpha[0]))\n",
    "    x1, x2 = beta.interval(0.95, alpha[1], alpha[0])\n",
    "    areaplot = np.multiply(beta.pdf(x, alpha[1],alpha[0]), rect(x,x1, x2))\n",
    "    plt.fill_between(x, 0, areaplot, alpha=0.5)\n",
    "\n",
    "def results_test (train_dir, test_dir, num_components=0, num_levels=0, default=False):\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dir, scaler)\n",
    "    if default:\n",
    "        model_directory = os.path.join(base_directory, f'0_components/models/0components_0lvls_Keras_Model.keras')\n",
    "    else:\n",
    "        model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    \n",
    "    mlp_edl = keras.models.load_model(model_directory, compile=False)\n",
    "    mlp_edl_scores = np.array([res_to_mean(r, dim=5) for r in mlp_edl.predict(X_test)])\n",
    "    y_predictions_edl = np.array(tf.argmax(mlp_edl.predict(X_test), axis=1))\n",
    "\n",
    "    print(summary_clf(y_test, y_predictions_edl, mlp_edl_scores))\n",
    "    accuracy = accuracy_score(y_test, y_predictions_edl)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predictions_edl)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=semantic_classes)\n",
    "    cmdisp = disp.plot(cmap=\"cividis\")\n",
    "    CM_directory = os.path.join(base_directory, f'PCA_ONLY/{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    os.makedirs(os.path.dirname(CM_directory), exist_ok=True)\n",
    "    cmdisp.figure_.savefig(CM_directory, bbox_inches='tight')\n",
    "\n",
    "    return round(accuracy, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edl_experiment(name, _X_train, _y_train_dummy, num_components=0, num_levels=0):\n",
    "\n",
    "    model_edl = None\n",
    "    num_classes = 5\n",
    "    \n",
    "    if name == \"Delayed-Fusing\":\n",
    "        num_epochs_annealing = 3\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(16,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5, activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.001\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing3\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(6,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    else:\n",
    "        \n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 64\n",
    "        lr = 0.001\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Input(shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.4))\n",
    "        model_edl.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.4))\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(5, activation='softplus'))\n",
    "\t\t\n",
    "        \"\"\"\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(4, activation='relu', kernel_initializer=keras.initializers.GlorotUniform(seed=random_state), input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.GlorotUniform(seed=random_state)))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\"\"\"\n",
    "\n",
    "    def KL(alpha):\n",
    "        beta=K.constant(np.ones((1,num_classes)),dtype=\"float32\")\n",
    "        S_alpha = K.sum(alpha,axis=1,keepdims=True)\n",
    "        S_beta = K.sum(beta,axis=1,keepdims=True)\n",
    "        lnB = tf.math.lgamma(S_alpha) - K.sum(tf.math.lgamma(alpha),axis=1,keepdims=True)\n",
    "        lnB_uni = K.sum(tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n",
    "\n",
    "        dg0 = tf.math.digamma(S_alpha)\n",
    "        dg1 = tf.math.digamma(alpha)\n",
    "\n",
    "        return K.sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "\n",
    "    # Loss function considering the expected squared error and the KL divergence\n",
    "    def mse_loss(yTrue,yPred):\n",
    "        alpha = yPred + 1\n",
    "        S = K.sum(alpha, axis=1, keepdims=True)\n",
    "        m = alpha / S\n",
    "\n",
    "        # A + B minimises the sum of squared loss, see discussion in EDL paper for the derivation\n",
    "        A = K.sum((yTrue-m)**2, axis=1, keepdims=True)\n",
    "        B = K.sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
    "\n",
    "        # the lambda_t parameter, in this case min{1, t/10} with t the number of epochs\n",
    "        ll = min(1.0, float(ep/float(num_epochs_annealing)))\n",
    "        \n",
    "        alp = yPred*(1-yTrue) + 1 \n",
    "        C =  ll * KL(alp)\n",
    "\n",
    "        return A + B + C\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model_edl.compile(loss=mse_loss, optimizer=optimizer, metrics=[edl_accuracy])\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "    reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "    model_edl.fit(_X_train, _y_train_dummy,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      callbacks=[early_stopping_cb, reduce_lr_cb],\n",
    "      shuffle=False)\n",
    "\n",
    "    model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    os.makedirs(os.path.dirname(model_directory), exist_ok=True)\n",
    "    model_edl.save(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "folder_name = f'models/new_single_antenna_{antenna}'\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"-------------- 0 components --------------\")\n",
    "directory = './dumps/NEW/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "print(\"Encoding train data...\")\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "\n",
    "print(\"Encoding test data...\")\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_train, z_labels_train], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_test, z_labels_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/NEW/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler=scaler)\n",
    "\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "directory = './dumps/NEW/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir, default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Output Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VAE to process CSI data\n",
    "z_data = np.zeros([0, 4])\n",
    "z_labels = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/NEW/VAE_QNTZD/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "bit_results = []\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "\n",
    "df_z_data_train = pd.DataFrame(z_data_train, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "df_z_data_test = pd.DataFrame(z_data_test, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "\n",
    "for lvl in levels:\n",
    "    print(f\"-------------- {lvl} lvls --------------\")\n",
    "    df_train_quantized = apply_quantization(df_z_data_train, lvl)\n",
    "    df_test_quantized = apply_quantization(df_z_data_test, lvl)\n",
    "    print (f\"DF_QUANTIZED\")\n",
    "        #QT_avg_bits_per_symbol, QT_avg_bits_per_window, QT_total_bits = compute_bits_needed(df_quantized, verbose=False)\n",
    "    QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_test_quantized, lvl)\n",
    "    print(f\"Bits needed: {QT_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "    z_data_train = df_train_quantized.to_numpy()\n",
    "    z_data_test = df_test_quantized.to_numpy()\n",
    "\n",
    "    sub_dir=os.path.join(directory, f'training/{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "    os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "    with open(sub_dir, 'wb') as f:\n",
    "       pickle.dump([z_data_train, z_labels_train], f)\n",
    "\n",
    "    sub_dir=os.path.join(directory, f'test/{lvl}lvls_single_antenna_{antenna}_test.pkl')\n",
    "    os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "    with open(sub_dir, 'wb') as f:\n",
    "       pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "    bit_results.append({\n",
    "            'num_levels': lvl,\n",
    "            'QT_bits': QT_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/VAE_bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/NEW/VAE_QNTZD/0_components'\n",
    "for num_levels in levels:  \n",
    "    print(f\"-------------- {num_levels} lvls --------------\")\n",
    "    filename = f'{lvl}lvls_single_antenna_{antenna}'\n",
    "    train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler)\n",
    "\n",
    "    name = \"No-Fused-1\"\n",
    "    run_edl_experiment(name, X_train, y_train_dummy, num_components, num_levels)\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_components, num_levels)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": num_levels,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/NEW/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_accuracy = pd.read_csv('results_csv/NEW/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/NEW/VAE_bit_results_single_antenna_0.csv')\n",
    "\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.title('Accuracy and Bits Comparison (quantized data post VAE)')\n",
    "plt.axhline(y=0.68, color=\"r\", linestyle=\":\", label=\"max accuracy\")\n",
    "plt.xlabel('Bits per symbol')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/NEW/accuracy_bit_POST_VAE[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'single_antenna_{antenna}'\n",
    "results = []\n",
    "\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/NEW/PCA_ONLY/{num_components}_components'\n",
    "\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    train_dump_dir = './dumps/NEW/single_antenna_0.pkl'\n",
    "    test_dump_dir = os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_components, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_components\": num_components,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results_csv/NEW/PCA_ONLY_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/NEW/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_bits, PCA_win_bits,total_PCA_bits = bits_needed(df_test_reduced)\n",
    "    print(f\"Bits needed: {PCA_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {PCA_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_PCA_bits} bits\")\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "    print (f\"DF_RECONSTRUCTED\")\n",
    "    REC_bits, REC_win_bits, total_REC_bits = bits_needed(df_test_reconstructed)\n",
    "    print(f\"Bits needed: {REC_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {REC_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_REC_bits} bits\")\n",
    "    \n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    bit_results.append({\n",
    "        'num_components': num_components,\n",
    "        'PCA_bits': PCA_bits,\n",
    "        'REC_bits': REC_bits,\n",
    "        'PCA_win_bits': PCA_win_bits,\n",
    "        'REC_win_bits': REC_win_bits,\n",
    "        'total_PCA_bits': total_PCA_bits,\n",
    "        'total_REC_bits': total_REC_bits\n",
    "    })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/NEW/PCA_ONLY_bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/NEW/PCA_ONLY_results.csv')\n",
    "df_bits = pd.read_csv('results_csv/NEW/PCA_ONLY_bit_results_single_antenna_0.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components'])\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_acc_bit['PCA_win_bits'], df_acc_bit['accuracy'], marker='o', linestyle='--', label='')\n",
    "plt.axhline(y=0.70, color=\"r\", linestyle=\":\", label=\"Max accuracy no PCA\")\n",
    "plt.title('Accuracy and Bits Comparison (Per PCA Component: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100])')\n",
    "plt.xlabel('avg bits per window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/NEW/accuracy_bit_PCA_Only[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/NEW/{num_components}_components'\n",
    "    for num_levels in levels:  \n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        train_dump_dir = './dumps/NEW/single_antenna_0.pkl'\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "        # Test model\n",
    "        accuracy = results_test(train_dump_dir, test_dump_dir, num_components, num_levels, default=True)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/NEW/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- 1 components ----------------------\n",
      "Explained variance ratio: [0.3882368]\n",
      "Final Cumulative Explained Variance: 0.3882368\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 9.0 bits\n",
      "Total Bits needed: 225 bits\n",
      "-------------- 1 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 1.0 bits\n",
      "Total Bits needed: 25 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 0.74 bits\n",
      "AvgBits needed per window: 1520.0 bits\n",
      "Total Bits needed: 38000 bits\n",
      "-------------- 1 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 2.0 bits\n",
      "Total Bits needed: 50 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 1.33 bits\n",
      "AvgBits needed per window: 2720.0 bits\n",
      "Total Bits needed: 68000 bits\n",
      "-------------- 1 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 3.0 bits\n",
      "Total Bits needed: 75 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 1.95 bits\n",
      "AvgBits needed per window: 4000.0 bits\n",
      "Total Bits needed: 100000 bits\n",
      "-------------- 1 components 16 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 4.0 bits\n",
      "Total Bits needed: 100 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 2.85 bits\n",
      "AvgBits needed per window: 5840.0 bits\n",
      "Total Bits needed: 146000 bits\n",
      "-------------- 1 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 5.0 bits\n",
      "Total Bits needed: 125 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 3.79 bits\n",
      "AvgBits needed per window: 7760.0 bits\n",
      "Total Bits needed: 194000 bits\n",
      "-------------- 1 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 6.0 bits\n",
      "Total Bits needed: 150 bits\n",
      "DF_RECONSTRUCTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits needed: 4.69 bits\n",
      "AvgBits needed per window: 9600.0 bits\n",
      "Total Bits needed: 240000 bits\n",
      "-------------- 1 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 7.0 bits\n",
      "Total Bits needed: 175 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 5.47 bits\n",
      "AvgBits needed per window: 11200.0 bits\n",
      "Total Bits needed: 280000 bits\n",
      "-------------- 1 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 8.0 bits\n",
      "Total Bits needed: 200 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.17 bits\n",
      "AvgBits needed per window: 12640.0 bits\n",
      "Total Bits needed: 316000 bits\n",
      "-------------- 2 components ----------------------\n",
      "Explained variance ratio: [0.3882368  0.25320023]\n",
      "Final Cumulative Explained Variance: 0.64143705\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 18.0 bits\n",
      "Total Bits needed: 450 bits\n",
      "-------------- 2 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 2.0 bits\n",
      "Total Bits needed: 50 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 1.17 bits\n",
      "AvgBits needed per window: 2401.2 bits\n",
      "Total Bits needed: 60030 bits\n",
      "-------------- 2 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 4.0 bits\n",
      "Total Bits needed: 100 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 2.23 bits\n",
      "AvgBits needed per window: 4562.28 bits\n",
      "Total Bits needed: 114057 bits\n",
      "-------------- 2 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 6.0 bits\n",
      "Total Bits needed: 150 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 3.36 bits\n",
      "AvgBits needed per window: 6883.44 bits\n",
      "Total Bits needed: 172086 bits\n",
      "-------------- 2 components 16 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 8.0 bits\n",
      "Total Bits needed: 200 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 4.65 bits\n",
      "AvgBits needed per window: 9524.76 bits\n",
      "Total Bits needed: 238119 bits\n",
      "-------------- 2 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 10.0 bits\n",
      "Total Bits needed: 250 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 5.82 bits\n",
      "AvgBits needed per window: 11925.96 bits\n",
      "Total Bits needed: 298149 bits\n",
      "-------------- 2 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 12.0 bits\n",
      "Total Bits needed: 300 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.96 bits\n",
      "AvgBits needed per window: 14247.12 bits\n",
      "Total Bits needed: 356178 bits\n",
      "-------------- 2 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 14.0 bits\n",
      "Total Bits needed: 350 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.74 bits\n",
      "AvgBits needed per window: 15847.92 bits\n",
      "Total Bits needed: 396198 bits\n",
      "-------------- 2 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 16.0 bits\n",
      "Total Bits needed: 400 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.21 bits\n",
      "AvgBits needed per window: 16808.4 bits\n",
      "Total Bits needed: 420210 bits\n",
      "-------------- 3 components ----------------------\n",
      "Explained variance ratio: [0.3882367  0.2532001  0.15861158]\n",
      "Final Cumulative Explained Variance: 0.8000484\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 27.0 bits\n",
      "Total Bits needed: 675 bits\n",
      "-------------- 3 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 3.0 bits\n",
      "Total Bits needed: 75 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 1.72 bits\n",
      "AvgBits needed per window: 3523.52 bits\n",
      "Total Bits needed: 88088 bits\n",
      "-------------- 3 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 6.0 bits\n",
      "Total Bits needed: 150 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 2.82 bits\n",
      "AvgBits needed per window: 5765.76 bits\n",
      "Total Bits needed: 144144 bits\n",
      "-------------- 3 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 9.0 bits\n",
      "Total Bits needed: 225 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 4.26 bits\n",
      "AvgBits needed per window: 8728.72 bits\n",
      "Total Bits needed: 218218 bits\n",
      "-------------- 3 components 16 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 12.0 bits\n",
      "Total Bits needed: 300 bits\n",
      "DF_RECONSTRUCTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits needed: 5.75 bits\n",
      "AvgBits needed per window: 11771.76 bits\n",
      "Total Bits needed: 294294 bits\n",
      "-------------- 3 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 15.0 bits\n",
      "Total Bits needed: 375 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.08 bits\n",
      "AvgBits needed per window: 14493.96 bits\n",
      "Total Bits needed: 362349 bits\n",
      "-------------- 3 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 18.0 bits\n",
      "Total Bits needed: 450 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.82 bits\n",
      "AvgBits needed per window: 16016.0 bits\n",
      "Total Bits needed: 400400 bits\n",
      "-------------- 3 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 21.0 bits\n",
      "Total Bits needed: 525 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.33 bits\n",
      "AvgBits needed per window: 17057.04 bits\n",
      "Total Bits needed: 426426 bits\n",
      "-------------- 3 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 24.0 bits\n",
      "Total Bits needed: 600 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 4 components ----------------------\n",
      "Explained variance ratio: [0.3882367  0.25320005 0.15861155 0.05222311]\n",
      "Final Cumulative Explained Variance: 0.8522714\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 36.0 bits\n",
      "Total Bits needed: 900 bits\n",
      "-------------- 4 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 4.0 bits\n",
      "Total Bits needed: 100 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 2.39 bits\n",
      "AvgBits needed per window: 4884.88 bits\n",
      "Total Bits needed: 122122 bits\n",
      "-------------- 4 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 8.0 bits\n",
      "Total Bits needed: 200 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 4.07 bits\n",
      "AvgBits needed per window: 8328.24 bits\n",
      "Total Bits needed: 208206 bits\n",
      "-------------- 4 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 12.0 bits\n",
      "Total Bits needed: 300 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 5.47 bits\n",
      "AvgBits needed per window: 11211.08 bits\n",
      "Total Bits needed: 280277 bits\n",
      "-------------- 4 components 16 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 16.0 bits\n",
      "Total Bits needed: 400 bits\n",
      "DF_RECONSTRUCTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits needed: 6.92 bits\n",
      "AvgBits needed per window: 14174.04 bits\n",
      "Total Bits needed: 354351 bits\n",
      "-------------- 4 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 20.0 bits\n",
      "Total Bits needed: 500 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.82 bits\n",
      "AvgBits needed per window: 16015.88 bits\n",
      "Total Bits needed: 400397 bits\n",
      "-------------- 4 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 24.0 bits\n",
      "Total Bits needed: 600 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.21 bits\n",
      "AvgBits needed per window: 16816.76 bits\n",
      "Total Bits needed: 420419 bits\n",
      "-------------- 4 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 28.0 bits\n",
      "Total Bits needed: 700 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.72 bits\n",
      "AvgBits needed per window: 17854.68 bits\n",
      "Total Bits needed: 446367 bits\n",
      "-------------- 4 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 32.0 bits\n",
      "Total Bits needed: 800 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 5 components ----------------------\n",
      "Explained variance ratio: [0.38823688 0.25320035 0.15861148 0.05222313 0.01894593]\n",
      "Final Cumulative Explained Variance: 0.8712178\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 45.0 bits\n",
      "Total Bits needed: 1125 bits\n",
      "-------------- 5 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 5.0 bits\n",
      "Total Bits needed: 125 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 2.86 bits\n",
      "AvgBits needed per window: 5845.08 bits\n",
      "Total Bits needed: 146127 bits\n",
      "-------------- 5 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 10.0 bits\n",
      "Total Bits needed: 250 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 4.73 bits\n",
      "AvgBits needed per window: 9688.52 bits\n",
      "Total Bits needed: 242213 bits\n",
      "-------------- 5 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 15.0 bits\n",
      "Total Bits needed: 375 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.26 bits\n",
      "AvgBits needed per window: 12811.56 bits\n",
      "Total Bits needed: 320289 bits\n",
      "-------------- 5 components 16 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 20.0 bits\n",
      "Total Bits needed: 500 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.39 bits\n",
      "AvgBits needed per window: 15133.8 bits\n",
      "Total Bits needed: 378345 bits\n",
      "-------------- 5 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 25.0 bits\n",
      "Total Bits needed: 625 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.21 bits\n",
      "AvgBits needed per window: 16814.6 bits\n",
      "Total Bits needed: 420365 bits\n",
      "-------------- 5 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 30.0 bits\n",
      "Total Bits needed: 750 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.6 bits\n",
      "AvgBits needed per window: 17615.88 bits\n",
      "Total Bits needed: 440397 bits\n",
      "-------------- 5 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 35.0 bits\n",
      "Total Bits needed: 875 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18016.8 bits\n",
      "Total Bits needed: 450420 bits\n",
      "-------------- 5 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 40.0 bits\n",
      "Total Bits needed: 1000 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18017.76 bits\n",
      "Total Bits needed: 450444 bits\n",
      "-------------- 6 components ----------------------\n",
      "Explained variance ratio: [0.38823706 0.2532004  0.15861148 0.05222312 0.01894592 0.01761444]\n",
      "Final Cumulative Explained Variance: 0.8888324\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 54.0 bits\n",
      "Total Bits needed: 1350 bits\n",
      "-------------- 6 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 6.0 bits\n",
      "Total Bits needed: 150 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 3.36 bits\n",
      "AvgBits needed per window: 6886.2 bits\n",
      "Total Bits needed: 172155 bits\n",
      "-------------- 6 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 12.0 bits\n",
      "Total Bits needed: 300 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 5.24 bits\n",
      "AvgBits needed per window: 10729.32 bits\n",
      "Total Bits needed: 268233 bits\n",
      "-------------- 6 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 18.0 bits\n",
      "Total Bits needed: 450 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.65 bits\n",
      "AvgBits needed per window: 13611.52 bits\n",
      "Total Bits needed: 340288 bits\n",
      "-------------- 6 components 16 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 24.0 bits\n",
      "Total Bits needed: 600 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.55 bits\n",
      "AvgBits needed per window: 15454.6 bits\n",
      "Total Bits needed: 386365 bits\n",
      "-------------- 6 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 30.0 bits\n",
      "Total Bits needed: 750 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.25 bits\n",
      "AvgBits needed per window: 16894.92 bits\n",
      "Total Bits needed: 422373 bits\n",
      "-------------- 6 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 36.0 bits\n",
      "Total Bits needed: 900 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18014.72 bits\n",
      "Total Bits needed: 450368 bits\n",
      "-------------- 6 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 42.0 bits\n",
      "Total Bits needed: 1050 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18017.56 bits\n",
      "Total Bits needed: 450439 bits\n",
      "-------------- 6 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 48.0 bits\n",
      "Total Bits needed: 1200 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 7 components ----------------------\n",
      "Explained variance ratio: [0.38823664 0.2532003  0.15861148 0.05222312 0.01894592 0.01761443\n",
      " 0.01468579]\n",
      "Final Cumulative Explained Variance: 0.9035177\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 63.0 bits\n",
      "Total Bits needed: 1575 bits\n",
      "-------------- 7 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 7.0 bits\n",
      "Total Bits needed: 175 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 3.83 bits\n",
      "AvgBits needed per window: 7847.08 bits\n",
      "Total Bits needed: 196177 bits\n",
      "-------------- 7 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 14.0 bits\n",
      "Total Bits needed: 350 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 5.71 bits\n",
      "AvgBits needed per window: 11690.88 bits\n",
      "Total Bits needed: 292272 bits\n",
      "-------------- 7 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 21.0 bits\n",
      "Total Bits needed: 525 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.92 bits\n",
      "AvgBits needed per window: 14173.84 bits\n",
      "Total Bits needed: 354346 bits\n",
      "-------------- 7 components 16 lvls --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 28.0 bits\n",
      "Total Bits needed: 700 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.9 bits\n",
      "AvgBits needed per window: 16175.44 bits\n",
      "Total Bits needed: 404386 bits\n",
      "-------------- 7 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 35.0 bits\n",
      "Total Bits needed: 875 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.53 bits\n",
      "AvgBits needed per window: 17455.84 bits\n",
      "Total Bits needed: 436396 bits\n",
      "-------------- 7 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 42.0 bits\n",
      "Total Bits needed: 1050 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18016.96 bits\n",
      "Total Bits needed: 450424 bits\n",
      "-------------- 7 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 49.0 bits\n",
      "Total Bits needed: 1225 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 7 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 56.0 bits\n",
      "Total Bits needed: 1400 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 8 components ----------------------\n",
      "Explained variance ratio: [0.38823664 0.25320017 0.15861148 0.05222312 0.0189459  0.01761443\n",
      " 0.0146858  0.01046431]\n",
      "Final Cumulative Explained Variance: 0.91398185\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 72.0 bits\n",
      "Total Bits needed: 1800 bits\n",
      "-------------- 8 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 8.0 bits\n",
      "Total Bits needed: 200 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 4.28 bits\n",
      "AvgBits needed per window: 8761.88 bits\n",
      "Total Bits needed: 219047 bits\n",
      "-------------- 8 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 16.0 bits\n",
      "Total Bits needed: 400 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.07 bits\n",
      "AvgBits needed per window: 12425.96 bits\n",
      "Total Bits needed: 310649 bits\n",
      "-------------- 8 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 24.0 bits\n",
      "Total Bits needed: 600 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.28 bits\n",
      "AvgBits needed per window: 14899.8 bits\n",
      "Total Bits needed: 372495 bits\n",
      "-------------- 8 components 16 lvls --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 32.0 bits\n",
      "Total Bits needed: 800 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.18 bits\n",
      "AvgBits needed per window: 16739.88 bits\n",
      "Total Bits needed: 418497 bits\n",
      "-------------- 8 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 40.0 bits\n",
      "Total Bits needed: 1000 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.68 bits\n",
      "AvgBits needed per window: 17774.56 bits\n",
      "Total Bits needed: 444364 bits\n",
      "-------------- 8 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 48.0 bits\n",
      "Total Bits needed: 1200 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 8 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 56.0 bits\n",
      "Total Bits needed: 1400 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 8 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 64.0 bits\n",
      "Total Bits needed: 1600 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 9 components ----------------------\n",
      "Explained variance ratio: [0.3882367  0.25320023 0.1586115  0.05222312 0.0189459  0.01761443\n",
      " 0.01468581 0.01046432 0.00955898]\n",
      "Final Cumulative Explained Variance: 0.923541\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 81.0 bits\n",
      "Total Bits needed: 2025 bits\n",
      "-------------- 9 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 9.0 bits\n",
      "Total Bits needed: 225 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 4.74 bits\n",
      "AvgBits needed per window: 9711.96 bits\n",
      "Total Bits needed: 242799 bits\n",
      "-------------- 9 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 18.0 bits\n",
      "Total Bits needed: 450 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 6.62 bits\n",
      "AvgBits needed per window: 13550.32 bits\n",
      "Total Bits needed: 338758 bits\n",
      "-------------- 9 components 8 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 3.0 bits\n",
      "AvgBits needed per window: 27.0 bits\n",
      "Total Bits needed: 675 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 7.63 bits\n",
      "AvgBits needed per window: 15619.48 bits\n",
      "Total Bits needed: 390487 bits\n",
      "-------------- 9 components 16 lvls --------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_2006520\\1711346252.py:10: RuntimeWarning: Mean of empty slice.\n",
      "  new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
      "c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF_QUANTIZED\n",
      "Bits needed: 4.0 bits\n",
      "AvgBits needed per window: 36.0 bits\n",
      "Total Bits needed: 900 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.33 bits\n",
      "AvgBits needed per window: 17060.24 bits\n",
      "Total Bits needed: 426506 bits\n",
      "-------------- 9 components 32 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 5.0 bits\n",
      "AvgBits needed per window: 45.0 bits\n",
      "Total Bits needed: 1125 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18017.68 bits\n",
      "Total Bits needed: 450442 bits\n",
      "-------------- 9 components 64 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 6.0 bits\n",
      "AvgBits needed per window: 54.0 bits\n",
      "Total Bits needed: 1350 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 9 components 128 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 7.0 bits\n",
      "AvgBits needed per window: 63.0 bits\n",
      "Total Bits needed: 1575 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 9 components 256 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 8.0 bits\n",
      "AvgBits needed per window: 72.0 bits\n",
      "Total Bits needed: 1800 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 8.8 bits\n",
      "AvgBits needed per window: 18018.0 bits\n",
      "Total Bits needed: 450450 bits\n",
      "-------------- 10 components ----------------------\n",
      "Explained variance ratio: [0.38823688 0.2532004  0.15861166 0.05222312 0.01894592 0.01761443\n",
      " 0.0146858  0.01046432 0.00955896 0.00734708]\n",
      "Final Cumulative Explained Variance: 0.93088853\n",
      "DF_CSI_DATA\n",
      "Bits needed: 9.0 bits\n",
      "AvgBits needed per window: 90.0 bits\n",
      "Total Bits needed: 2250 bits\n",
      "-------------- 10 components 2 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 1.0 bits\n",
      "AvgBits needed per window: 10.0 bits\n",
      "Total Bits needed: 250 bits\n",
      "DF_RECONSTRUCTED\n",
      "Bits needed: 5.21 bits\n",
      "AvgBits needed per window: 10671.32 bits\n",
      "Total Bits needed: 266783 bits\n",
      "-------------- 10 components 4 lvls --------------\n",
      "DF_QUANTIZED\n",
      "Bits needed: 2.0 bits\n",
      "AvgBits needed per window: 20.0 bits\n",
      "Total Bits needed: 500 bits\n",
      "DF_RECONSTRUCTED\n"
     ]
    }
   ],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/NEW/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_bits, PCA_win_bits,total_PCA_bits = bits_needed(df_test_reduced)\n",
    "    print(f\"Bits needed: {PCA_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {PCA_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_PCA_bits} bits\")\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        df_test_quantized = apply_quantization(df_test_reduced, num_levels)\n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_test_quantized, num_levels)\n",
    "        print(f\"Bits needed: {QT_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        REC_bits, REC_win_bits, total_REC_bits = bits_needed(df_test_reconstructed)\n",
    "        print(f\"Bits needed: {REC_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {REC_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_REC_bits} bits\")\n",
    "        \n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'num_components': num_components,\n",
    "            'num_levels': num_levels,\n",
    "            'PCA_bits': PCA_bits,\n",
    "            'QT_bits': QT_bits,\n",
    "            'REC_bits': REC_bits,\n",
    "            'PCA_win_bits': PCA_win_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'REC_win_bits': REC_win_bits,\n",
    "            'total_PCA_bits': total_PCA_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "            'total_REC_bits': total_REC_bits\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/NEW/bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/NEW/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/NEW/bit_results_single_antenna_0.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "\n",
    "\"\"\"df_VAE_accuracy = pd.read_csv('results_csv/NEW/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/NEW/VAE_bit_results_single_antenna_0.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\"\"\"\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "#plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.axhline(y=0.56, color=\"r\", linestyle=\":\", label=\"max accuracy no PCA\")\n",
    "plt.title('Accuracy and Bits Comparison (PCA and Quantized data)')\n",
    "plt.xlabel('Bits per symbol')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "#plt.savefig(os.path.join('results_graphs/NEW/accuracy_bit[BxS][filtered].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/NEW/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/NEW/bit_results_single_antenna_0.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "\n",
    "\"\"\"df_VAE_accuracy = pd.read_csv('results_csv/NEW/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/NEW/VAE_bit_results_single_antenna_0.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\"\"\"\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_win_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "#plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.axhline(y=0.56, color=\"r\", linestyle=\":\", label=\"max accuracy no PCA\")\n",
    "plt.title('Accuracy and Bits Comparison (PCA and Quantized data)')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.savefig(os.path.join('results_graphs/NEW/accuracy_bit[BxW][1-20].png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
