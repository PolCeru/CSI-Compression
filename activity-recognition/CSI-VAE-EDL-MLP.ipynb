{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.cluster import vq\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"]= '1' # Use legacy keras for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTENNAS = 1\n",
    "antenna = 0  # if ANTENNAS==1, this value selects the antenna ID (from 0 to 3)\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "latent_dim = 2\n",
    "num_activities = 5\n",
    "folder_name = f'models/single_antenna_{antenna}'\n",
    "\n",
    "base_directory = './models'\n",
    "os.makedirs(base_directory, exist_ok=True)\n",
    "os.makedirs('./results/DEFAULT', exist_ok=True)\n",
    "os.makedirs('./results_csv/DEFAULT', exist_ok=True)\n",
    "os.makedirs('./results_graphs/DEFAULT', exist_ok=True)\n",
    "\n",
    "saveGraph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state) # predictable random numbers, for demonstration only\n",
    "tf.random.set_seed(random_state) # reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # make operations deterministic\n",
    "os.environ['PYTHONHASHSEED'] = str(random_state) # reproducibility\n",
    "\n",
    "# computes golden ratio for figures\n",
    "def goldenrect(h):\n",
    "    return (h * 1.618, h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf_keras.layers.Layer):\n",
    "    \"\"\"Takes a couple (z_mean, z_log_var) to draw a sample z from the latent space.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(encoder_inputs)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(x)\n",
    "    x = tf_keras.layers.Conv2D(32, (2, 4), activation='relu', strides=(2, 4), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (2, 4), activation='relu', strides=(2, 4), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf_keras.Model):\n",
    "    def __init__(self, enc_input_shape=(450, 2048, 1), dec_input_shape=(9, 8, 32), latent_dim=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_csi_encoder(enc_input_shape, latent_dim)\n",
    "        self.decoder = create_csi_decoder(dec_input_shape, latent_dim, enc_input_shape[-1])\n",
    "        self.total_loss_tracker = tf_keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = tf_keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = tf_keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf_keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vae_encoder(vae, source):\n",
    "    #Use the VAE to process CSI data\n",
    "    z_data = np.zeros([0, 4])\n",
    "    z_labels = np.zeros([0])\n",
    "\n",
    "    for (data, labels) in source:\n",
    "        labels = tf.squeeze(labels)\n",
    "        z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "        z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "        z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "        z_labels = np.concatenate([z_labels, labels.numpy().ravel()], axis=0)\n",
    "        \n",
    "    return z_data, z_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./{folder_name}/' + 'cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = tf_keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)\n",
    "early_stopping_cb = tf_keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "csv_logger_cb = tf_keras.callbacks.CSVLogger(f'./{folder_name}/model_history_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSI data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsiData(tf_keras.utils.Sequence):\n",
    "    def __init__(self, csi, labels, indices, batch_size=25, window_size=450, antennas=1):\n",
    "        self.csi = csi\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.antennas = antennas\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.indices.shape[-1] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        first_idx = batch_idx * self.batch_size\n",
    "        last_idx = (batch_idx + 1) * self.batch_size\n",
    "\n",
    "        #print(f'first_idx: {first_idx}, last_idx: {last_idx}')\n",
    "        \n",
    "        data_batch = [self.csi[x:x + self.window_size, ...] for x in range(first_idx, last_idx)]\n",
    "        labels_batch = np.transpose([self.labels[first_idx:last_idx]])\n",
    "\n",
    "        data_batch = tf.convert_to_tensor(data_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "\n",
    "        if self.antennas == 1:\n",
    "            data_batch = tf.expand_dims(data_batch, 3)\n",
    "            labels_batch = tf.expand_dims(labels_batch, 2)\n",
    "\n",
    "        return data_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test_CSI_data(file_list, num_samples=12000, window_size=450, batch_size=25, antennas=1, random_state=42, verbose=False):\n",
    "    #This code split the data into train and test sets and avoids data leakage with overlapping windows\n",
    "    #and returns the data in the format required by the CsiData class\n",
    "    csi_per_sec = 150\n",
    "    windows_per_activity = 5\n",
    "    train_seconds = 9\n",
    "    test_seconds = 3\n",
    "    ignore_second = 1\n",
    "\n",
    "    if antennas == 1:\n",
    "        train_data = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "        test_data = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "    else:\n",
    "        train_data = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "        test_data = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "\n",
    "    train_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    test_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    train_indices = tf.zeros([0], dtype=tf.int32)\n",
    "    test_indices = tf.zeros([0], dtype=tf.int32)\n",
    "\n",
    "    for file in file_list:\n",
    "        if verbose: print(f\"\\n*************** activity {file_list.index(file)} ***************\")\n",
    "        # Load CSI data from MATLAB file\n",
    "        mat = sio.loadmat(file)      # WARNING This code does not handle exceptions for simplicity...\n",
    "        data = np.array(mat['csi'])  # ...exceptions would require keeping track of indices\n",
    "        if antennas == 1:\n",
    "            data = data[range(num_samples), ..., int(antenna)]\n",
    "        data = np.round(np.abs(data))\n",
    "        train_index_offset = train_data.shape[0]\n",
    "        test_index_offset = test_data.shape[0]\n",
    "        activity_label = file_list.index(file)  # Labels depend on file index \n",
    "\n",
    "        n_samples_activity = data.shape[0]//windows_per_activity\n",
    "\n",
    "        tmp_train_data = np.zeros((0, 2048))\n",
    "        tmp_test_data = np.zeros((0, 2048))\n",
    "\n",
    "        for iter in range(windows_per_activity):\n",
    "            if verbose: print(f\"\\n-------- {iter} ---------\")\n",
    "            start_train_idx = 0 + (n_samples_activity) * iter\n",
    "            end_train_idx = start_train_idx + csi_per_sec * train_seconds\n",
    "            if verbose: print(f\"train idx {start_train_idx} to {end_train_idx}\")\n",
    "            tmp_train_data = np.append(tmp_train_data, data[start_train_idx : end_train_idx], axis=0)\n",
    "            \n",
    "            if verbose: print(\"train shape\", tmp_train_data.shape)\n",
    "\n",
    "            start_test_idx = end_train_idx + csi_per_sec * ignore_second\n",
    "            end_test_idx =  start_test_idx + csi_per_sec * test_seconds\n",
    "            if verbose: print(f\"ignoring idx from {end_train_idx} to {start_test_idx}\")\n",
    "\n",
    "            if verbose: print(f\"test idx {start_test_idx} to {end_test_idx}\")\n",
    "            tmp_test_data = np.append(tmp_test_data, data[start_test_idx : end_test_idx], axis=0)\n",
    "            \n",
    "            if verbose: print(\"test shape\", tmp_test_data.shape)\n",
    "\n",
    "        train_num_samples = tmp_train_data.shape[0]\n",
    "        tmp_train_data = tf.convert_to_tensor(tmp_train_data, dtype=tf.float32)\n",
    "        tmp_train_labels = tf.convert_to_tensor(activity_label * np.ones(train_num_samples - window_size), dtype=tf.int32)\n",
    "        tmp_train_indices = tf.convert_to_tensor(tf.range(train_index_offset, train_index_offset + train_num_samples - window_size), dtype=tf.int32)\n",
    "\n",
    "        test_num_samples = tmp_test_data.shape[0]\n",
    "        tmp_test_data = tf.convert_to_tensor(tmp_test_data, dtype=tf.float32)\n",
    "        tmp_test_labels = tf.convert_to_tensor(activity_label * np.ones(test_num_samples - window_size), dtype=tf.int32)\n",
    "        tmp_test_indices = tf.convert_to_tensor(tf.range(test_index_offset, test_index_offset + test_num_samples - window_size), dtype=tf.int32)\n",
    "\n",
    "        train_data = tf.concat([train_data, tmp_train_data], axis=0)\n",
    "        train_labels = tf.concat([train_labels, tmp_train_labels], axis=0)\n",
    "        train_indices = tf.concat([train_indices, tmp_train_indices], axis=0)\n",
    "\n",
    "        test_data = tf.concat([test_data, tmp_test_data], axis=0)\n",
    "        test_labels = tf.concat([test_labels, tmp_test_labels], axis=0)\n",
    "        test_indices = tf.concat([test_indices, tmp_test_indices], axis=0)\n",
    "\n",
    "        if verbose: print(train_data.shape, train_labels.shape)\n",
    "        if verbose: print(test_data.shape, test_labels.shape)\n",
    "\n",
    "    # Normalize the CSI dataset\n",
    "    if antennas == 1:\n",
    "        train_data = tf.math.divide(train_data, tf.math.reduce_max(train_data, axis=(0, 1)))\n",
    "        test_data = tf.math.divide(test_data, tf.math.reduce_max(test_data, axis=(0, 1)))\n",
    "    else:\n",
    "        train_data = tf.math.divide(train_data, tf.math.reduce_max(train_data, axis=(0, 1, 2)))\n",
    "        test_data = tf.math.divide(test_data, tf.math.reduce_max(test_data, axis=(0, 1, 2)))\n",
    "\n",
    "    train_data = CsiData(train_data, train_labels, train_indices, batch_size=batch_size, window_size=window_size, antennas=antennas)\n",
    "    test_data = CsiData(test_data, test_labels, test_indices, batch_size=batch_size, window_size=window_size, antennas=antennas)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components(data, target, directory=base_directory, saveGraph=False, plotGraph=True): #Useless\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    #Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "\n",
    "    var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "    #finds PCs that explain 95% of the variance\n",
    "    num_components = np.argmax(var_cumulative > target) + 1\n",
    "    print(f\"Number of components explaining {target}% variance: \"+ str(num_components))\n",
    "\n",
    "    if plotGraph:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title('Cumulative Explained Variance explained by the components')\n",
    "        plt.ylabel('Cumulative Explained variance')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.axvline(x=num_components, color=\"r\", linestyle=\"--\")\n",
    "        plt.axhline(y=target, color=\"r\", linestyle=\"--\")\n",
    "        plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', linestyle='--')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'var_cumulative_x_component.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "\n",
    "    return num_components\n",
    "\n",
    "def analyze_PCA(data, n_components, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    if (plotGraph):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "        plt.title('Cumulative Explained Variance by PCA Components')\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'cumulative_explained_variance.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "    \n",
    "    return reduced_df, pca\n",
    "\n",
    "def reconstruct_data(df, pca, columns):\n",
    "    df_reconstructed = pca.inverse_transform(df.values)\n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=columns)    \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    centroids = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (centroids[:-1] + centroids[1:]) / 2 #Defines intervals of centroids\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_centroids = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update centroids to better represent the data\n",
    "        \n",
    "        empty_centroids = np.isnan(new_centroids) #Restore previous cluster if empty\n",
    "        new_centroids[empty_centroids] = centroids[empty_centroids] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_centroids - centroids)) < delta:\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "   \n",
    "    quantized_data = centroids[indices]   #Quantize the data based on the final centroids\n",
    "    indices = indices.reshape(data.shape) #Reshape indices to match the original data shape\n",
    "\n",
    "    return quantized_data, centroids, indices\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, centroids, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return centroids[indices]\n",
    "\n",
    "def apply_quantization(reduced_df, lvls):\n",
    "    quantized_data, centroids, indices = lloyd_max_quantization(reduced_df.values, num_levels=lvls)\n",
    "    df_quantized = pd.DataFrame(quantized_data, columns=reduced_df.columns)\n",
    "    return df_quantized, centroids, indices\n",
    "\n",
    "def apply_existing_quantization(data, centroids):\n",
    "    thresholds = (centroids[:-1] + centroids[1:]) / 2\n",
    "    indices = np.digitize(data, thresholds)\n",
    "    quantized_data = centroids[indices] #Quantize the data based on the final centroids\n",
    "    indices = indices.reshape(data.shape) #Reshape indices to match the original data shape\n",
    "\n",
    "    df_quantized = pd.DataFrame(quantized_data, columns=data.columns)\n",
    "    return df_quantized, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vector_quantization(source, num_levels):\n",
    "    data = source.copy()\n",
    "    codebook, _ = vq.kmeans(data, num_levels, seed=random_state)\n",
    "    quantized_data, _ = vq.vq(data, codebook)\n",
    "    return quantized_data, codebook\n",
    "\n",
    "def apply_existing_vector_quantization(source, codebook):\n",
    "    data = source.copy()\n",
    "    quantized_data, _ = vq.vq(data, codebook)\n",
    "    return quantized_data\n",
    "\n",
    "def get_quantized_data(source, codebook):\n",
    "    data = source.copy()\n",
    "    quantized_data = codebook[data]\n",
    "    return quantized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self, value=None, frequency=0, left=None, right=None):\n",
    "        self.value = value\n",
    "        self.frequency = frequency\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other): #redefined \"less than\" operator for heapq\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "def build_tree(data):\n",
    "    heap = [Node(value, frequency) for value, frequency in data.items()]  #Init heap\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:  #pop two smallest nodes, merge them and push the merged node back\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = Node(frequency=left.frequency + right.frequency, left=left, right=right)\n",
    "        heapq.heappush(heap, merged) \n",
    "\n",
    "    return heap[0] #root\n",
    "\n",
    "def generate_codes(node, code=\"\", huffman_codes=None):\n",
    "    if huffman_codes is None: \n",
    "        huffman_codes = {}\n",
    "\n",
    "    if node.value is not None: #leaf node case\n",
    "        huffman_codes[node.value] = code\n",
    "    else:\n",
    "        generate_codes(node.left, code + \"0\", huffman_codes)\n",
    "        generate_codes(node.right, code + \"1\", huffman_codes)\n",
    "    return huffman_codes\n",
    "\n",
    "def encode_huffman(data, huffman_codes):\n",
    "    emptyStr = \"\"\n",
    "    return emptyStr.join([huffman_codes[val] for val in data]) \n",
    "\n",
    "def decode_huffman(encoded_data, huffman_codes):\n",
    "    decoded_data = []\n",
    "    code = \"\"\n",
    "    for bit in encoded_data: #traverse the encoded data and searches for the code\n",
    "        code += bit\n",
    "        for key, value in huffman_codes.items():\n",
    "            if value == code: #If found, append the corresponding value to the decoded data, otherwise add another bit to the code\n",
    "                decoded_data.append(key)\n",
    "                code = \"\"\n",
    "                break\n",
    "                \n",
    "    return decoded_data\n",
    "\n",
    "def apply_huffman_encode_per_feature(data):\n",
    "    encoded_dict = {}\n",
    "    huffman_codes = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col])\n",
    "        if len(freq_per_data) == 1:  # If only one unique value, there's no tree; assign it a code of '0'\n",
    "            code = {list(freq_per_data.keys())[0]: '0'}\n",
    "        else:\n",
    "            root = build_tree(freq_per_data)\n",
    "            code = generate_codes(root)\n",
    "        encoded_dict[col] = data[col].apply(lambda x: encode_huffman([x], code))\n",
    "        huffman_codes[col] = code\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_dict)\n",
    "    return encoded_df, huffman_codes\n",
    "\n",
    "\n",
    "def apply_huffman_decode_per_feature(encoded_data, huffman_codes):\n",
    "    decoded_df = pd.DataFrame()\n",
    "\n",
    "    for col in encoded_data.columns:\n",
    "        decoded_df[col] = decode_huffman(''.join(encoded_data[col]), huffman_codes[col])\n",
    "    return decoded_df\n",
    "\n",
    "def apply_encoding(df_quantized):\n",
    "    encoded_df, huffman_codes = apply_huffman_encode_per_feature(df_quantized)\n",
    "    return encoded_df, huffman_codes\n",
    "\n",
    "def apply_decoding(encoded_df, huffman_codes):\n",
    "    decoded_df = apply_huffman_decode_per_feature(encoded_df.iloc[:, 2:-1], huffman_codes)\n",
    "    return decoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_needed(source, num_lvls=-1, verbose=False):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    num_features = len(data.columns)\n",
    "    number_of_windows = len(data) // window_size\n",
    "    dataset_total_bits = 0 #Sum of the bits needed per frame\n",
    "\n",
    "    #These values will be consistent for every frame and window if num_lvls > 0\n",
    "    bits_per_feature = 0     #Bits needed per feature                                    log2(lvl or num_symbols)\n",
    "    window_frame_bits = 0    #Bits needed per frame (row) inside the window              log2(lvl or num_symbols) * num_features\n",
    "    window_feature_bits = 0  #Bits needed for one feature (col) inside the window        log2(lvl or num_symbols) * window_size\n",
    "    window_total_bits = 0    #Sum of the bits needed per frame (row) inside the window   log2(lvl or num_symbols) * num_features * window_size\n",
    "\n",
    "    if num_lvls > 0:\n",
    "        bits_per_feature = np.ceil(np.log2(num_lvls)).astype(int)\n",
    "        window_frame_bits = bits_per_feature * num_features\n",
    "        window_feature_bits = bits_per_feature * window_size #Bits needed for one feature in the window\n",
    "        window_total_bits = bits_per_feature * num_features * window_size\n",
    "        \n",
    "        dataset_total_bits = window_total_bits * number_of_windows\n",
    "    else: #Calculate the bits needed for each window and average it out if there's no quantization \n",
    "        bits_per_feature, bits_needed_per_frame, window_feature_bits, window_total_bits = {}, {}, {}, {}\n",
    "        for window_idx in range(0, len(data), window_size):\n",
    "            windowed_data = data.iloc[window_idx : window_idx + window_size] #Get the window\n",
    "            bits = {}\n",
    "            \n",
    "            for col in windowed_data.columns:                     \n",
    "                num_symbols = len(windowed_data[col].unique())\n",
    "                bits[col] = np.ceil(np.log2(num_symbols)).astype(int)\n",
    "            avg_bits_per_feature = np.mean(list(bits.values())).round(2)\n",
    "\n",
    "            bits_per_feature[window_idx] = avg_bits_per_feature\n",
    "            bits_needed_per_frame[window_idx] = avg_bits_per_feature * num_features\n",
    "            window_feature_bits[window_idx] = avg_bits_per_feature * window_size #Bits needed for one feature in the window\n",
    "            window_total_bits[window_idx] = avg_bits_per_feature * num_features * window_size\n",
    "\n",
    "        #Average it out for all windows\n",
    "        dataset_total_bits = sum(window_total_bits.values()).round(2)\n",
    "        \n",
    "        bits_per_feature = np.mean(list(bits_per_feature.values())).round(2)\n",
    "        window_frame_bits = np.mean(list(bits_needed_per_frame.values())).round(2) \n",
    "        window_feature_bits = np.mean(list(window_feature_bits.values())).round(2)\n",
    "        window_total_bits = np.mean(list(window_total_bits.values())).round(2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Number of windows: {number_of_windows}\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per feature: {bits_per_feature} bits\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per frame{\"\" if num_lvls>0 else \" per window\"}: {window_frame_bits} bits\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per feature per window: {window_feature_bits} bits\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per window: {window_total_bits} bits\")\n",
    "        print(f\"Total bits needed for the dataset: {dataset_total_bits} bits\")\n",
    "\n",
    "    return bits_per_feature, window_frame_bits, window_feature_bits, window_total_bits, dataset_total_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data, verbose=True):\n",
    "    entropy = 0\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col])  # Get frequency of each unique value\n",
    "        total_count = sum(freq_per_data.values())\n",
    "        col_entropy = 0\n",
    "        for count in freq_per_data.values():\n",
    "            p_i = count / total_count  # probability of each unique value\n",
    "            col_entropy += -p_i * np.log2(p_i)  # Entropy formula\n",
    "        if verbose: print(f\"Entropy of column {col}: {col_entropy} bits\")\n",
    "        entropy += col_entropy\n",
    "    return entropy.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy = 0.56\n",
    "def accuracy_loss(data, max_accuracy):\n",
    "    data['accuracy'] = data['accuracy'].clip(upper=max_accuracy) * 100\n",
    "    loss = (max_accuracy * 100) - data['accuracy']\n",
    "    data['accuracy_loss'] = loss\n",
    "    return data\n",
    "\n",
    "\"\"\"def normalized_accuracy(data, max_accuracy):\n",
    "    data['accuracy'] = data['accuracy'].clip(upper=max_accuracy) * 100\n",
    "    normalized_accuracy = 100 - ((max_accuracy * 100) - data['accuracy']) \n",
    "    data['normalized_accuracy'] = normalized_accuracy\n",
    "    return data\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "The VAE has been trained without any information about the target classes; it just tries to minimize reconstruction loss + KL loss.\n",
    "\n",
    "The Encoder in the VAE maps sequences of CSI into **2 Gaussian variables** with parameters (z_mean, z_log_var).\n",
    "\n",
    "More in detail, from the dataset we load `data` and `labels`.\n",
    "- `data`: every element is a 4-tuple with the values (z1_mean, z2_mean, z1_log_var, z2_log_var)\n",
    "- `labels`: 5 different classes, labelled with integers from 0 to 4 (0 = walk, 1 = run, 2 = jump, 3 = sit, 4 = empty)\n",
    "\n",
    "Available datasets:\n",
    "- `single_antenna`: data of just antenna 1, normalized wrt to the maximum value over the entire dataset (four antennas are available, numbered from 0 to 3)\n",
    "- `four_antennas`: data of the four antennas fused together, normalized wrt to the maximum value over the entire dataset\n",
    "- `four_antennas_latent_space_3`: same as `four_antennas`, but the CSI is mapped onto 3 Gaussian variables; hence, every element in `data` is a 6-tuple with the values (z1_mean, z2_mean, z3_mean, z1_log_var, z2_log_var, z3_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_classes = [\"Walk\", \"Run\", \"Jump\", \"Sit\", \"Empty\"]\n",
    "base_directory = './results/DEFAULT'\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(directory, scaler=None):\n",
    "    data = None\n",
    "    labels = None\n",
    "\n",
    "    with open(directory, 'rb') as f:\n",
    "        data, labels = pickle.load(f)\n",
    "    \n",
    "    fcolumns = ['mu1','mu2','sigma1','sigma2']\n",
    "\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=fcolumns)\n",
    "    df['signal'] = labels\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler().fit(df[fcolumns])\n",
    "    df[fcolumns] = scaler.transform(df[fcolumns])\n",
    "    \n",
    "    X = df[fcolumns]\n",
    "    y = df['signal']\n",
    "\n",
    "    # one-hot-encoding\n",
    "    y_dummy = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, y_dummy, scaler, fcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_annealing = 1\n",
    "num_classes = 5\n",
    "\n",
    "ep = 1.0\n",
    "class GetEpochs(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global ep\n",
    "        ep += 1\n",
    "\n",
    "def res_to_mean(ev, dim = 5):\n",
    "    return np.max(dirichlet.mean(ev.reshape(dim,)+1))\n",
    "\n",
    "def res_to_dirichlet(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    S = np.sum(alpha)\n",
    "    K = 2.0\n",
    "    return dirichlet.mean(alpha), K/S\n",
    "\n",
    "def edl_accuracy(yTrue, yPred):\n",
    "    pred = K.argmax(yPred, axis=1)\n",
    "    truth = K.argmax(yTrue, axis=1)\n",
    "    match = K.reshape(K.cast(K.equal(pred, truth), \"float32\"),(-1,1))\n",
    "    return K.mean(match)\n",
    "\n",
    "def load_edl_experiment(name):\n",
    "    keras.models.load_model(name)\n",
    "\n",
    "def plot_res_beta(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    plt.figure(figsize=(16,9))\n",
    "    x = np.linspace(0,1,1000)\n",
    "    plt.plot(x, beta.pdf(x, alpha[1], alpha[0]))\n",
    "    x1, x2 = beta.interval(0.95, alpha[1], alpha[0])\n",
    "    areaplot = np.multiply(beta.pdf(x, alpha[1],alpha[0]), rect(x,x1, x2))\n",
    "    plt.fill_between(x, 0, areaplot, alpha=0.5)\n",
    "\n",
    "def results_test (train_dir, test_dir, num_components=0, num_levels=0, default=False):\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dir, scaler)\n",
    "    if default:\n",
    "        model_directory = os.path.join(base_directory, f'0_components/models/0components_0lvls_Keras_Model.keras')\n",
    "    else:\n",
    "        model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    \n",
    "    mlp_edl = keras.models.load_model(model_directory, compile=False)\n",
    "    mlp_edl_scores = np.array([res_to_mean(r, dim=5) for r in mlp_edl.predict(X_test)])\n",
    "    y_predictions_edl = np.array(tf.argmax(mlp_edl.predict(X_test), axis=1))\n",
    "\n",
    "    print(classification_report(y_test, y_predictions_edl, labels=semantic_classes))\n",
    "    accuracy = accuracy_score(y_test, y_predictions_edl)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predictions_edl)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=semantic_classes)\n",
    "    cmdisp = disp.plot(cmap=\"cividis\")\n",
    "    CM_directory = os.path.join(base_directory, f'{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    os.makedirs(os.path.dirname(CM_directory), exist_ok=True)\n",
    "    cmdisp.figure_.savefig(CM_directory, bbox_inches='tight')\n",
    "\n",
    "    return round(accuracy, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edl_experiment(name, _X_train, _y_train_dummy, num_components=0, num_levels=0, X_val=None, y_val_dummy=None):\n",
    "\n",
    "    model_edl = None\n",
    "    num_classes = 5\n",
    "    \n",
    "    if name == \"Delayed-Fusing\":\n",
    "        num_epochs_annealing = 3\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(16,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5, activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.001\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing3\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(6,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    else:\n",
    "    \n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 64\n",
    "        lr = 0.001\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Input(shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.4))\n",
    "        model_edl.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.4))\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(5, activation='softplus'))\n",
    "        \"\"\"\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(4, activation='relu', kernel_initializer=keras.initializers.GlorotUniform(seed=random_state), input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=keras.initializers.GlorotUniform(seed=random_state)))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "        \"\"\"\n",
    "\n",
    "    def KL(alpha):\n",
    "        beta=K.constant(np.ones((1,num_classes)),dtype=\"float32\")\n",
    "        S_alpha = K.sum(alpha,axis=1,keepdims=True)\n",
    "        S_beta = K.sum(beta,axis=1,keepdims=True)\n",
    "        lnB = tf.math.lgamma(S_alpha) - K.sum(tf.math.lgamma(alpha),axis=1,keepdims=True)\n",
    "        lnB_uni = K.sum(tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n",
    "\n",
    "        dg0 = tf.math.digamma(S_alpha)\n",
    "        dg1 = tf.math.digamma(alpha)\n",
    "\n",
    "        return K.sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "\n",
    "    # Loss function considering the expected squared error and the KL divergence\n",
    "    def mse_loss(yTrue,yPred):\n",
    "        alpha = yPred + 1\n",
    "        S = K.sum(alpha, axis=1, keepdims=True)\n",
    "        m = alpha / S\n",
    "\n",
    "        # A + B minimises the sum of squared loss, see discussion in EDL paper for the derivation\n",
    "        A = K.sum((yTrue-m)**2, axis=1, keepdims=True)\n",
    "        B = K.sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
    "\n",
    "        # the lambda_t parameter, in this case min{1, t/10} with t the number of epochs\n",
    "        ll = min(1.0, float(ep/float(num_epochs_annealing)))\n",
    "        \n",
    "        alp = yPred*(1-yTrue) + 1 \n",
    "        C =  ll * KL(alp)\n",
    "\n",
    "        return A + B + C\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model_edl.compile(loss=mse_loss, optimizer=optimizer, metrics=[edl_accuracy])\n",
    "\n",
    "    model_edl.fit(_X_train, _y_train_dummy,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      #validation_data=(X_val, y_val_dummy),\n",
    "      shuffle=False)\n",
    "\n",
    "    model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    os.makedirs(os.path.dirname(model_directory), exist_ok=True)\n",
    "    model_edl.save(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "filename = f'single_antenna_{antenna}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,  # Use LaTeX for text rendering\n",
    "    \"font.family\": \"serif\",  # Use a serif font like LaTeX\n",
    "    \"axes.labelsize\": 12,  # Axis label font size\n",
    "    \"axes.titlesize\": 12,  # Title font size\n",
    "})\n",
    "\n",
    "target = 95\n",
    "\n",
    "#Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(df_csi_train)\n",
    "\n",
    "var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "#finds PCs that explain 95% of the variance\n",
    "k = np.argmax(var_cumulative > target) + 1\n",
    "print(f\"Number of components explaining {target}% variance: \"+ str(k))\n",
    "\n",
    "plt.figure(figsize=(5, 2))\n",
    "#plt.title('Cumulative Explained Variance explained per component')\n",
    "plt.ylabel(f'Cumulative Explained\\nVariance (\\%)', fontsize=10)\n",
    "plt.xlabel('Principal Components', fontsize=10)\n",
    "plt.xscale('log')\n",
    "plt.xticks([1, 10, 100, 1000, 2048], [1, 10, 100, 1000, 2048])\n",
    "#plt.xlim(0, 100)\n",
    "plt.yticks(np.arange(30, 101, 10))\n",
    "plt.axvline(x=k, color=\"r\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.axhline(y=target, color=\"r\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', markersize=2, linestyle='--', linewidth=0.7)\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('results_graphs', 'cumulative_variance.pdf'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "bit_results = []\n",
    "\n",
    "df_train_data = pd.DataFrame(train_data.csi)\n",
    "df_test_data = pd.DataFrame(test_data.csi)\n",
    "\n",
    "ORI_feature, ORI_frame, ORI_window_feature, ORI_window, ORI_total = bits_needed(df_test_data, verbose=True)\n",
    "\n",
    "bit_results.append({\n",
    "    'training_length': len(df_train_data),\n",
    "    'testing_length': len(df_test_data),\n",
    "    'ORI_feature_test': ORI_feature,\n",
    "    'ORI_frame_test': ORI_frame,\n",
    "    'ORI_window_test': ORI_window,\n",
    "    'ORI_total_test': ORI_total,\n",
    "})\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/original_bits.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS, verbose=True)\n",
    "\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_models = True\n",
    "\n",
    "if load_pretrained_models:\n",
    "    print('Using pretrained models')\n",
    "else:\n",
    "    # Train from scratch\n",
    "    vae = VAE()\n",
    "    vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "    vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "    vae.fit(train_data, epochs=20, shuffle=True, callbacks=[checkpoint_cb, csv_logger_cb])\n",
    "    vae.save_weights(f'./{folder_name}/train_weights_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"-------------- No PCA components --------------\")\n",
    "directory = './dumps/DEFAULT/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "print(\"Encoding train data...\")\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "\n",
    "print(\"Encoding test data...\")\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_train, z_labels_train], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_test, z_labels_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/DEFAULT/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler=scaler)\n",
    "\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "base_directory = './results/DEFAULT'\n",
    "directory = './dumps/DEFAULT/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir, default=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Output Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VAE to process CSI data\n",
    "z_data = np.zeros([0, 4])\n",
    "z_labels = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT/VAE_QNTZD'\n",
    "directory = f'./dumps/DEFAULT/VAE_QNTZD/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "bit_results = []\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "df_z_data_train = pd.DataFrame(z_data_train, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "df_z_data_test = pd.DataFrame(z_data_test, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "\n",
    "for lvl in levels:\n",
    "   print(f\"-------------- {lvl} lvls --------------\")\n",
    "   df_train_quantized, centroids, train_indices = apply_quantization(df_z_data_train, lvl) #LLoyd-Max quantization\n",
    "   df_test_quantized, test_indices = apply_existing_quantization(df_z_data_test, centroids)\n",
    "   \n",
    "   print (f\"DF_QUANTIZED\")\n",
    "   df_test_indices = pd.DataFrame(test_indices, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "   QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_indices, lvl, verbose=True)\n",
    "\n",
    "   z_data_train = df_train_quantized.to_numpy()\n",
    "   z_data_test = df_test_quantized.to_numpy()\n",
    "\n",
    "   df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "   print (f\"DF_ENCODED\")\n",
    "   ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, verbose=True)\n",
    "   enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'training/{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_train, z_labels_train], f)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'test/{lvl}lvls_single_antenna_{antenna}_test.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "   bit_results.append({\n",
    "         'num_levels': lvl,\n",
    "         'QT_feature': QT_feature,\n",
    "         'ENC_feature': ENC_feature,\n",
    "         'QT_frame': QT_frame,\n",
    "         'ENC_frame': ENC_frame,\n",
    "         'QT_window': QT_window,\n",
    "         'ENC_window': ENC_window,\n",
    "         'QT_total': QT_total,\n",
    "         'ENC_total': ENC_total,\n",
    "      })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/DEFAULT/VAE_bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/DEFAULT/VAE_QNTZD/0_components'\n",
    "base_directory = './results/DEFAULT/VAE_QNTZD'\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "results = []\n",
    "for num_levels in levels:  \n",
    "    print(f\"-------------- {num_levels} lvls --------------\")\n",
    "    filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "    train_dump_dir = './dumps/DEFAULT/single_antenna_0.pkl'\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_levels=num_levels, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": num_levels,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/DEFAULT/VAE_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_accuracy = pd.read_csv('results_csv/DEFAULT/VAE_accuracy.csv')\n",
    "df_VAE_accuracy = accuracy_loss(df_VAE_accuracy, max_accuracy)\n",
    "df_VAE_bits = pd.read_csv('results_csv/DEFAULT/VAE_bits.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_acc_bit.to_csv(f'./results_csv/DEFAULT/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/VAE_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (post VAE)')\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(df_VAE_acc_bit['QT_frame'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit_POST-VAE[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/VAE_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (post VAE)')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(df_VAE_acc_bit['QT_window'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit_POST-VAE[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT'\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/DEFAULT/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        \n",
    "        #Quantize the data\n",
    "        df_train_quantized, centroids, train_indices = apply_quantization(df_train_reduced, num_levels) #LLoyd-Max quantization\n",
    "        df_test_quantized, test_indices = apply_existing_quantization(df_test_reduced, centroids)\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "        train_data.csi = reconstructed_train_data\n",
    "\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "        vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "        \n",
    "        print(\"Encoding train data...\")\n",
    "        z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "        \n",
    "        print(\"Encoding test data...\")\n",
    "        z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "        train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "        os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "        os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "        print(\"Saving data...\")\n",
    "        with open(train_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_train, z_labels_train], f)\n",
    "        with open(test_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT'\n",
    "results = []\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/DEFAULT/{num_components}_components'\n",
    "    for num_levels in levels:  \n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        train_dump_dir = './dumps/DEFAULT/single_antenna_0.pkl'\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "        # Test model\n",
    "        accuracy = results_test(train_dump_dir, test_dump_dir, num_components, num_levels, default=True)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/DEFAULT/accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT'\n",
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/DEFAULT/{num_components}_components'\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_feature, PCA_frame, PCA_window_feature, PCA_window, PCA_total = bits_needed(df_test_reduced, verbose=True)\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components {num_levels} lvls --------------\")\n",
    "\n",
    "        #Quantize the data\n",
    "        df_train_quantized, centroids, train_indices = apply_quantization(df_train_reduced, num_levels) #LLoyd-Max quantization\n",
    "        df_test_quantized, test_indices = apply_existing_quantization(df_test_reduced, centroids)\n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        df_test_indices = pd.DataFrame(test_indices, columns=[f'PC{i}' for i in range(num_components)])\n",
    "        QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_indices, num_levels, verbose=True)\n",
    "\n",
    "        #Huffman\n",
    "        df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "        print (f\"DF_ENCODED\")\n",
    "        ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, verbose=True)\n",
    "        enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "        print(f\"Entropy of encoded data: {enc_entropy} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        REC_feature, REC_frame, REC_window_feature, REC_window, REC_total = bits_needed(df_test_reconstructed)\n",
    "        \n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'num_components': num_components,\n",
    "            'num_levels': num_levels,\n",
    "            'PCA_feature': PCA_feature,\n",
    "            'QT_feature': QT_feature,\n",
    "            'ENC_feature': ENC_feature,\n",
    "            'REC_feature': REC_feature,\n",
    "            'PCA_frame': PCA_frame,\n",
    "            'QT_frame': QT_frame,\n",
    "            'ENC_frame': ENC_frame,\n",
    "            'REC_frame': REC_frame,\n",
    "            'QT_window': QT_window,\n",
    "            'PCA_window': PCA_window,\n",
    "            'ENC_window': ENC_window,\n",
    "            'REC_window': REC_window,\n",
    "            'PCA_total': PCA_total,\n",
    "            'QT_total': QT_total,\n",
    "            'ENC_total': ENC_total,\n",
    "            'REC_total': REC_total\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/DEFAULT/bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv\n",
    "df_accuracy = pd.read_csv('results_csv/DEFAULT/accuracy.csv')\n",
    "df_accuracy = accuracy_loss(df_accuracy, max_accuracy)\n",
    "df_bits = pd.read_csv('results_csv/DEFAULT/bits.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "df_acc_bit.to_csv(f'./results_csv/DEFAULT/results.csv', index=False)\n",
    "\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/DEFAULT/VAE_accuracy.csv')\n",
    "df_VAE_accuracy = accuracy_loss(df_VAE_accuracy, max_accuracy)\n",
    "df_VAE_bits = pd.read_csv('results_csv/DEFAULT/VAE_bits.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_acc_bit.to_csv(f'./results_csv/DEFAULT/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_frame'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], target_data['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data and PCA applied')\n",
    "plt.yticks(np.arange(0, 19, 1))\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(np.arange(0, 340, 10))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_window'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], target_data['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data and PCA applied')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(np.arange(0, 160000, 10000))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 19, 1))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_frame'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], target_data['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per frame')\n",
    "plt.xticks(np.arange(0, 190, 10))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 19, 1))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy_bit_Encoded[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_window'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], target_data['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.xticks(np.arange(0, 85000, 5000))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 19, 1))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy_bit_Encoded[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for lvl in df_acc_bit['num_levels'].unique():\n",
    "    target_data = df_acc_bit[df_acc_bit['num_levels'] == lvl]\n",
    "\n",
    "    original_bits = target_data['QT_window']\n",
    "    huffman_bits = target_data['ENC_window']\n",
    "\n",
    "    saved_space_percentage = [(orig - huff) / orig * 100 for orig, huff in zip(original_bits, huffman_bits)]\n",
    "    # Plotting\n",
    "    plt.plot(components, saved_space_percentage, marker='o', linestyle='--', label=f'{lvl} levels')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks(components)\n",
    "plt.ylabel('Saved Space (%)')\n",
    "plt.title('Percentage of saved space per window with encoded data versus non-encoded data (quantized data with PCA applied)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig(os.path.join('results_graphs/DEFAULT/saved_space_percentage[QT-ENC].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for lvl in df_acc_bit['num_levels'].unique():\n",
    "    target_data = df_acc_bit[df_acc_bit['num_levels'] == lvl]\n",
    "\n",
    "    original_bits = target_data['PCA_window']\n",
    "    huffman_bits = target_data['ENC_window']\n",
    "\n",
    "    saved_space_percentage = [(orig - huff) / orig * 100 for orig, huff in zip(original_bits, huffman_bits)]\n",
    "    # Plotting\n",
    "    plt.plot(components, saved_space_percentage, marker='o', linestyle='--', label=f'{lvl} levels')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks(components)\n",
    "plt.ylabel('Saved Space (%)')\n",
    "plt.title('Percentage of saved space per window with encoded data versus non-encoded PCA data')\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig(os.path.join('results_graphs/DEFAULT/saved_space_percentage[PCA-ENC].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT/PCA_ONLY'\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "filename = f'single_antenna_{antenna}'\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/DEFAULT/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_train_reconstructed = reconstruct_data(df_train_reduced, pca, csi_subcarriers)\n",
    "    df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "    reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "    train_data.csi = reconstructed_train_data\n",
    "\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "    vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "    vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "    \n",
    "    print(\"Encoding train data...\")\n",
    "    z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "    \n",
    "    print(\"Encoding test data...\")\n",
    "    z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "    train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "    os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "    os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "    print(\"Saving data...\")\n",
    "    with open(train_dump_dir, 'wb') as f:\n",
    "        pickle.dump([z_data_train, z_labels_train], f)\n",
    "    with open(test_dump_dir, 'wb') as f:\n",
    "        pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT/PCA_ONLY'\n",
    "filename = f'single_antenna_{antenna}'\n",
    "results = []\n",
    "\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/DEFAULT/PCA_ONLY/{num_components}_components'\n",
    "\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    train_dump_dir = './dumps/DEFAULT/single_antenna_0.pkl'\n",
    "    test_dump_dir = os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_components, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_components\": num_components,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/DEFAULT/PCA_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT/PCA_ONLY'\n",
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/DEFAULT/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_feature, PCA_frame, PCA_window_feature, PCA_window, PCA_total = bits_needed(df_test_reduced, verbose=True)\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "\n",
    "    print (f\"DF_RECONSTRUCTED\")\n",
    "    REC_feature, REC_frame, REC_window_feature, REC_window, REC_total = bits_needed(df_test_reconstructed, verbose=True)\n",
    "    \n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    bit_results.append({\n",
    "        'num_components': num_components,\n",
    "        'PCA_feature': PCA_feature,\n",
    "        'REC_feature': REC_feature,\n",
    "        'PCA_frame': PCA_frame,\n",
    "        'REC_frame': REC_frame,\n",
    "        'PCA_window': PCA_window,\n",
    "        'REC_window': REC_window,\n",
    "        'PCA_total': PCA_total,\n",
    "        'REC_total': REC_total\n",
    "    })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/DEFAULT/PCA_bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 51, 5)) + list(range(60, 101, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv\n",
    "df_accuracy = pd.read_csv('results_csv/DEFAULT/PCA_accuracy.csv')\n",
    "df_accuracy = accuracy_loss(df_accuracy, max_accuracy)\n",
    "df_bits = pd.read_csv('results_csv/DEFAULT/PCA_bits.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components'])\n",
    "df_acc_bit.to_csv(f'./results_csv/DEFAULT/PCA_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/PCA_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_acc_bit['PCA_frame'], df_acc_bit['accuracy_loss'], marker='o', linestyle='--')\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]\n",
    "plt.title('Accuracy loss with PCA applied')\n",
    "plt.xlabel('Average bits per frame')\n",
    "plt.xticks(np.arange(0, 950, 50))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 17, 1))\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit_PCA-Only[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/PCA_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_acc_bit['PCA_window'], df_acc_bit['accuracy_loss'], marker='o', linestyle='--')\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]\n",
    "plt.title('Accuracy loss with PCA applied')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.xticks(np.arange(0, 425000, 25000))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 17, 1))\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit_PCA-Only[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Input Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [2**i for i in range(1, 9)]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/DEFAULT/QNT_Only'\n",
    "directory = f'./dumps/DEFAULT/QNT_Only/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "bit_results = []\n",
    "\n",
    "for lvl in levels:\n",
    "   print(f\"-------------- {lvl} lvls --------------\")\n",
    "   df_train = df_csi_train.copy()\n",
    "   df_test = df_csi_test.copy()\n",
    "   df_train_quantized, centroids, train_indices = apply_quantization(df_train, lvl) #LLoyd-Max quantization\n",
    "   df_test_quantized, test_indices = apply_existing_quantization(df_test, centroids)\n",
    "\n",
    "   print (f\"DF_QUANTIZED\")\n",
    "   df_test_indices = pd.DataFrame(test_indices, columns=csi_subcarriers)\n",
    "   QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_indices, lvl, verbose=True)\n",
    "\n",
    "   train_qntz_data =tf.convert_to_tensor(df_train_quantized.to_numpy(), dtype=tf.float32)\n",
    "   test_qntz_data = tf.convert_to_tensor(df_test_quantized.to_numpy(), dtype=tf.float32)\n",
    "\n",
    "   train_data.csi = train_qntz_data\n",
    "   test_data.csi = test_qntz_data\n",
    "\n",
    "   df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "   print (f\"DF_ENCODED\")\n",
    "   ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, verbose=True)\n",
    "   enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "\n",
    "   vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "   vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "   vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "   z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "   z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'training/{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_train, z_labels_train], f)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'test/{lvl}lvls_single_antenna_{antenna}_test.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "   bit_results.append({\n",
    "         'num_levels': lvl,\n",
    "         'QT_feature': QT_feature,\n",
    "         'ENC_feature': ENC_feature,\n",
    "         'QT_frame': QT_frame,\n",
    "         'ENC_frame': ENC_frame,\n",
    "         'QT_window': QT_window,\n",
    "         'ENC_window': ENC_window,\n",
    "         'QT_total': QT_total,\n",
    "         'ENC_total': ENC_total,\n",
    "      })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/DEFAULT/QNT_bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/DEFAULT/QNT_Only/0_components'\n",
    "base_directory = './results/DEFAULT/QNT_Only'\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "results = []\n",
    "for num_levels in levels:  \n",
    "    print(f\"-------------- {num_levels} lvls --------------\")\n",
    "    filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "    train_dump_dir = './dumps/DEFAULT/single_antenna_0.pkl'\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_levels=num_levels, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": num_levels,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/DEFAULT/QNT_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_accuracy = pd.read_csv('results_csv/DEFAULT/QNT_accuracy.csv')\n",
    "df_VAE_accuracy = accuracy_loss(df_VAE_accuracy, max_accuracy)\n",
    "df_VAE_bits = pd.read_csv('results_csv/DEFAULT/QNT_bits.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_acc_bit.to_csv(f'./results_csv/DEFAULT/QNT_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/QNT_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (pre VAE)')\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(df_VAE_acc_bit['QT_frame'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit_QNT-Only[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/DEFAULT/QNT_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (pre VAE)')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(df_VAE_acc_bit['QT_window'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/DEFAULT/accuracy-bit_QNT-Only[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Vector Quantization Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Output Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VAE to process CSI data\n",
    "z_data = np.zeros([0, 4])\n",
    "z_labels = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/VQ/VAE_QNTZD'\n",
    "directory = f'./dumps/VQ/VAE_QNTZD/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "bit_results = []\n",
    "levels = [2**i for i in range(1, 15)]\n",
    "\n",
    "labels_train = z_labels_train.copy()\n",
    "labels_test = z_labels_test.copy()\n",
    "\n",
    "for lvl in levels:\n",
    "   print(f\"-------------- {lvl} lvls --------------\")\n",
    "   data_train = z_data_train.copy()\n",
    "   data_test = z_data_test.copy()\n",
    "   \n",
    "   print (f\"DF_QUANTIZED\")\n",
    "   train_quantized, codebook = apply_vector_quantization(data_train, lvl) #Vector quantization\n",
    "   test_quantized = apply_existing_vector_quantization(data_test, codebook)\n",
    "   \n",
    "   df_test_quantized = pd.DataFrame(test_quantized, columns=['centroid_num'])\n",
    "   QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_quantized, lvl, verbose=True)\n",
    "\n",
    "   train_quantized = get_quantized_data(train_quantized, codebook)\n",
    "   test_quantized = get_quantized_data(test_quantized, codebook)\n",
    "\n",
    "   df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "   print (f\"DF_ENCODED\")\n",
    "   ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, verbose=True)\n",
    "   enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "   print(f\"Entropy of encoded data: {enc_entropy} bits\")\n",
    "\n",
    "   df_train_quantized = pd.DataFrame(train_quantized, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "   df_test_quantized = pd.DataFrame(test_quantized, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "   data_train = df_train_quantized.to_numpy()\n",
    "   data_test = df_test_quantized.to_numpy()\n",
    "\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'training/{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([data_train, labels_train], f)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'test/{lvl}lvls_single_antenna_{antenna}_test.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([data_test, labels_test], f)\n",
    "\n",
    "   bit_results.append({\n",
    "         'num_levels': lvl,\n",
    "         'QT_feature': QT_feature,\n",
    "         'ENC_feature': ENC_feature,\n",
    "         'QT_frame': QT_frame,\n",
    "         'ENC_frame': ENC_frame,\n",
    "         'QT_window': QT_window,\n",
    "         'ENC_window': ENC_window,\n",
    "         'QT_total': QT_total,\n",
    "         'ENC_total': ENC_total,\n",
    "      })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/VQ/VAE_bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/VQ/VAE_QNTZD'\n",
    "directory = f'./dumps/VQ/VAE_QNTZD/0_components'\n",
    "levels = [2**i for i in range(1, 15)]\n",
    "results = []\n",
    "for num_levels in levels:  \n",
    "    print(f\"-------------- {num_levels} lvls --------------\")\n",
    "    filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "    train_dump_dir = './dumps/VQ/single_antenna_0.pkl'\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_levels=num_levels, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": num_levels,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/VQ/VAE_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/VQ/VAE_accuracy.csv')\n",
    "df_VAE_accuracy = accuracy_loss(df_VAE_accuracy, max_accuracy)\n",
    "df_VAE_bits = pd.read_csv('results_csv/VQ/VAE_bits.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "df_VAE_acc_bit.to_csv(f'./results_csv/VQ/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/VQ/VAE_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (post VAE)')\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(df_VAE_acc_bit['QT_frame'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 25, 2))\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy-bit_POST-VAE[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/VQ/VAE_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (post VAE)')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(df_VAE_acc_bit['QT_window'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 25, 2))\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy-bit_POST-VAE[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "base_directory = './results/VQ'\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/VQ/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        #Quantize the data\n",
    "        train_quantized, codebook = apply_vector_quantization(df_train_reduced, num_levels) #Vector quantization\n",
    "        test_quantized = apply_existing_vector_quantization(df_test_reduced, codebook)\n",
    "        \n",
    "        train_quantized = get_quantized_data(train_quantized, codebook)\n",
    "        test_quantized = get_quantized_data(test_quantized, codebook)\n",
    "\n",
    "        df_train_quantized = pd.DataFrame(train_quantized, columns=[f'PC{i}' for i in range(num_components)])\n",
    "        df_test_quantized = pd.DataFrame(test_quantized, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "        train_data.csi = reconstructed_train_data\n",
    "\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "        vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "        \n",
    "        print(\"Encoding train data...\")\n",
    "        z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "        \n",
    "        print(\"Encoding test data...\")\n",
    "        z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "        train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "        os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "        os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "        print(\"Saving data...\")\n",
    "        with open(train_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_train, z_labels_train], f)\n",
    "        with open(test_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/VQ'\n",
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "results = []\n",
    "for num_components in components:\n",
    "    directory = f'./dumps/VQ/{num_components}_components'\n",
    "    for num_levels in levels:  \n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        train_dump_dir = './dumps/VQ/single_antenna_0.pkl'\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "        # Test model\n",
    "        accuracy = results_test(train_dump_dir, test_dump_dir, num_components, num_levels, default=True)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/VQ/accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/VQ'\n",
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/VQ/{num_components}_components'\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_feature, PCA_frame, PCA_window_feature, PCA_window, PCA_total = bits_needed(df_test_reduced, verbose=True)\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        train_quantized, codebook = apply_vector_quantization(df_train_reduced, num_levels) #Vector quantization\n",
    "        test_quantized = apply_existing_vector_quantization(df_test_reduced, codebook)\n",
    "        \n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        df_test_quantized = pd.DataFrame(test_quantized, columns=['centroid_num'])\n",
    "        QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_quantized, num_levels, verbose=True)\n",
    "\n",
    "        #Huffman\n",
    "        df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "        print (f\"DF_ENCODED\")\n",
    "        ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, verbose=True)\n",
    "        enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "        print(f\"Entropy of encoded data: {enc_entropy} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        test_quantized = get_quantized_data(test_quantized, codebook)\n",
    "        df_test_quantized = pd.DataFrame(test_quantized, columns=[f'PC{i}' for i in range(num_components)])\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        REC_feature, REC_frame, REC_window_feature, REC_window, REC_total = bits_needed(df_test_reconstructed)\n",
    "        \n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'num_components': num_components,\n",
    "            'num_levels': num_levels,\n",
    "            'PCA_feature': PCA_feature,\n",
    "            'QT_feature': QT_feature,\n",
    "            'ENC_feature': ENC_feature,\n",
    "            'REC_feature': REC_feature,\n",
    "            'PCA_frame': PCA_frame,\n",
    "            'QT_frame': QT_frame,\n",
    "            'ENC_frame': ENC_frame,\n",
    "            'REC_frame': REC_frame,\n",
    "            'QT_window': QT_window,\n",
    "            'PCA_window': PCA_window,\n",
    "            'ENC_window': ENC_window,\n",
    "            'REC_window': REC_window,\n",
    "            'PCA_total': PCA_total,\n",
    "            'QT_total': QT_total,\n",
    "            'ENC_total': ENC_total,\n",
    "            'REC_total': REC_total\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/VQ/bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv\n",
    "df_accuracy = pd.read_csv('results_csv/VQ/accuracy.csv')\n",
    "df_accuracy = accuracy_loss(df_accuracy, max_accuracy)\n",
    "df_bits = pd.read_csv('results_csv/VQ/bits.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "df_acc_bit.to_csv(f'./results_csv/VQ/results.csv', index=False)\n",
    "\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/VQ/VAE_accuracy.csv')\n",
    "df_VAE_accuracy = accuracy_loss(df_VAE_accuracy, max_accuracy)\n",
    "df_VAE_bits = pd.read_csv('results_csv/VQ/VAE_bits.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_acc_bit.to_csv(f'./results_csv/VQ/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "components = [1, 2, 3, 4, 5, 6, 10]\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "df_acc_bit = pd.read_csv('results_csv/VQ/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv('results_csv/VQ/VAE_results.csv')\n",
    "df_VAE_acc_bit = df_VAE_acc_bit[df_VAE_acc_bit['num_levels'] <= df_acc_bit['num_levels'].max()] #Remove levels that are not in acc_bit\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_frame'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], df_VAE_acc_bit['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data and PCA applied')\n",
    "plt.yticks(np.arange(0, 32, 2))\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(target_data['QT_frame'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy-bit[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "components = [1, 2, 3, 4, 5, 6, 10]\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "df_acc_bit = pd.read_csv('results_csv/VQ/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv('results_csv/VQ/VAE_results.csv')\n",
    "df_VAE_acc_bit = df_VAE_acc_bit[df_VAE_acc_bit['num_levels'] <= df_acc_bit['num_levels'].max()]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_window'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], df_VAE_acc_bit['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data and PCA applied')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(target_data['QT_window'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 32, 2))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy-bit[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "components = [1, 2, 3, 4, 5, 6, 10]\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "df_acc_bit = pd.read_csv('results_csv/VQ/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv('results_csv/VQ/VAE_results.csv')\n",
    "df_VAE_acc_bit = df_VAE_acc_bit[df_VAE_acc_bit['num_levels'] <= df_acc_bit['num_levels'].max()]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_frame'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], df_VAE_acc_bit['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per frame')\n",
    "plt.xticks(np.arange(0, 12, 1))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 32, 2))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy_bit_Encoded[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "components = [1, 2, 3, 4, 5, 6, 10]\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "df_acc_bit = pd.read_csv('results_csv/VQ/results.csv')\n",
    "df_VAE_acc_bit = pd.read_csv('results_csv/VQ/VAE_results.csv')\n",
    "df_VAE_acc_bit = df_VAE_acc_bit[df_VAE_acc_bit['num_levels'] <= df_acc_bit['num_levels'].max()]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_window'], target_data['accuracy_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], df_VAE_acc_bit['accuracy_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.xticks(np.arange(0, 5500, 500))\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.yticks(np.arange(0, 32, 2))\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy_bit_Encoded[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "df_accuracy = pd.read_csv('results_csv/VQ/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/VQ/bit_results_single_antenna_02.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "\n",
    "df_acc_bit = df_acc_bit[df_acc_bit['num_components'].isin(components)]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for lvl in df_acc_bit['num_levels'].unique():\n",
    "    target_data = df_acc_bit[df_acc_bit['num_levels'] == lvl]\n",
    "\n",
    "    original_bits = target_data['QT_window']\n",
    "    huffman_bits = target_data['ENC_window']\n",
    "\n",
    "    saved_space_percentage = [(orig - huff) / orig * 100 for orig, huff in zip(original_bits, huffman_bits)]\n",
    "    # Plotting\n",
    "    plt.plot(components, saved_space_percentage, marker='o', linestyle='--', label=f'{lvl} levels')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks(components)\n",
    "plt.ylabel('Saved Space (%)')\n",
    "plt.title('Percentage of saved space per window with encoded data versus non-encoded data (quantized data with PCA applied)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig(os.path.join('results_graphs/VQ/saved_space_percentage[QT-ENC].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11))\n",
    "levels = [2**i for i in range(1, 12)]\n",
    "\n",
    "df_accuracy = pd.read_csv('results_csv/VQ/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/VQ/bit_results_single_antenna_02.csv')\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "\n",
    "df_acc_bit = df_acc_bit[df_acc_bit['num_components'].isin(components)]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for lvl in df_acc_bit['num_levels'].unique():\n",
    "    target_data = df_acc_bit[df_acc_bit['num_levels'] == lvl]\n",
    "\n",
    "    original_bits = target_data['PCA_window']\n",
    "    huffman_bits = target_data['ENC_window']\n",
    "\n",
    "    saved_space_percentage = [(orig - huff) / orig * 100 for orig, huff in zip(original_bits, huffman_bits)]\n",
    "    # Plotting\n",
    "    plt.plot(components, saved_space_percentage, marker='o', linestyle='--', label=f'{lvl} levels')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks(components)\n",
    "plt.ylabel('Saved Space (%)')\n",
    "plt.title('Percentage of saved space per window on encoded data versus non-encoded PCA data')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig(os.path.join('results_graphs/VQ/saved_space_percentage[PCA-ENC].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Input Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [2**i for i in range(1, 9)]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = './results/VQ/QNT_Only'\n",
    "directory = f'./dumps/VQ/QNT_Only/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "bit_results = []\n",
    "\n",
    "for lvl in levels:\n",
    "   print(f\"-------------- {lvl} lvls --------------\")\n",
    "   df_train = df_csi_train.copy()\n",
    "   df_test = df_csi_test.copy()\n",
    "   \n",
    "   train_quantized, codebook = apply_vector_quantization(df_train, lvl) #LLoyd-Max quantization\n",
    "   test_quantized = apply_existing_vector_quantization(df_test, codebook)\n",
    "   \n",
    "   print (f\"DF_QUANTIZED\")\n",
    "   df_test_quantized_bits = pd.DataFrame(test_quantized, columns=['centroid_num'])\n",
    "   QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_quantized_bits, lvl, verbose=True)\n",
    "\n",
    "   train_quantized = get_quantized_data(train_quantized, codebook)\n",
    "   test_quantized = get_quantized_data(test_quantized, codebook)\n",
    "\n",
    "   df_train_quantized = pd.DataFrame(train_quantized, columns=csi_subcarriers)\n",
    "   df_test_quantized = pd.DataFrame(test_quantized, columns=csi_subcarriers)\n",
    "\n",
    "\n",
    "   train_qntz_data =tf.convert_to_tensor(df_train_quantized.to_numpy(), dtype=tf.float32)\n",
    "   test_qntz_data = tf.convert_to_tensor(df_test_quantized.to_numpy(), dtype=tf.float32)\n",
    "\n",
    "   train_data.csi = train_qntz_data\n",
    "   test_data.csi = test_qntz_data\n",
    "\n",
    "   df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "   print (f\"DF_ENCODED\")\n",
    "   ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, verbose=True)\n",
    "   enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "   print(f\"Entropy of encoded data: {enc_entropy} bits\")\n",
    "   \"\"\"\n",
    "   vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "   vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "   vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "   z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "   z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'training/{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_train, z_labels_train], f)\n",
    "\n",
    "   sub_dir=os.path.join(directory, f'test/{lvl}lvls_single_antenna_{antenna}_test.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_test, z_labels_test], f)\"\"\"\n",
    "\n",
    "   bit_results.append({\n",
    "         'num_levels': lvl,\n",
    "         'QT_feature': QT_feature,\n",
    "         'ENC_feature': ENC_feature,\n",
    "         'QT_frame': QT_frame,\n",
    "         'ENC_frame': ENC_frame,\n",
    "         'QT_window': QT_window,\n",
    "         'ENC_window': ENC_window,\n",
    "         'QT_total': QT_total,\n",
    "         'ENC_total': ENC_total,\n",
    "      })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/VQ/QNT_bits2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./dumps/VQ/QNT_Only/0_components'\n",
    "base_directory = './results/VQ/QNT_Only'\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "results = []\n",
    "for num_levels in levels:  \n",
    "    print(f\"-------------- {num_levels} lvls --------------\")\n",
    "    filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "    train_dump_dir = './dumps/VQ/single_antenna_0.pkl'\n",
    "    test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "    # Test model\n",
    "    accuracy = results_test(train_dump_dir, test_dump_dir, num_levels=num_levels, default=True)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": num_levels,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/VQ/QNT_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_accuracy = pd.read_csv('results_csv/VQ/QNT_accuracy.csv')\n",
    "df_VAE_accuracy = accuracy_loss(df_VAE_accuracy, max_accuracy)\n",
    "df_VAE_bits = pd.read_csv('results_csv/VQ/QNT_bits.csv')\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_acc_bit.to_csv(f'./results_csv/VQ/QNT_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/VQ/QNT_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_frame'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (pre VAE)')\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(df_VAE_acc_bit['QT_frame'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy-bit_QNT-Only[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_acc_bit = pd.read_csv(f'./results_csv/VQ/QNT_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_acc_bit['QT_window'], df_VAE_acc_bit['accuracy_loss'], marker='o', linewidth=2)\n",
    "plt.title('Accuracy loss with quantized data (pre VAE)')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(df_VAE_acc_bit['QT_window'])\n",
    "plt.ylabel('Accuracy Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/VQ/accuracy-bit_QNT-Only[BxW].png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
