{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training and processing\n",
    "\n",
    "Sample code to train a new VAE and run the CSI processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"]= '1' # Use legacy keras for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANTENNAS = 1\n",
    "antenna = 0  # if ANTENNAS==1, this value selects the antenna ID (from 0 to 3)\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "latent_dim = 2\n",
    "num_activities = 5\n",
    "folder_name = f'models/single_antenna_{antenna}'\n",
    "\n",
    "base_directory = './models'\n",
    "saveGraph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state) # predictable random numbers, for demonstration only\n",
    "tf.random.set_seed(random_state) # reproducibility\n",
    "\n",
    "# computes golden ratio for figures\n",
    "def goldenrect(h):\n",
    "    return (h * 1.618, h)\n",
    "\n",
    "def summary_clf(y_test, predicted, y_score, _labels = None):\n",
    "    print(classification_report(y_test, predicted, labels= _labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSI data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsiData(tf_keras.utils.Sequence):\n",
    "    def __init__(self, csi, labels, indices, batch_size=25, window_size=450, antennas=1):\n",
    "        self.csi = csi\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.antennas = antennas\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.indices.shape[-1] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        first_idx = batch_idx * self.batch_size\n",
    "        last_idx = (batch_idx + 1) * self.batch_size\n",
    "        \n",
    "        data_batch = [self.csi[x:x + self.window_size, ...] for x in range(first_idx, last_idx)]\n",
    "        labels_batch = np.transpose([self.labels[first_idx:last_idx]])\n",
    "\n",
    "        data_batch = tf.convert_to_tensor(data_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "\n",
    "        if self.antennas == 1:\n",
    "            data_batch = tf.expand_dims(data_batch, 3)\n",
    "            labels_batch = tf.expand_dims(labels_batch, 2)\n",
    "\n",
    "        return data_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split_train_test_CSI_data(file_list, num_samples=12000, window_size=450, batch_size=25, antennas=1, random_state=42, verbose=False):\n",
    "    csi_per_sec = 150\n",
    "    windows_per_activity = 5\n",
    "    train_seconds = 9\n",
    "    test_seconds = 1\n",
    "    ignore_seconds = 3\n",
    "\n",
    "    if antennas == 1:\n",
    "        train_data = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "        test_data = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "    else:\n",
    "        train_data = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "        test_data = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "\n",
    "    train_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    test_labels = tf.zeros([0], dtype=tf.int32)\n",
    "    train_indices = tf.zeros([0], dtype=tf.int32)\n",
    "    test_indices = tf.zeros([0], dtype=tf.int32)\n",
    "\n",
    "    for file in file_list:\n",
    "        if verbose: print(f\"\\n*************** activity {file_list.index(file)} ***************\")\n",
    "        # Load CSI data from MATLAB file\n",
    "        mat = sio.loadmat(file)      # WARNING This code does not handle exceptions for simplicity...\n",
    "        data = np.array(mat['csi'])  # ...exceptions would require keeping track of indices\n",
    "        if antennas == 1:\n",
    "            data = data[range(num_samples), ..., int(antenna)]\n",
    "        data = np.round(np.abs(data))\n",
    "        train_index_offset = train_data.shape[0]\n",
    "        test_index_offset = test_data.shape[0]\n",
    "        activity_label = file_list.index(file)  # Labels depend on file index \n",
    "\n",
    "        n_samples_activity = data.shape[0]//windows_per_activity\n",
    "\n",
    "        tmp_train_data = np.zeros((0, 2048))\n",
    "        tmp_test_data = np.zeros((0, 2048))\n",
    "\n",
    "        for iter in range(windows_per_activity):\n",
    "            if verbose: print(f\"\\n-------- {iter} ---------\")\n",
    "            start_train_idx = 0 + (n_samples_activity) * iter\n",
    "            end_train_idx = start_train_idx + csi_per_sec * train_seconds\n",
    "            if verbose: print(f\"train idx {start_train_idx} to {end_train_idx}\")\n",
    "            tmp_train_data = np.append(tmp_train_data, data[start_train_idx : end_train_idx], axis=0)\n",
    "            \n",
    "            if verbose: print(\"train shape\", tmp_train_data.shape)\n",
    "\n",
    "            start_test_idx = end_train_idx + csi_per_sec * test_seconds\n",
    "            end_test_idx =  start_test_idx + csi_per_sec * ignore_seconds\n",
    "            if verbose: print(f\"ignoring idx from {end_train_idx} to {start_test_idx}\")\n",
    "\n",
    "            if verbose: print(f\"test idx {start_test_idx} to {end_test_idx}\")\n",
    "            tmp_test_data = np.append(tmp_test_data, data[start_test_idx : end_test_idx], axis=0)\n",
    "            \n",
    "            if verbose: print(\"test shape\", tmp_test_data.shape)\n",
    "\n",
    "        train_num_samples = tmp_train_data.shape[0]\n",
    "        tmp_train_data = tf.convert_to_tensor(tmp_train_data, dtype=tf.float32)\n",
    "        tmp_train_labels = tf.convert_to_tensor(activity_label * np.ones(train_num_samples - window_size), dtype=tf.int32)\n",
    "        tmp_train_indices = tf.convert_to_tensor(tf.range(train_index_offset, train_index_offset + train_num_samples - window_size), dtype=tf.int32)\n",
    "\n",
    "        test_num_samples = tmp_test_data.shape[0]\n",
    "        tmp_test_data = tf.convert_to_tensor(tmp_test_data, dtype=tf.float32)\n",
    "        tmp_test_labels = tf.convert_to_tensor(activity_label * np.ones(test_num_samples - window_size), dtype=tf.int32)\n",
    "        tmp_test_indices = tf.convert_to_tensor(tf.range(test_index_offset, test_index_offset + test_num_samples - window_size), dtype=tf.int32)\n",
    "\n",
    "        train_data = tf.concat([train_data, tmp_train_data], axis=0)\n",
    "        train_labels = tf.concat([train_labels, tmp_train_labels], axis=0)\n",
    "        train_indices = tf.concat([train_indices, tmp_train_indices], axis=0)\n",
    "\n",
    "        test_data = tf.concat([test_data, tmp_test_data], axis=0)\n",
    "        test_labels = tf.concat([test_labels, tmp_test_labels], axis=0)\n",
    "        test_indices = tf.concat([test_indices, tmp_test_indices], axis=0)\n",
    "\n",
    "        if verbose: print(train_data.shape, train_labels.shape)\n",
    "        if verbose: print(test_data.shape, test_labels.shape)\n",
    "\n",
    "    # Normalize the CSI dataset\n",
    "    if antennas == 1:\n",
    "        train_data = tf.math.divide(train_data, tf.math.reduce_max(train_data, axis=(0, 1)))\n",
    "        test_data = tf.math.divide(test_data, tf.math.reduce_max(test_data, axis=(0, 1)))\n",
    "    else:\n",
    "        train_data = tf.math.divide(train_data, tf.math.reduce_max(train_data, axis=(0, 1, 2)))\n",
    "        test_data = tf.math.divide(test_data, tf.math.reduce_max(test_data, axis=(0, 1, 2)))\n",
    "\n",
    "    train_data = CsiData(train_data, train_labels, train_indices, batch_size=batch_size, window_size=window_size, antennas=antennas)\n",
    "    test_data = CsiData(test_data, test_labels, test_indices, batch_size=batch_size, window_size=window_size, antennas=antennas)\n",
    "\n",
    "    return train_data, test_data\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf_keras.layers.Layer):\n",
    "    \"\"\"Takes a couple (z_mean, z_log_var) to draw a sample z from the latent space.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(encoder_inputs)\n",
    "    x = tf_keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(x)\n",
    "    x = tf_keras.layers.Conv2D(32, (2, 4), activation='relu', strides=(2, 4), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (2, 4), activation='relu', strides=(2, 4), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf_keras.Model):\n",
    "    def __init__(self, enc_input_shape=(450, 2048, 1), dec_input_shape=(9, 8, 32), latent_dim=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_csi_encoder(enc_input_shape, latent_dim)\n",
    "        self.decoder = create_csi_decoder(dec_input_shape, latent_dim, enc_input_shape[-1])\n",
    "        self.total_loss_tracker = tf_keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = tf_keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = tf_keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf_keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vae_encoder(vae, source):\n",
    "    #Use the VAE to process CSI data\n",
    "    z_data = np.zeros([0, 4])\n",
    "    z_labels = np.zeros([0])\n",
    "\n",
    "    for (data, labels) in source:\n",
    "        labels = tf.squeeze(labels)\n",
    "        z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "        z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "        z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "        z_labels = np.concatenate([z_labels, labels.numpy().ravel()], axis=0)\n",
    "        \n",
    "    return z_data, z_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./{folder_name}/' + 'cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = tf_keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)\n",
    "early_stopping_cb = tf_keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "csv_logger_cb = tf_keras.callbacks.CSVLogger(f'./{folder_name}/model_history_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components(data, target, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    #Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "\n",
    "    var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "    #finds PCs that explain 95% of the variance\n",
    "    num_components = np.argmax(var_cumulative > target) + 1\n",
    "    print(f\"Number of components explaining {target}% variance: \"+ str(num_components))\n",
    "\n",
    "    if plotGraph:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title('Cumulative Explained Variance explained by the components')\n",
    "        plt.ylabel('Cumulative Explained variance')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.axvline(x=num_components, color=\"r\", linestyle=\"--\")\n",
    "        plt.axhline(y=target, color=\"r\", linestyle=\"--\")\n",
    "        plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', linestyle='--')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'var_cumulative_x_component.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "\n",
    "    return num_components\n",
    "\n",
    "def analyze_PCA(data, n_components, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    if (plotGraph):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "        plt.title('Cumulative Explained Variance by PCA Components')\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'cumulative_explained_variance.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "    \n",
    "    return reduced_df, pca\n",
    "\n",
    "def reconstruct_data(df, pca, columns):\n",
    "    df_reconstructed = pca.inverse_transform(df.values)\n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=columns)    \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    clusters = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (clusters[:-1] + clusters[1:]) / 2 #Defines intervals of clusters\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
    "        \n",
    "        empty_clusters = np.isnan(new_clusters) #Restore previous cluster if empty\n",
    "        new_clusters[empty_clusters] = clusters[empty_clusters] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_clusters - clusters)) < delta:\n",
    "            break\n",
    "\n",
    "        clusters = new_clusters\n",
    "\n",
    "    #Quantize the data based on the final clusters\n",
    "    quantized_data = clusters[indices]\n",
    "\n",
    "    return quantized_data, clusters, thresholds\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, clusters, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return clusters[indices]\n",
    "\n",
    "def apply_quantization(reduced_df, lvls):\n",
    "    df_quantized = reduced_df.apply(lambda col: lloyd_max_quantization(col.values, num_levels=lvls)[0])\n",
    "    return df_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bits_needed(source, verbose=True):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    bits_needed_window = {}\n",
    "    total_bits = 0\n",
    "    total_symbols = 0\n",
    "    \n",
    "    for index in range(0, len(data), window_size):\n",
    "        bits_needed = {}\n",
    "        data_window = data.iloc[index : index + window_size] \n",
    "        window_total_bits = 0\n",
    "        window_total_symbols = 0\n",
    "        \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            total_num_symbols = len(data_window[col])\n",
    "            \n",
    "            if num_symbols > 1:\n",
    "                bits_needed[col] = np.ceil(np.log2(num_symbols)).astype(int)  # Number of bits to represent each symbol\n",
    "            else:\n",
    "                bits_needed[col] = 1  # If only one unique symbol\n",
    "            if verbose: print(f\"Column: {col}, Bits needed: {bits_needed[col]} bits\")\n",
    "            \n",
    "            # bits this column in the window\n",
    "            column_bits = bits_needed[col] * total_num_symbols\n",
    "            window_total_bits += column_bits\n",
    "            window_total_symbols += total_num_symbols\n",
    "\n",
    "        bits_needed_window[index] = window_total_bits\n",
    "        if verbose: print(f\"Window: {index}, Average bits needed: {window_total_bits:.2f} bits\")\n",
    "    \n",
    "        total_bits += window_total_bits\n",
    "        total_symbols += window_total_symbols\n",
    "\n",
    "    average_bits_per_symbol = total_bits / total_symbols if total_symbols > 0 else 0\n",
    "    average_bits_per_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    #print(f\"\\nGlobal metrics:\")\n",
    "    print(f\"Average bits per symbol: {average_bits_per_symbol:.2f} bits\")\n",
    "    print(f\"Average bits per window: {average_bits_per_window:.2f} bits\")\n",
    "    print(f\"Bits for the whole dataset: {total_bits:.2f} bits\")\n",
    "\n",
    "    return average_bits_per_symbol.round(2), average_bits_per_window, total_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_needed(source, num_lvls=-1):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    num_features = len(data.columns)\n",
    "    bits_needed_unique = {}\n",
    "    avg_bits_needed = {}\n",
    "    bits_needed_window = {}\n",
    "    total_bits_needed_dataset = 0\n",
    "\n",
    "    for index in range(0, len(data), window_size):\n",
    "        data_window = data.iloc[index : index + window_size] \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            if num_lvls > 0:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_lvls)).astype(int)\n",
    "                #print(f\"Column: {col}, Bits needed: {bits_needed_unique[col]} bits (num levels: {num_lvls})\")\n",
    "            else:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_symbols)).astype(int)\n",
    "                \n",
    "        avg_bits_needed[index] = np.mean(list(bits_needed_unique.values())).round(2)\n",
    "        bits_needed_window[index] = sum(bits_needed_unique.values())\n",
    "        total_bits_needed_dataset += sum(bits_needed_unique.values())\n",
    "\n",
    "    bits_needed = np.mean(list(avg_bits_needed.values())).round(2)\n",
    "    bits_needed_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    return bits_needed, bits_needed_window, total_bits_needed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "\n",
    "The VAE has been trained without any information about the target classes; it just tries to minimize reconstruction loss + KL loss.\n",
    "\n",
    "The Encoder in the VAE maps sequences of CSI into **2 Gaussian variables** with parameters (z_mean, z_log_var).\n",
    "\n",
    "More in detail, from the dataset we load `data` and `labels`.\n",
    "- `data`: every element is a 4-tuple with the values (z1_mean, z2_mean, z1_log_var, z2_log_var)\n",
    "- `labels`: 5 different classes, labelled with integers from 0 to 4 (0 = walk, 1 = run, 2 = jump, 3 = sit, 4 = empty)\n",
    "\n",
    "Available datasets:\n",
    "- `single_antenna`: data of just antenna 1, normalized wrt to the maximum value over the entire dataset (four antennas are available, numbered from 0 to 3)\n",
    "- `four_antennas`: data of the four antennas fused together, normalized wrt to the maximum value over the entire dataset\n",
    "- `four_antennas_latent_space_3`: same as `four_antennas`, but the CSI is mapped onto 3 Gaussian variables; hence, every element in `data` is a 6-tuple with the values (z1_mean, z2_mean, z3_mean, z1_log_var, z2_log_var, z3_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_classes = [\"Walk\", \"Run\", \"Jump\", \"Sit\", \"Empty\"]\n",
    "base_directory = './results'\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(directory, scaler=None):\n",
    "    data = None\n",
    "    labels = None\n",
    "    \n",
    "    # features columns\n",
    "    fcolumns = ['mu1','mu2','sigma1','sigma2']\n",
    "    \n",
    "    # check which experiments we wants to load\n",
    "    with open(directory, 'rb') as f:\n",
    "        data, labels = pickle.load(f)\n",
    "\n",
    "    # labels are categoricals\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    \n",
    "    # let's load into a dataframe\n",
    "    df = pd.DataFrame(data, columns=fcolumns)\n",
    "    df['signal'] = labels\n",
    "    \n",
    "    if scaler is None:\n",
    "        # Fit scaler on training data\n",
    "        scaler = StandardScaler().fit(df[fcolumns])\n",
    "    df[fcolumns] = scaler.transform(df[fcolumns])\n",
    "    \n",
    "    X = df[fcolumns]\n",
    "    y = df['signal']\n",
    "\n",
    "    # one-hot-encoding\n",
    "    y_dummy = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, y_dummy, scaler, fcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_reconstructed(directory, _test_size = 0.2, _random_state = 42):\n",
    "    data = None\n",
    "    labels = None\n",
    "    \n",
    "    # features columns\n",
    "    fcolumns = ['mu1','mu2','sigma1','sigma2']\n",
    "    \n",
    "    # check which experiments we wants to load\n",
    "    with open(directory, 'rb') as f:\n",
    "        data, labels = pickle.load(f)\n",
    "\n",
    "    # labels are categoricals\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    \n",
    "    # let's load into a dataframe\n",
    "    df = pd.DataFrame(data, columns=fcolumns)\n",
    "    df['signal'] = labels\n",
    "    \n",
    "    # standard scaler\n",
    "    scaler = StandardScaler().fit(df[fcolumns])\n",
    "    df[fcolumns] = scaler.transform(df[fcolumns])\n",
    "    \n",
    "    # test/train split \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[fcolumns], df['signal'], \n",
    "                                                        test_size=_test_size, \n",
    "                                                        random_state=_random_state, \n",
    "                                                        stratify=df['signal'])\n",
    "    \n",
    "    # one-hot-encoding\n",
    "    y_train_dummy = keras.utils.to_categorical(y_train)\n",
    "    y_test_dummy = keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, y_train_dummy, y_test_dummy, scaler, df, fcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_test2(dump_directory, num_components=0, num_levels=0):\n",
    "    X_train, X_test, y_train, y_test, y_train_dummy, y_test_dummy, scaler, df, fcolumns = load_experiment_reconstructed(dump_directory)\n",
    "    model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    mlp_edl = keras.models.load_model(model_directory, compile=False)\n",
    "    mlp_edl_scores = np.array([res_to_mean(r, dim=5) for r in mlp_edl.predict(X_test)])\n",
    "    y_predictions_edl = np.array(tf.argmax(mlp_edl.predict(X_test), axis=1))\n",
    "    print(summary_clf(y_test, y_predictions_edl, mlp_edl_scores))\n",
    "    accuracy = accuracy_score(y_test, y_predictions_edl)\n",
    "    cm = confusion_matrix(y_test, y_predictions_edl)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=semantic_classes)\n",
    "    cmdisp = disp.plot(cmap=\"cividis\")\n",
    "    CM_directory = os.path.join(base_directory, f'{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    os.makedirs(os.path.dirname(CM_directory), exist_ok=True)\n",
    "    cmdisp.figure_.savefig(CM_directory, bbox_inches='tight')\n",
    "    return round(accuracy, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_annealing = 1\n",
    "num_classes = 5\n",
    "\n",
    "ep = 1.0\n",
    "class GetEpochs(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global ep\n",
    "        ep += 1\n",
    "\n",
    "def res_to_mean(ev, dim = 5):\n",
    "    return np.max(dirichlet.mean(ev.reshape(dim,)+1))\n",
    "\n",
    "def res_to_dirichlet(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    S = np.sum(alpha)\n",
    "    K = 2.0\n",
    "    return dirichlet.mean(alpha), K/S\n",
    "\n",
    "def edl_accuracy(yTrue, yPred):\n",
    "    pred = K.argmax(yPred, axis=1)\n",
    "    truth = K.argmax(yTrue, axis=1)\n",
    "    match = K.reshape(K.cast(K.equal(pred, truth), \"float32\"),(-1,1))\n",
    "    return K.mean(match)\n",
    "\n",
    "def load_edl_experiment(name):\n",
    "    keras.models.load_model(name)\n",
    "\n",
    "def plot_res_beta(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    plt.figure(figsize=(16,9))\n",
    "    x = np.linspace(0,1,1000)\n",
    "    plt.plot(x, beta.pdf(x, alpha[1], alpha[0]))\n",
    "    x1, x2 = beta.interval(0.95, alpha[1], alpha[0])\n",
    "    areaplot = np.multiply(beta.pdf(x, alpha[1],alpha[0]), rect(x,x1, x2))\n",
    "    plt.fill_between(x, 0, areaplot, alpha=0.5)\n",
    "\n",
    "def results_test (train_dir, test_dir, num_components=0, num_levels=0):\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dir, scaler)\n",
    "    model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    \n",
    "    mlp_edl = keras.models.load_model(model_directory, compile=False)\n",
    "    mlp_edl_scores = np.array([res_to_mean(r, dim=5) for r in mlp_edl.predict(X_test)])\n",
    "    y_predictions_edl = np.array(tf.argmax(mlp_edl.predict(X_test), axis=1))\n",
    "\n",
    "    print(summary_clf(y_test, y_predictions_edl, mlp_edl_scores))\n",
    "    accuracy = accuracy_score(y_test, y_predictions_edl)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predictions_edl)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=semantic_classes)\n",
    "    cmdisp = disp.plot(cmap=\"cividis\")\n",
    "    CM_directory = os.path.join(base_directory, f'{num_components}_components/CMs/{num_components}components_{num_levels}lvls_ConfusionMatrix.png')\n",
    "    os.makedirs(os.path.dirname(CM_directory), exist_ok=True)\n",
    "    cmdisp.figure_.savefig(CM_directory, bbox_inches='tight')\n",
    "\n",
    "    return round(accuracy, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edl_experiment(name, _X_train, _y_train_dummy, num_components=0, num_levels=0):\n",
    "\n",
    "    model_edl = None\n",
    "    num_classes = 5\n",
    "    \n",
    "    if name == \"Delayed-Fusing\":\n",
    "        num_epochs_annealing = 3\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(16,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5, activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.001\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    elif name == \"Early-Fusing3\":\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 50\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu', input_shape=(6,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 64\n",
    "        lr = 0.0001\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.5))\n",
    "        model_edl.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model_edl.add(tf.keras.layers.Dropout(0.5))\n",
    "        model_edl.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "        model_edl.add(tf.keras.layers.Dense(5, activation='softmax'))\n",
    "        \"\"\"\n",
    "        num_epochs_annealing = 22\n",
    "        batch_size = 128\n",
    "        lr = 0.01\n",
    "        epochs = 100\n",
    "        model_edl = tf.keras.models.Sequential()\n",
    "        model_edl.add(tf.keras.layers.Dense(4, activation='relu', input_shape=(4,)))\n",
    "        model_edl.add(tf.keras.layers.Dense(8, activation='relu'))\n",
    "        model_edl.add(tf.keras.layers.Dense(units=5,activation='softplus'))\n",
    "        \n",
    "\n",
    "\n",
    "    def KL(alpha):\n",
    "        beta=K.constant(np.ones((1,num_classes)),dtype=\"float32\")\n",
    "        S_alpha = K.sum(alpha,axis=1,keepdims=True)\n",
    "        S_beta = K.sum(beta,axis=1,keepdims=True)\n",
    "        lnB = tf.math.lgamma(S_alpha) - K.sum(tf.math.lgamma(alpha),axis=1,keepdims=True)\n",
    "        lnB_uni = K.sum(tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n",
    "\n",
    "        dg0 = tf.math.digamma(S_alpha)\n",
    "        dg1 = tf.math.digamma(alpha)\n",
    "\n",
    "        return K.sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "\n",
    "    # Loss function considering the expected squared error and the KL divergence\n",
    "    def mse_loss(yTrue,yPred):\n",
    "        alpha = yPred + 1\n",
    "        S = K.sum(alpha, axis=1, keepdims=True)\n",
    "        m = alpha / S\n",
    "\n",
    "        # A + B minimises the sum of squared loss, see discussion in EDL paper for the derivation\n",
    "        A = K.sum((yTrue-m)**2, axis=1, keepdims=True)\n",
    "        B = K.sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
    "\n",
    "        # the lambda_t parameter, in this case min{1, t/10} with t the number of epochs\n",
    "        ll = min(1.0, float(ep/float(num_epochs_annealing)))\n",
    "        \n",
    "        alp = yPred*(1-yTrue) + 1 \n",
    "        C =  ll * KL(alp)\n",
    "\n",
    "        return A + B + C\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model_edl.compile(loss=mse_loss, optimizer=optimizer, metrics=[edl_accuracy])\n",
    "\n",
    "    model_edl.fit(_X_train, _y_train_dummy,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    model_directory = os.path.join(base_directory, f'{num_components}_components/models/{num_components}components_{num_levels}lvls_Keras_Model.keras')\n",
    "    os.makedirs(os.path.dirname(model_directory), exist_ok=True)\n",
    "    model_edl.save(model_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 41, 5)) + list(range(50, 101, 10))\n",
    "levels = [2**i for i in range(1, 8)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_generator = CsiDataGenerator(file_list, batch_size=BATCH_SIZE, antenna_select=antenna)\n",
    "\n",
    "csi_data = csi_generator.csi.numpy()\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "df_csi_data_original = pd.DataFrame(csi_data, columns=csi_subcarriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    directory = f'./results/{num_components}_components/dumps'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    df_csi_data = df_csi_data_original.copy()\n",
    "    #Apply PCA\n",
    "    \n",
    "    df_reduced, pca = analyze_PCA(df_csi_data, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        df_train_quantized = apply_quantization(df_reduced, num_levels)\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed)\n",
    "        csi_generator.csi = reconstructed_train_data\n",
    "\n",
    "        #Use the VAE to process CSI data\n",
    "        z_data = np.zeros([0, 4])\n",
    "        z_labels = np.zeros([0])\n",
    "\n",
    "        vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "        vae.compile(optimizer=keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./{folder_name}/weights_vae').expect_partial()\n",
    "\n",
    "        for (data, labels) in csi_generator:\n",
    "            labels = tf.squeeze(labels)\n",
    "            z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "            z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "            z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "            z_labels = np.concatenate([z_labels, labels], axis=0)\n",
    "\n",
    "        # Store the latent space representation of CSI data to file.\n",
    "        sub_dir=os.path.join(directory, f'{num_components}components_{num_levels}lvls_single_antenna_{antenna}.pkl')\n",
    "        with open(sub_dir, 'wb') as f:\n",
    "            pickle.dump([z_data, z_labels], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for num_components in components:\n",
    "    for num_levels in levels:\n",
    "        print(f\"------------------------- Running experiment for {num_components} components with {num_levels} levels -------------------------\")\n",
    "        dump_directory =  os.path.join(base_directory, f'{num_components}_components/dumps/{num_components}components_{num_levels}lvls_single_antenna_{antenna}.pkl')\n",
    "        \n",
    "        # Load data\n",
    "        X_train, X_test, y_train, y_test, y_train_dummy, y_test_dummy, scaler, df, fcolumns = load_experiment_reconstructed(dump_directory)\n",
    "        \n",
    "        # Run model\n",
    "        name = \"No-Fused-1\"\n",
    "        run_edl_experiment(name, num_components, num_levels, X_train, y_train_dummy)\n",
    "\n",
    "        # Test model\n",
    "        accuracy = results_test(num_components, num_levels, dump_directory)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('results_csv/results2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/bit_results_single_antenna_0.csv')\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\n",
    "components = [1, 2, 3, 4, 10]\n",
    "#components = [30, 35, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    #if num_components not in components:\n",
    "    #    continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.axhline(y=0.95, color=\"r\", linestyle=\":\", label=\"max accuracy\")\n",
    "plt.title('Accuracy and Bits Comparison (quantized data)')\n",
    "plt.xlabel('Bits per symbol')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('accuracy_bit_comparison[BxS][1-100].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/bit_results_single_antenna_0.csv')\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\n",
    "components = [1, 2, 3, 4, 10]\n",
    "#components = [30, 35, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_win_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_win_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE')\n",
    "plt.axhline(y=0.95, color=\"r\", linestyle=\":\", label=\"max accuracy\")\n",
    "plt.title('Accuracy and Bits Comparison (quantized data)')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('accuracy_bit_comparison[BxW][1-100].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 41, 5))\n",
    "components = (range(50, 101, 10))\n",
    "levels = [2**i for i in range(1, 8)]\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_generator = CsiDataGenerator(file_list, batch_size=BATCH_SIZE, antenna_select=antenna)\n",
    "\n",
    "csi_data = csi_generator.csi.numpy()\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "df_csi_data_original = pd.DataFrame(csi_data, columns=csi_subcarriers)\n",
    "\n",
    "OG_bits, OG_win_bits,total_OG_bits = bits_needed(df_csi_data_original)\n",
    "print(f\"Bits needed: {OG_bits} bits\")\n",
    "print(f\"AvgBits needed per window: {OG_win_bits} bits\")\n",
    "print(f\"Total Bits needed: {total_OG_bits} bits\")\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    directory = f'./results/{num_components}_components/dumps'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    df_csi_data = df_csi_data_original.copy()\n",
    "\n",
    "    #Apply PCA\n",
    "    df_reduced, pca = analyze_PCA(df_csi_data, num_components, directory=directory, plotGraph=False)\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    #PCA_avg_bits_per_symbol, PCA_avg_bits_per_window, PCA_total_bits = compute_bits_needed(df_reduced, verbose=False)\n",
    "    PCA_bits, PCA_win_bits,total_PCA_bits = bits_needed(df_reduced)\n",
    "    print(f\"Bits needed: {PCA_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {PCA_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_PCA_bits} bits\")\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        df_train_quantized = apply_quantization(df_reduced, num_levels)\n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        #QT_avg_bits_per_symbol, QT_avg_bits_per_window, QT_total_bits = compute_bits_needed(df_quantized, verbose=False)\n",
    "        QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_train_quantized, num_levels)\n",
    "        print(f\"Bits needed: {QT_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        #REC_avg_bits_per_symbol, REC_avg_bits_per_window, REC_total_bits = compute_bits_needed(df_reconstructed, verbose=False)\n",
    "        REC_bits, REC_win_bits, total_REC_bits = bits_needed(df_train_reconstructed)\n",
    "        print(f\"Bits needed: {REC_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {REC_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_REC_bits} bits\")\n",
    "        \n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed)\n",
    "        csi_generator.csi = reconstructed_train_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'num_components': num_components,\n",
    "            'num_levels': num_levels,\n",
    "            'OG_bits': OG_bits,\n",
    "            'PCA_bits': PCA_bits,\n",
    "            'QT_bits': QT_bits,\n",
    "            'REC_bits': REC_bits,\n",
    "            'OG_win_bits': OG_win_bits,\n",
    "            'PCA_win_bits': PCA_win_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'REC_win_bits': REC_win_bits,\n",
    "            'total_OG_bits': total_OG_bits,\n",
    "            'total_PCA_bits': total_PCA_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "            'total_REC_bits': total_REC_bits\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/bit_results_single_antenna2_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Output Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_generator = CsiDataGenerator(file_list, batch_size=BATCH_SIZE, antenna_select=antenna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VAE to process CSI data\n",
    "z_data = np.zeros([0, 4])\n",
    "z_labels = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/weights_vae').expect_partial()\n",
    "\n",
    "for (data, labels) in csi_generator:\n",
    "    labels = tf.squeeze(labels)\n",
    "    z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "    z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "    z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "    z_labels = np.concatenate([z_labels, labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./results/0_components/dumps'\n",
    "bit_results = []\n",
    "\n",
    "df_z_data = pd.DataFrame(z_data, columns=[f'z_mean_{i}' for i in range(2)] + [f'z_log_var_{i}' for i in range(2)])\n",
    "\n",
    "for lvl in levels:\n",
    "    print(f\"-------------- {lvl} lvls --------------\")\n",
    "    sub_dir=os.path.join(directory, f'{lvl}lvls_single_antenna_{antenna}.pkl')\n",
    "\n",
    "    df_train_quantized = apply_quantization(df_z_data, lvl)\n",
    "    print (f\"DF_QUANTIZED\")\n",
    "        #QT_avg_bits_per_symbol, QT_avg_bits_per_window, QT_total_bits = compute_bits_needed(df_quantized, verbose=False)\n",
    "    QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_train_quantized, lvl)\n",
    "    print(f\"Bits needed: {QT_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "    z_data = df_train_quantized.to_numpy()\n",
    "\n",
    "#    with open(sub_dir, 'wb') as f:\n",
    "#       pickle.dump([z_data, z_labels], f)\n",
    "\n",
    "    bit_results.append({\n",
    "            'num_levels': lvl,\n",
    "            'QT_bits': QT_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results/csv/VAE_bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Comprehenisve Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 41, 5)) + list(range(50, 101, 10))\n",
    "components = [1, 2, 3, 4, 10, 20, 30, 40, 50, 100]\n",
    "levels = [2**i for i in range(1, 8)]\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    directory = f'./dumps/{num_components}_components'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}lvls_single_antenna_{antenna}'\n",
    "        #Quantize the data\n",
    "        df_train_quantized = apply_quantization(df_train_reduced, num_levels)\n",
    "        df_test_quantized = apply_quantization(df_test_reduced, num_levels)\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "        train_data.csi = reconstructed_train_data\n",
    "\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "        vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "        \n",
    "        print(\"Encoding train data...\")\n",
    "        z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "        \n",
    "        print(\"Encoding test data...\")\n",
    "        z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "        train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "        os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "        os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "        print(\"Saving data...\")\n",
    "        with open(train_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_train, z_labels_train], f)\n",
    "        with open(test_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_components in components:\n",
    "    directory = f'./dumps/{num_components}_components'\n",
    "    for num_levels in levels:  \n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "        test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "\n",
    "        X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "        X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler)\n",
    "\n",
    "        name = \"No-Fused-1\"\n",
    "        run_edl_experiment(name, num_components, num_levels, X_train, y_train_dummy)\n",
    "\n",
    "        # Test model\n",
    "        accuracy = results_test(num_components, num_levels, train_dump_dir, test_dump_dir)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"accuracy\": accuracy\n",
    "            })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/bit_results_single_antenna_0.csv')\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\n",
    "components = [1, 2, 3, 4, 10]\n",
    "#components = [30, 35, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    #if num_components not in components:\n",
    "    #    continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE', linewidth=3)\n",
    "plt.axhline(y=0.95, color=\"r\", linestyle=\":\", label=\"max accuracy\")\n",
    "plt.title('Accuracy and Bits Comparison (quantized data)')\n",
    "plt.xlabel('Bits per symbol')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.savefig(os.path.join('accuracy_bit_comparison[BxS][1-100].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accuracy = pd.read_csv('results_csv/results.csv')\n",
    "df_bits = pd.read_csv('results_csv/bit_results_single_antenna_0.csv')\n",
    "df_VAE_accuracy = pd.read_csv('results_csv/VAE_results.csv')\n",
    "df_VAE_bits = pd.read_csv('results_csv/VAE_bit_results_single_antenna_0.csv')\n",
    "\n",
    "df_acc_bit = pd.merge(df_accuracy, df_bits, on=['num_components', 'num_levels'])\n",
    "df_VAE_acc_bit = pd.merge(df_VAE_accuracy, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25]\n",
    "components = [1, 2, 3, 4, 10]\n",
    "#components = [30, 35, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_acc_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_acc_bit[df_acc_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_win_bits'], target_data['accuracy'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_acc_bit['QT_win_bits'], df_VAE_acc_bit['accuracy'], marker='o', label='Post VAE')\n",
    "plt.axhline(y=0.95, color=\"r\", linestyle=\":\", label=\"max accuracy\")\n",
    "plt.title('Accuracy and Bits Comparison (quantized data)')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.savefig(os.path.join('accuracy_bit_comparison[BxW][1-100].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "train_data, test_data = load_split_train_test_CSI_data(file_list, batch_size=BATCH_SIZE, antennas=ANTENNAS, verbose=False)\n",
    "\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_models = True\n",
    "\n",
    "if load_pretrained_models:\n",
    "    print('Loading pretrained models...')\n",
    "    !wget https://zenodo.org/record/7983057/files/VAE_models.zip\n",
    "    !unzip -o VAE_models.zip\n",
    "    !rm VAE_models.zip\n",
    "else:\n",
    "    # Train from scratch\n",
    "    #!mkdir {folder_name}\n",
    "    vae = VAE()\n",
    "    vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "    vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "    vae.fit(train_data, epochs=15, shuffle=False, callbacks=[checkpoint_cb, csv_logger_cb])\n",
    "    vae.save_weights(f'./{folder_name}/train_weights_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "folder_name = f'models/single_antenna_{antenna}'\n",
    "\n",
    "print(f\"-------------- 0 components --------------\")\n",
    "directory = './dumps/0_components'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "filename = f'old_0lvls_single_antenna_{antenna}'\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "print(\"Encoding train data...\")\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "\n",
    "print(\"Encoding test data...\")\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_train, z_labels_train], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler)\n",
    "\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/0_components'\n",
    "filename = f'old_0lvls_single_antenna_{antenna}'\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler)\n",
    "\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/0_components/vae_single_antenna_0.pkl'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "with open(directory, 'rb') as f:\n",
    "    vae_data, vae_labels = pickle.load(f)\n",
    "\n",
    "n_samples_activity = vae_data.shape[0]//5\n",
    "file_number = list(range(5))\n",
    "print(\"vae shapes: \", vae_data.shape, vae_labels.shape)\n",
    "print(\"num samples per activity: \", n_samples_activity)\n",
    "print(\"num samples per window (for an activity): \", n_samples_activity//5)\n",
    "print(\"seconds per window: \", (n_samples_activity//5)/150)\n",
    "train_data = np.zeros([0, 4])\n",
    "train_labels = np.zeros([0])\n",
    "test_data = np.zeros([0, 4])\n",
    "test_labels = np.zeros([0])\n",
    "\n",
    "for file_number in range(5):\n",
    "    print(f\"\\n*************** activity {file_number} ***************\")\n",
    "    first_idx = n_samples_activity * file_number\n",
    "    last_idx = first_idx + n_samples_activity\n",
    "\n",
    "    #print(type(vae_data), type(vae_labels))\n",
    "    print(f\"first index: {first_idx} last index: {last_idx}\")\n",
    "    print(f\"first label: {vae_labels[first_idx]} last label: {vae_labels[last_idx-1]}\")\n",
    "\n",
    "\n",
    "    data = vae_data[first_idx : last_idx]\n",
    "    labels = vae_labels[first_idx : last_idx]\n",
    "    iter = 0\n",
    "\n",
    "    for iter in range(5):\n",
    "        print(f\"\\n-------- {iter} ---------\")\n",
    "        start_train_idx = 0 + (n_samples_activity//5) * iter\n",
    "        end_train_idx = start_train_idx + 150 * 9\n",
    "        print(f\"train idx {start_train_idx} to {end_train_idx}\")\n",
    "        train = data[start_train_idx : end_train_idx]\n",
    "        train_label = labels[start_train_idx : end_train_idx]\n",
    "        print(\"train shape\", train.shape)\n",
    "\n",
    "        start_test_idx = end_train_idx + 150 * 3\n",
    "        end_test_idx =  start_test_idx + 150 * 1\n",
    "        print(f\"ignoring idx from {end_train_idx} to {start_test_idx}\")\n",
    "\n",
    "        print(f\"test idx {start_test_idx} to {end_test_idx}\")\n",
    "        test = data[start_test_idx : end_test_idx]\n",
    "        test_label = labels[start_test_idx : end_test_idx]\n",
    "        print(\"test shape\", test.shape)\n",
    "\n",
    "        train_data = np.append(train_data, train, axis=0)\n",
    "        train_labels = np.append(train_labels, train_label, axis=0)\n",
    "        test_data = np.append(test_data, test, axis=0)\n",
    "        test_labels = np.append(test_labels, test_label, axis=0)\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "print(train_labels.shape, test_labels.shape)\n",
    "\n",
    "directory = './dumps/0_components'\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([train_data, train_labels], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([test_data, test_labels], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/0_components'\n",
    "filename = f'0lvls_single_antenna_{antenna}'\n",
    "train_dump_dir=os.path.join(directory, f'training/{filename}.pkl')\n",
    "test_dump_dir=os.path.join(directory, f'test/{filename}_test.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler)\n",
    "\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './dumps/0_components/vae_single_antenna_0.pkl'\n",
    "\n",
    "X_train, X_test, y_train, y_test, y_train_dummy, y_test_dummy, scaler, df, fcolumns = load_experiment_reconstructed(directory)\n",
    "name = \"No-Fused-1\"\n",
    "run_edl_experiment(name, X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "accuracy = results_test2(directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
