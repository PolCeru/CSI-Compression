{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training and processing\n",
    "\n",
    "Sample code to train a new VAE and run the CSI processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_USE_LEGACY_KERAS\"]=\"1\"\n",
    "ANTENNAS = 1\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "antenna = 0  # if ANTENNAS==1, this value selects the antenna ID (from 0 to 3)\n",
    "latent_dim = 2\n",
    "num_activities = 5\n",
    "folder_name = f'models/single_antenna_{antenna}'\n",
    "\n",
    "base_directory = './models'\n",
    "saveGraph = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSI data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsiDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, files, num_samples=12000, window_size=450, antennas=1, batch_size=25, antenna_select=0):\n",
    "        if antennas == 1:\n",
    "            self.csi = tf.zeros([0, 2048], dtype=tf.float32)\n",
    "        else:\n",
    "            self.csi = tf.zeros([0, 2048, antennas], dtype=tf.float32)\n",
    "\n",
    "        self.labels = tf.zeros([0], dtype=tf.int32)\n",
    "        self.indices = tf.zeros([0], dtype=tf.int32)\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.antennas = antennas\n",
    "\n",
    "        for file in files:\n",
    "            # Load CSI data from MATLAB file\n",
    "            mat = sio.loadmat(file)      # WARNING This code does not handle exceptions for simplicity...\n",
    "            data = np.array(mat['csi'])  # ...exceptions would require keeping track of indices\n",
    "            if self.antennas == 1:\n",
    "                data = data[range(num_samples), ..., int(antenna_select)]\n",
    "            data = np.round(np.abs(data))\n",
    "            index_offset = self.csi.shape[0]\n",
    "            activity_label = files.index(file)  # Labels depend on file index\n",
    "\n",
    "            # Cast CSI data into temporary TF tensors for building the dataset\n",
    "            csi = tf.convert_to_tensor(data, dtype=tf.float32)\n",
    "            labels = tf.convert_to_tensor(activity_label * np.ones(num_samples - window_size), dtype=tf.int32)\n",
    "            indices = tf.convert_to_tensor(tf.range(index_offset, index_offset + num_samples - window_size),\n",
    "                                           dtype=tf.int32)\n",
    "\n",
    "            # Concatenate to the previous tensors\n",
    "            self.csi = tf.concat([self.csi, csi], axis=0)\n",
    "            self.labels = tf.concat([self.labels, labels], axis=0)\n",
    "            self.indices = tf.concat([self.indices, indices], axis=0)\n",
    "\n",
    "        # Normalize the CSI dataset\n",
    "        if self.antennas == 1:\n",
    "            self.csi = tf.math.divide(self.csi, tf.math.reduce_max(self.csi, axis=(0, 1)))\n",
    "        else:\n",
    "            self.csi = tf.math.divide(self.csi, tf.math.reduce_max(self.csi, axis=(0, 1, 2)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.indices.shape[-1] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        first_idx = batch_idx * self.batch_size\n",
    "        last_idx = (batch_idx + 1) * self.batch_size\n",
    "\n",
    "        data_batch = [self.csi[x:x + self.window_size, ...] for x in range(first_idx, last_idx)]\n",
    "        labels_batch = np.transpose([self.labels[first_idx:last_idx]])\n",
    "\n",
    "        data_batch = tf.convert_to_tensor(data_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "\n",
    "        if self.antennas == 1:\n",
    "            data_batch = tf.expand_dims(data_batch, 3)\n",
    "            labels_batch = tf.expand_dims(labels_batch, 2)\n",
    "\n",
    "        return data_batch, labels_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    \"\"\"Takes a couple (z_mean, z_log_var) to draw a sample z from the latent space.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    x = keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(encoder_inputs)\n",
    "    x = keras.layers.Conv2D(32, (5, 8), activation='relu', strides=(5, 8), padding='valid')(x)\n",
    "    x = keras.layers.Conv2D(32, (2, 4), activation='relu', strides=(2, 4), padding='valid')(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(16, activation='relu')(x)\n",
    "\n",
    "    z_mean = keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = keras.layers.Reshape(input_shape)(x)\n",
    "    x = keras.layers.Conv2DTranspose(32, (2, 4), activation='relu', strides=(2, 4), padding='same')(x)\n",
    "    x = keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    x = keras.layers.Conv2DTranspose(32, (5, 8), activation='relu', strides=(5, 8), padding='same')(x)\n",
    "    decoder_outputs = keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, enc_input_shape=(450, 2048, 1), dec_input_shape=(9, 8, 32), latent_dim=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_csi_encoder(enc_input_shape, latent_dim)\n",
    "        self.decoder = create_csi_decoder(dec_input_shape, latent_dim, enc_input_shape[-1])\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./{folder_name}/' + 'cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "csv_logger_cb = keras.callbacks.CSVLogger(f'./{folder_name}/model_history_log.csv', append=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the VAEs from scratch can take a very long time, so it is advised to load the pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained_models = True\n",
    "\n",
    "if load_pretrained_models:\n",
    "    print('Loading pretrained models...')\n",
    "    \"\"\"\n",
    "    !wget https://zenodo.org/record/7983057/files/VAE_models.zip\n",
    "    !unzip -o VAE_models.zip\n",
    "    !rm VAE_models.zip\n",
    "    \"\"\"\n",
    "else:\n",
    "    # Train from scratch\n",
    "    #!mkdir {folder_name}\n",
    "    vae = VAE()\n",
    "    vae.compile(optimizer=keras.optimizers.Adam())\n",
    "    vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "    vae.fit(csi_generator, epochs=20, shuffle=True,\n",
    "            callbacks=[checkpoint_cb, early_stopping_cb, csv_logger_cb])\n",
    "    vae.save_weights(f'./{folder_name}/weights_vae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components(data, target, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    #Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "\n",
    "    var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "    #finds PCs that explain 95% of the variance\n",
    "    num_components = np.argmax(var_cumulative > target) + 1\n",
    "    print(f\"Number of components explaining {target}% variance: \"+ str(num_components))\n",
    "\n",
    "    if plotGraph:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.title('Cumulative Explained Variance explained by the components')\n",
    "        plt.ylabel('Cumulative Explained variance')\n",
    "        plt.xlabel('Principal components')\n",
    "        plt.axvline(x=num_components, color=\"r\", linestyle=\"--\")\n",
    "        plt.axhline(y=target, color=\"r\", linestyle=\"--\")\n",
    "        plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', linestyle='--')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'var_cumulative_x_component.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "\n",
    "    return num_components\n",
    "\n",
    "def analyze_PCA(data, n_components, directory=base_directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    if (plotGraph):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "        plt.title('Cumulative Explained Variance by PCA Components')\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'cumulative_explained_variance.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "    \n",
    "    return reduced_df, pca\n",
    "\n",
    "def reconstruct_data(df, pca, columns):\n",
    "    df_reconstructed = pca.inverse_transform(df.values)\n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=columns)    \n",
    "    return df_reconstructed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    clusters = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (clusters[:-1] + clusters[1:]) / 2 #Defines intervals of clusters\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
    "        \n",
    "        empty_clusters = np.isnan(new_clusters) #Restore previous cluster if empty\n",
    "        new_clusters[empty_clusters] = clusters[empty_clusters] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_clusters - clusters)) < delta:\n",
    "            break\n",
    "\n",
    "        clusters = new_clusters\n",
    "\n",
    "    #Quantize the data based on the final clusters\n",
    "    quantized_data = clusters[indices]\n",
    "\n",
    "    return quantized_data, clusters, thresholds\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, clusters, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return clusters[indices]\n",
    "\n",
    "def apply_quantization(reduced_df, lvls):\n",
    "    df_quantized = reduced_df.apply(lambda col: lloyd_max_quantization(col.values, num_levels=lvls)[0])\n",
    "    return df_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bits_needed(source, verbose=True):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    bits_needed_window = {}\n",
    "    total_bits = 0\n",
    "    total_symbols = 0\n",
    "    \n",
    "    for index in range(0, len(data), window_size):\n",
    "        bits_needed = {}\n",
    "        data_window = data.iloc[index : index + window_size] \n",
    "        window_total_bits = 0\n",
    "        window_total_symbols = 0\n",
    "        \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            total_num_symbols = len(data_window[col])\n",
    "            \n",
    "            if num_symbols > 1:\n",
    "                bits_needed[col] = np.ceil(np.log2(num_symbols)).astype(int)  # Number of bits to represent each symbol\n",
    "            else:\n",
    "                bits_needed[col] = 1  # If only one unique symbol\n",
    "            if verbose: print(f\"Column: {col}, Bits needed: {bits_needed[col]} bits\")\n",
    "            \n",
    "            # bits this column in the window\n",
    "            column_bits = bits_needed[col] * total_num_symbols\n",
    "            window_total_bits += column_bits\n",
    "            window_total_symbols += total_num_symbols\n",
    "\n",
    "        bits_needed_window[index] = window_total_bits\n",
    "        if verbose: print(f\"Window: {index}, Average bits needed: {window_total_bits:.2f} bits\")\n",
    "    \n",
    "        total_bits += window_total_bits\n",
    "        total_symbols += window_total_symbols\n",
    "\n",
    "    average_bits_per_symbol = total_bits / total_symbols if total_symbols > 0 else 0\n",
    "    average_bits_per_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    #print(f\"\\nGlobal metrics:\")\n",
    "    print(f\"Average bits per symbol: {average_bits_per_symbol:.2f} bits\")\n",
    "    print(f\"Average bits per window: {average_bits_per_window:.2f} bits\")\n",
    "    print(f\"Bits for the whole dataset: {total_bits:.2f} bits\")\n",
    "\n",
    "    return average_bits_per_symbol.round(2), average_bits_per_window, total_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_needed(source, num_lvls=-1):\n",
    "    data = source.copy()\n",
    "    window_size = 450\n",
    "    num_features = len(data.columns)\n",
    "    bits_needed_unique = {}\n",
    "    avg_bits_needed = {}\n",
    "    bits_needed_window = {}\n",
    "    total_bits_needed_dataset = 0\n",
    "\n",
    "    for index in range(0, len(data), window_size):\n",
    "        data_window = data.iloc[index : index + window_size] \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            if num_lvls > 0:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_lvls)).astype(int)\n",
    "                #print(f\"Column: {col}, Bits needed: {bits_needed_unique[col]} bits (num levels: {num_lvls})\")\n",
    "            else:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_symbols)).astype(int)\n",
    "        avg_bits_needed[index] = np.mean(list(bits_needed_unique.values())).round(2)\n",
    "        bits_needed_window[index] = sum(bits_needed_unique.values())\n",
    "        total_bits_needed_dataset += sum(bits_needed_unique.values())\n",
    "\n",
    "    bits_needed = np.mean(list(avg_bits_needed.values())).round(2)\n",
    "    bits_needed_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    return bits_needed, bits_needed_window, total_bits_needed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 41, 5))\n",
    "levels = [2**i for i in range(1, 8)]\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_generator = CsiDataGenerator(file_list, batch_size=BATCH_SIZE, antenna_select=antenna)\n",
    "\n",
    "csi_data = csi_generator.csi.numpy()\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "df_csi_data_original = pd.DataFrame(csi_data, columns=csi_subcarriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    directory = f'./results/{num_components}_components/dumps'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    df_csi_data = df_csi_data_original.copy()\n",
    "    #Apply PCA\n",
    "    \n",
    "    df_reduced, pca = analyze_PCA(df_csi_data, num_components, directory=directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        df_quantized = apply_quantization(df_reduced, num_levels)\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_reconstructed = reconstruct_data(df_quantized, pca, csi_subcarriers)\n",
    "        df_reconstructed = df_reconstructed.to_numpy()\n",
    "        reconstructed_csi_data = tf.convert_to_tensor(df_reconstructed)\n",
    "        csi_generator.csi = reconstructed_csi_data\n",
    "\n",
    "        #Use the VAE to process CSI data\n",
    "        z_data = np.zeros([0, 4])\n",
    "        z_labels = np.zeros([0])\n",
    "\n",
    "        vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "        vae.compile(optimizer=keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./{folder_name}/weights_vae').expect_partial()\n",
    "\n",
    "        for (data, labels) in csi_generator:\n",
    "            labels = tf.squeeze(labels)\n",
    "            z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "            z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "            z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "            z_labels = np.concatenate([z_labels, labels], axis=0)\n",
    "\n",
    "        # Store the latent space representation of CSI data to file.\n",
    "        sub_dir=os.path.join(directory, f'{num_components}components_{num_levels}lvls_single_antenna_{antenna}.pkl')\n",
    "        with open(sub_dir, 'wb') as f:\n",
    "            pickle.dump([z_data, z_labels], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(range(1, 11)) + list(range(15, 41, 5))\n",
    "levels = [2**i for i in range(1, 8)]\n",
    "bit_results = []\n",
    "\n",
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_generator = CsiDataGenerator(file_list, batch_size=BATCH_SIZE, antenna_select=antenna)\n",
    "\n",
    "csi_data = csi_generator.csi.numpy()\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "df_csi_data_original = pd.DataFrame(csi_data, columns=csi_subcarriers)\n",
    "\n",
    "OG_bits, OG_win_bits,total_OG_bits = bits_needed(df_csi_data_original)\n",
    "print(f\"Bits needed: {OG_bits} bits\")\n",
    "print(f\"AvgBits needed per window: {OG_win_bits} bits\")\n",
    "print(f\"Total Bits needed: {total_OG_bits} bits\")\n",
    "\n",
    "for target in targets:\n",
    "    print(f\"-------------- {target}% ----------------------\")\n",
    "    directory = f'./results/{target}%/dumps'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    df_csi_data = df_csi_data_original.copy()\n",
    "\n",
    "    #Apply PCA\n",
    "    num_components = find_n_components(df_csi_data, target, directory=directory, plotGraph=False)\n",
    "    df_reduced, pca = analyze_PCA(df_csi_data, num_components, directory=directory, plotGraph=False)\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    #PCA_avg_bits_per_symbol, PCA_avg_bits_per_window, PCA_total_bits = compute_bits_needed(df_reduced, verbose=False)\n",
    "    PCA_bits, PCA_win_bits,total_PCA_bits = bits_needed(df_reduced)\n",
    "    print(f\"Bits needed: {PCA_bits} bits\")\n",
    "    print(f\"AvgBits needed per window: {PCA_win_bits} bits\")\n",
    "    print(f\"Total Bits needed: {total_PCA_bits} bits\")\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {target}% {num_levels} lvls --------------\")\n",
    "        #Quantize the data\n",
    "        df_quantized = apply_quantization(df_reduced, num_levels)\n",
    "        \n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        #QT_avg_bits_per_symbol, QT_avg_bits_per_window, QT_total_bits = compute_bits_needed(df_quantized, verbose=False)\n",
    "        QT_bits, QT_win_bits, total_QT_bits = bits_needed(df_quantized, num_levels)\n",
    "        print(f\"Bits needed: {QT_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {QT_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_QT_bits} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_reconstructed = reconstruct_data(df_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        #REC_avg_bits_per_symbol, REC_avg_bits_per_window, REC_total_bits = compute_bits_needed(df_reconstructed, verbose=False)\n",
    "        REC_bits, REC_win_bits, total_REC_bits = bits_needed(df_reconstructed)\n",
    "        print(f\"Bits needed: {REC_bits} bits\")\n",
    "        print(f\"AvgBits needed per window: {REC_win_bits} bits\")\n",
    "        print(f\"Total Bits needed: {total_REC_bits} bits\")\n",
    "        \n",
    "        df_reconstructed = df_reconstructed.to_numpy()\n",
    "        reconstructed_csi_data = tf.convert_to_tensor(df_reconstructed)\n",
    "        csi_generator.csi = reconstructed_csi_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'target': target,\n",
    "            'num_levels': num_levels,\n",
    "            'OG_bits': OG_bits,\n",
    "            'PCA_bits': PCA_bits,\n",
    "            'QT_bits': QT_bits,\n",
    "            'REC_bits': REC_bits,\n",
    "            'OG_win_bits': OG_win_bits,\n",
    "            'PCA_win_bits': PCA_win_bits,\n",
    "            'QT_win_bits': QT_win_bits,\n",
    "            'REC_win_bits': REC_win_bits,\n",
    "            'total_OG_bits': total_OG_bits,\n",
    "            'total_PCA_bits': total_PCA_bits,\n",
    "            'total_QT_bits': total_QT_bits,\n",
    "            'total_REC_bits': total_REC_bits\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./bit_results_single_antenna_{antenna}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [f'./dataset/S1a_{x}.mat' for x in string.ascii_uppercase[:num_activities]]\n",
    "csi_generator = CsiDataGenerator(file_list, batch_size=BATCH_SIZE, antenna_select=antenna)\n",
    "\n",
    "target = 90\n",
    "num_levels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_data = csi_generator.csi.numpy()\n",
    "csi_subcarriers = [f\"Ampl_{i}\" for i in range(1024)] + [f\"Ampl_{-i}\" for i in range(1, 1025)]\n",
    "\n",
    "df_csi_data = pd.DataFrame(csi_data, columns=csi_subcarriers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = find_n_components(df_csi_data, target, plotGraph=False)\n",
    "df_reduced, pca = analyze_PCA(df_csi_data, num_components, plotGraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quantized = apply_quantization(df_reduced, num_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reconstructed = reconstruct_data(df_quantized, pca, csi_subcarriers)\n",
    "\n",
    "df_reconstructed = df_reconstructed.to_numpy()\n",
    "reconstructed_csi_data = tf.convert_to_tensor(df_reconstructed)\n",
    "\n",
    "print('Original csi data shape:', df_csi_data.shape)\n",
    "print('PCA df shape:', df_reduced.shape)\n",
    "print('Reconstructed csi data shape:', reconstructed_csi_data.shape)\n",
    "\n",
    "csi_generator.csi = reconstructed_csi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the VAE to process CSI data\n",
    "z_data = np.zeros([0, 4])\n",
    "z_labels = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(450, 2048, ANTENNAS))\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.load_weights(f'./{folder_name}/weights_vae').expect_partial()\n",
    "\n",
    "for (data, labels) in csi_generator:\n",
    "    labels = tf.squeeze(labels)\n",
    "    z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "    z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "    z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "    z_labels = np.concatenate([z_labels, labels], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the latent space representation of CSI data to file.\n",
    "with open(f'./{folder_name}_reconstructed.pkl', 'wb') as f:\n",
    "    pickle.dump([z_data, z_labels], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aaaaaaaaaaaaaaaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
