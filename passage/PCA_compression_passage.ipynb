{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import cmath\n",
    "import warnings\n",
    "from enum import Enum\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compression_Method(Enum):\n",
    "    XY = 1               #applies PCA on X and Y then filters (1)\n",
    "    AmpPhase = 2         #applies PCA on Amplitude and Phase then filters (2)\n",
    "    AmpPhaseFiltered = 3 #applies PCA on Amplitude and Phase after filtering (3)\n",
    "\n",
    "#Modify this to change the approach used: XY, AmpPhase, AmpPhaseFiltered\n",
    "method = Compression_Method.AmpPhase\n",
    "scaler = StandardScaler()\n",
    "ignorePhases = True\n",
    "saveCSV = True\n",
    "\n",
    "base_directory = 'csv/step_' + str(method.value)\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables and Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notInterestedIndexes = list(range(-32,-28)) + list(range(0,1)) + list(range(29,32)) #null columns in the dataset\n",
    "interestedIndexes = list(range(-28,0)) + list(range(1,29)) #non null columns in the dataset\n",
    "\n",
    "w1=5 #for filtering\n",
    "w2=3 #for windows\n",
    "lambda1=3 #threshold\n",
    "\n",
    "#for ground truth\n",
    "t2,t3,t4 = [570.0, 873.0, 1254.0] #separation times for gt\n",
    "gt1 = [120,180,240,300,390,540]\n",
    "gt2 = [t2+i for i in range(60,300,30)]\n",
    "gt3 = [t3+30,t3+57] + [t3+i for i in range(90,390,30)]\n",
    "gt4 = [t4+i for i in range(30,630,30)]\n",
    "lower_bounds = gt1+gt2+gt3+gt4\n",
    "upper_bounds = [l + 1 for l in lower_bounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkGT(predicted_true,predicted_false):\n",
    "    tp,tn,fp,fn = [0,0,0,0]\n",
    "    \n",
    "    for t in predicted_true:\n",
    "        fp = fp+1  #add false positive\n",
    "        for l in lower_bounds:\n",
    "            if abs(t-l) <=w2:\n",
    "                tp = tp+1 #put true positive\n",
    "                fp = fp-1#remove the old false positive\n",
    "                break\n",
    "                \n",
    "    for t in predicted_false:\n",
    "        tn= tn+1  #add false positive\n",
    "        for l in lower_bounds:\n",
    "            if abs(t-l) <=w2:\n",
    "                tn = tn-1 #put true neg\n",
    "                fn = fn+1#remove the old false neg\n",
    "                \n",
    "                break\n",
    "    return tp,tn,fp,fn\n",
    "\n",
    "def classify_passage(dataframe, ycol=\"MuStdAmplPaper\",gt=\"Label\",plot_roc=True):\n",
    "    dfPeaks = pd.DataFrame(columns=[\"Time\",ycol])\n",
    "    for index, row in dataframe.iterrows():\n",
    "        if index==0 or index == dataframe.tail(1).index:\n",
    "            continue\n",
    "        if row[ycol] >= dataframe.iloc[index-1][ycol] and row[ycol] > dataframe.iloc[index+1][ycol]:\n",
    "            new_row = pd.DataFrame([row[[\"Time\", ycol]]])\n",
    "            dfPeaks = pd.concat([dfPeaks, new_row], ignore_index=True)\n",
    "    #function that returns the ROC plot and the AUC (skipping multiple misprediction)\n",
    "    tau = min(dfPeaks[ycol])\n",
    "    num_iter = 1000\n",
    "    step = (max(dfPeaks[ycol]) - tau) / num_iter\n",
    "    tpr = []\n",
    "    fpr = []\n",
    "    while tau < max(dfPeaks[ycol]):\n",
    "        ttrues = list(dfPeaks.loc[dfPeaks[ycol]>=tau,\"Time\"])\n",
    "        tfalses = list(dfPeaks.loc[dfPeaks[ycol]<tau,\"Time\"])\n",
    "        tp,tn,fp,fn = checkGT(ttrues,tfalses)\n",
    "        tpr.append(tp/(tp+fn))\n",
    "        fpr.append(fp/(fp+tn))\n",
    "        #print(fp/(fp+tn),tp/(tp+fn))\n",
    "        \n",
    "        tau = tau+step\n",
    "    \n",
    "    if(plot_roc):\n",
    "        #Plot the roc curve\n",
    "        plt.figure(figsize=(3,3),dpi=220)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.plot([0, 1], [0, 1], color = 'green')\n",
    "        plt.xlim(-0.05, 1.05)\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC curve\")\n",
    "        plt.show()\n",
    "\n",
    "    #print(auc(fpr, tpr))\n",
    "    return auc(fpr, tpr)\n",
    "\n",
    "def extractWindowedFeatures(data,column_indexes = [],w2=3):\n",
    "    data[\"TimeWindow\"] = np.floor(data[\"Timestamp\"] / w2)*w2\n",
    "    #vertical mean/std\n",
    "    dataStd = data.groupby(by=\"TimeWindow\").std().drop([\"Timestamp\",\"Frame_num\"],axis=1)\n",
    "    #dataMean = data.groupby(by=\"TimeWindow\").mean().drop([\"Timestamp\",\"Frame_num\"],axis=1)\n",
    "    \n",
    "    featuredDf = pd.DataFrame()\n",
    "    featuredDf[\"Time\"] = data[\"TimeWindow\"].unique()\n",
    "    #horizontal\n",
    "    featuredDf[\"MuStdAmplPaper\"] = dataStd[[j for j in column_indexes if j.startswith('Ampl')]].mean(axis=1).reset_index(drop=True) #Axis=1: mean over different columns -> into one col\n",
    "    return featuredDf\n",
    "\n",
    "def extractWindowedOptimized(dataStd,column_indexes = interestedIndexes,w2=3):\n",
    "    featuredDf = pd.DataFrame()\n",
    "    featuredDf[\"Time\"] = dataStd[\"Time\"].unique()\n",
    "    #horizontal\n",
    "    featuredDf[\"MuStdAmplPaper\"] = dataStd[[f\"Ampl{j}\" for j in column_indexes]].mean(axis=1).reset_index(drop=True) #Axis=1: mean over different columns -> into one col\n",
    "    return featuredDf\n",
    "\n",
    "#removes outliers from the data\n",
    "def filterData(df,w1=3,lambda1=3):\n",
    "    data = df.copy() #clone and return a copy\n",
    "    #w1 = 5 #best=2\n",
    "    #lambda1 = 3 #best=4\n",
    "    #col_list = [f\"Ampl{j}\" for j in interestedIndexes]\n",
    "    col_list = [j for j in data.columns if \"Ampl\" in j]\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if index == 0:\n",
    "            prev_row = row\n",
    "            continue\n",
    "        if (index%50 == 0): print(index)\n",
    "        subDf = data.loc[(data[\"Timestamp\"]<=row['Timestamp']) & (data[\"Timestamp\"]> row['Timestamp'] - w1),col_list]\n",
    "        means = subDf.mean(axis=0)\n",
    "        stds = subDf.std(axis=0)\n",
    "\n",
    "        for c in col_list: \n",
    "            if (abs(row[c] - means[c]) / stds[c]) > lambda1:\n",
    "                data.at[index,c] = prev_row[c]\n",
    "                #row[c] = prev_row[c]\n",
    "\n",
    "        prev_row = row\n",
    "    return data\n",
    "\n",
    "def complex_real(complex_value):\n",
    "    return complex(complex_value).real\n",
    "\n",
    "def complex_imag(complex_value):\n",
    "    return complex(complex_value).imag\n",
    "\n",
    "def complex_rebuild(real,imag):\n",
    "    return (real + 1j*imag)\n",
    "\n",
    "#Function to get top N features for each principal component\n",
    "def get_top_n_features(loadings_df, n):\n",
    "    top_features = {}\n",
    "    for pc in loadings_df.columns:\n",
    "        top_features[pc] = loadings_df[pc].abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    clusters = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (clusters[:-1] + clusters[1:]) / 2 #Defines intervals of clusters\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
    "        \n",
    "        empty_clusters = np.isnan(new_clusters) #Restore previous cluster if empty\n",
    "        new_clusters[empty_clusters] = clusters[empty_clusters] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_clusters - clusters)) < delta:\n",
    "            break\n",
    "\n",
    "        clusters = new_clusters\n",
    "\n",
    "    #Quantize the data based on the final clusters\n",
    "    quantized_data = clusters[indices]\n",
    "\n",
    "    return quantized_data, clusters, thresholds\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, clusters, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return clusters[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self, value=None, frequency=0, left=None, right=None):\n",
    "        self.value = value\n",
    "        self.frequency = frequency\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other): #redefined \"less than\" operator for heapq\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "def build_tree(data):\n",
    "    heap = [Node(value, frequency) for value, frequency in data.items()]  #Init heap\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:  #pop two smallest nodes, merge them and push the merged node back\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = Node(frequency=left.frequency + right.frequency, left=left, right=right)\n",
    "        heapq.heappush(heap, merged) \n",
    "\n",
    "    return heap[0] #root\n",
    "\n",
    "def generate_codes(node, code=\"\", huffman_codes=None):\n",
    "    if huffman_codes is None: \n",
    "        huffman_codes = {}\n",
    "\n",
    "    if node.value is not None: #leaf node case\n",
    "        huffman_codes[node.value] = code\n",
    "        return\n",
    "    else:\n",
    "        generate_codes(node.left, code + \"0\", huffman_codes)\n",
    "        generate_codes(node.right, code + \"1\", huffman_codes)\n",
    "    return huffman_codes\n",
    "\n",
    "def encode_huffman(data, huffman_codes):\n",
    "    emptyStr = \"\"\n",
    "    return emptyStr.join([huffman_codes[val] for val in data]) \n",
    "\n",
    "def decode_huffman(encoded_data, huffman_codes):\n",
    "    decoded_data = []\n",
    "    code = \"\"\n",
    "    for bit in encoded_data: #traverse the encoded data and searches for the code\n",
    "        code += bit\n",
    "        for key, value in huffman_codes.items():\n",
    "            if value == code: #If found, append the corresponding value to the decoded data, otherwise add another bit to the code\n",
    "                decoded_data.append(key)\n",
    "                code = \"\"\n",
    "                break\n",
    "                \n",
    "    return decoded_data\n",
    "\n",
    "def apply_huffman_encode_per_feature(data):\n",
    "    encoded_df = pd.DataFrame()\n",
    "    huffman_codes = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col]) \n",
    "        root = build_tree(freq_per_data)\n",
    "        code = generate_codes(root)\n",
    "        #print(\"data[\"+ col +\"]:\\n\", data[col])\n",
    "        encoded_df[col] = data[col].apply(lambda x: encode_huffman([x], code))\n",
    "        huffman_codes[col] = code\n",
    "    return encoded_df, huffman_codes\n",
    "\n",
    "def apply_huffman_decode_per_feature(encoded_data, huffman_codes):\n",
    "    decoded_df = pd.DataFrame()\n",
    "\n",
    "    for col in encoded_data.columns:\n",
    "        decoded_df[col] = decode_huffman(''.join(encoded_data[col]), huffman_codes[col])\n",
    "    return decoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, method):\n",
    "    #df = pd.read_csv('csv/passage.csv')\n",
    "    df['Timestamp'] = round(df['Timestamp'], 4)\n",
    "    data = df.copy()\n",
    "    \n",
    "    columns_to_drop = (['Frame_num', 'Source_address', 'TimeWindow'] + \n",
    "                    [f\"Phase{i}\" for i in notInterestedIndexes] + \n",
    "                    [f\"Ampl{i}\" for i in notInterestedIndexes] + \n",
    "                    [f\"CSI{i}\" for i in notInterestedIndexes])\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    if ignorePhases:\n",
    "        data.drop(columns=[col for col in data.columns if col.startswith('Phase')], inplace=True); #Removes Phase columns\n",
    "\n",
    "    if method == Compression_Method.XY:  \n",
    "        for j in interestedIndexes:\n",
    "            data[f'X{j}'] = data[f\"CSI{j}\"].apply(complex_real)\n",
    "            data[f'Y{j}'] = data[f\"CSI{j}\"].apply(complex_imag)\n",
    "        data.drop(columns=[col for col in data.columns if col.startswith(('Ampl', 'Phase'))], inplace=True); #Removes Ampl and Phase columns\n",
    "    elif method == Compression_Method.AmpPhaseFiltered:\n",
    "        data = filterData(data)\n",
    "\n",
    "    data.drop(columns=[col for col in data.columns if col.startswith('CSI')], inplace=True); #Removes CSI columns\n",
    "    data.set_index('Timestamp', inplace=True)\n",
    "    print(\"Number of features:\", len(data.columns))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many components are needed to have an explanation of 95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components(data, target, directory):\n",
    "    #Fit and transform the data\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    #Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "\n",
    "    var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "    #finds PCs that explain 95% of the variance\n",
    "    k = np.argmax(var_cumulative > target) + 1\n",
    "    print(f\"Number of components explaining {target}% variance: \"+ str(k))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Cumulative Explained Variance explained by the components')\n",
    "    plt.ylabel('Cumulative Explained variance')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.axvline(x=k, color=\"r\", linestyle=\"--\")\n",
    "    plt.axhline(y=target, color=\"r\", linestyle=\"--\")\n",
    "    plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', linestyle='--')\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(directory, 'var_cumulative_x_component.png'))\n",
    "    plt.show()\n",
    "\n",
    "    return scaled_data, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA, check the explained variance ratio and the cumulative explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_PCA(scaled_data, n_components, directory):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "    plt.title('Cumulative Explained Variance by PCA Components')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(directory, 'zoomed_var_cumulative_x_component.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return reduced_df, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Principal Component, find the top \"n\" features that contribute most to the variance of that component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_PC(data, pca, n_components):\n",
    "    loadings = pca.components_\n",
    "    loadings_df = pd.DataFrame(data=loadings.T, index=data.columns, columns=[f'PC{i+1}' for i in range(loadings.shape[0])])\n",
    "    column = []\n",
    "\n",
    "    top_n_features = get_top_n_features(loadings_df, n_components)\n",
    "\n",
    "    for pc, features in top_n_features.items():\n",
    "        #print(f\"Top {n_components} features for {pc}: {features}\") #uncomment to see the top features per PC\n",
    "        for feature in features:\n",
    "            if feature not in column:\n",
    "                column.append(feature)\n",
    "    print(\"available features: \", len(data.columns))\n",
    "    print(\"features used: \", len(column))\n",
    "\n",
    "    difference = set(data.columns) - set(column)\n",
    "    print(\"Unused Features:\", difference)\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lloyd-Max Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization(reduced_df, lvls):\n",
    "    df_quantized = reduced_df.apply(lambda col: lloyd_max_quantization(col.values, num_levels=lvls)[0])\n",
    "    return df_quantized\n",
    "\n",
    "\n",
    "#YET TO TEST\n",
    "\"\"\"\n",
    "clusters, thresholds = lloyd_max_quantization(reduced_df.values)[1:3]\n",
    "\n",
    "print(\"Centroids:\", clusters)\n",
    "print(\"Thresholds:\", thresholds)\n",
    "\n",
    "#Dequantize the quantized data\n",
    "df_dequantized = df_quantized.apply(lambda col: dequantize_lloyd_max(col.values, clusters, thresholds))\n",
    "\n",
    "#Reconstruct the original data\n",
    "reconstructed_data = pca.inverse_transform(df_dequantized.values)\n",
    "df_reconstructed = scaler.inverse_transform(reconstructed_data)\n",
    "\n",
    "df_reconstructed = pd.DataFrame(data=df_reconstructed, columns=data.columns)\n",
    "\n",
    "print(\"Original data:\")\n",
    "data.reset_index(inplace=True)\n",
    "data.drop(columns=['Timestamp'], inplace=True)\n",
    "print(data)\n",
    "print(\"Reconstruction error:\", np.mean((data.values - df_reconstructed.values)**2))\n",
    "print(df_reconstructed)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy Coding (Huffman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_encoding(df_quantized, df):\n",
    "    encoded_df, huffman_codes = apply_huffman_encode_per_feature(df_quantized)\n",
    "    encoded_df = pd.concat([df[['Frame_num', 'Timestamp']], encoded_df, df['TimeWindow']], axis=1)\n",
    "\n",
    "    return encoded_df, huffman_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_decoding(encoded_df, huffman_codes):\n",
    "    decoded_df = apply_huffman_decode_per_feature(encoded_df.iloc[:, 2:-1], huffman_codes)\n",
    "    return decoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the dataset (without CSI components) and save it in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(decoded_df, encoded_df, pca, scaler, data):\n",
    "\n",
    "    df_scaled_reconstructed = pca.inverse_transform(decoded_df.values)\n",
    "    df_reconstructed = scaler.inverse_transform(df_scaled_reconstructed)\n",
    "\n",
    "    \"\"\"\n",
    "    print('Original data shape:', df.shape)\n",
    "    print('Scaled data shape:', reduced_data.shape)\n",
    "    print('PCA components shape:', reduced_df.shape)\n",
    "    print('Reconstructed scaled data shape:', df_scaled_reconstructed.shape)\n",
    "    print('Reconstructed original data shape:', df_reconstructed.shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=data.columns)\n",
    "    df_reconstructed = pd.concat([encoded_df.iloc[:, [0, 1, -1]], df_reconstructed], axis=1)\n",
    "\n",
    "    if method == Compression_Method.XY:\n",
    "        for j in interestedIndexes:\n",
    "            df_reconstructed[f'CSI{j}'] = df_reconstructed.apply(lambda x: complex_rebuild(x[f'X{j}'], x[f'Y{j}']), axis=1)\n",
    "                \n",
    "            #compute back ampl and phases\n",
    "            df_reconstructed[f'Ampl{j}'] = df_reconstructed[f'CSI{j}'].apply(abs)\n",
    "            df_reconstructed[f'Phase{j}'] = df_reconstructed[f'CSI{j}'].apply(cmath.phase)\n",
    "\n",
    "        df_reconstructed.drop(columns=[f'X{j}' for j in interestedIndexes], inplace=True)\n",
    "        df_reconstructed.drop(columns=[f'Y{j}' for j in interestedIndexes], inplace=True)\n",
    "        \n",
    "    return df_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ampl_comparison(data, reconstruct_data, column, directory):\n",
    "\n",
    "    data = data[(data['Timestamp'] >= 0) & (data['Timestamp'] <= 700)]\n",
    "    reconstruct_data = reconstruct_data[(reconstruct_data['Timestamp'] >= 0) & (data['Timestamp'] <= 700)]\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "\n",
    "    columns = [f'Ampl{column}', f'Ampl-{column}'] \n",
    "\n",
    "    for i in range(columns.__len__()):\n",
    "        if (columns[i] not in data.columns) or (columns[i] not in reconstruct_data.columns):\n",
    "            print(f\"Column {columns[i]} not found in the data\")\n",
    "            return\n",
    "\n",
    "        # Plot the original data\n",
    "        plt.plot(data['Timestamp'], data[columns[i]], label=f'Original {columns[i]}', color=\"red\" if i == 0 else \"green\")\n",
    "\n",
    "        # Plot the reconstructed data\n",
    "        plt.plot(reconstruct_data['Timestamp'], reconstruct_data[columns[i]], label=f'Reconstructed {columns[i]}', color=\"blue\" if i == 0 else \"orange\")\n",
    "    \n",
    "    # Add plot details\n",
    "    plt.title('Amplitude Comparison')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(directory, f'Ampl{column}_Ampl-{column}_comparison.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification (partially from another notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comparison():\n",
    "    filteredFeaturesPassage = pd.read_csv(\"csv/filteredFeaturesPassage.csv\")\n",
    "    #featuredDf = extractWindowedFeatures(filteredDf,column_indexes = interestedIndexes,w2=w2)\n",
    "    orig_auc = classify_passage(filteredFeaturesPassage, plot_roc=False)\n",
    "    return orig_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filtering(df_reconstructed):\n",
    "    reconstructedPassage = df_reconstructed\n",
    "\n",
    "    if method == Compression_Method.AmpPhaseFiltered:\n",
    "        reconstructed_filtered = reconstructedPassage\n",
    "    else:\n",
    "        reconstructed_filtered = filterData(reconstructedPassage) #removes outliers\n",
    "    reconstructed_filtered.drop(columns=[col for col in reconstructed_filtered.columns if col.startswith('CSI')], inplace=True); #Removes CSI columns\n",
    "\n",
    "    return reconstructed_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classification(reconstructed_filtered):\n",
    "    #compute features\n",
    "    reconstructed_featured = extractWindowedFeatures(reconstructed_filtered,column_indexes = reconstructed_filtered.columns,w2=w2)\n",
    "    \n",
    "    #classify\n",
    "    print(classify_passage(reconstructed_featured,plot_roc=False))\n",
    "\n",
    "    return reconstructed_featured, classify_passage(reconstructed_featured,plot_roc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "8 levels:\n",
    "- **AmpPhaseFiltered**: 0.9705615942028984 (PCA 0.95, LLoyd-Max: num_levels=**8**, max_iter=100, tol=1e-6) \n",
    "- **AmpPhase**: 0.9716093445814408 (PCA 0.95, LLoyd-Max: num_levels=**8**, max_iter=100, tol=1e-6)\n",
    "- **XY**: 0.9370967741935484 (PCA 0.95, LLoyd-Max: num_levels=**8**, max_iter=100, tol=1e-6)\n",
    "---\n",
    "16 levels:\n",
    "- **AmpPhaseFiltered**: 0.9661764705882354 (PCA 0.95, LLoyd-Max: num_levels=**16**, max_iter=100, tol=1e-6) \n",
    "- **AmpPhase**:  (PCA 0.95, LLoyd-Max: num_levels=**16**, max_iter=100, tol=1e-6)\n",
    "- **XY**:  (PCA 0.95, LLoyd-Max: num_levels=**16**, max_iter=100, tol=1e-6)\n",
    "---\n",
    "Only Ampl:\n",
    "- **AmpPhaseFiltered**: (PCA 0.95, LLoyd-Max: num_levels=8 max_iter=100, tol=1e-6) \n",
    "- **AmpPhase**: 0.971077504725898 (PCA 0.95, LLoyd-Max: num_levels=8 max_iter=100, tol=1e-6) \n",
    "- **XY**:  (PCA 0.95, LLoyd-Max: num_levels=8 max_iter=100, tol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions:\n",
    "- Cleaned dataset to work only on needed columns then restores it before saving it\n",
    "- 95% explained variance treshold\n",
    "- 8 e 16 quantization lvls\n",
    "- 10^-6 delta of values in Lloyd-Max\n",
    "\n",
    "Questions:\n",
    "- Is it okay to apply PCA on the whole dataset? it's not big\n",
    "- Is it okay to use the same windows that the paper used or should I try to find the optimal ones (like the paper did?)\n",
    "- Is it better to have a single huffman tree for the whole dataset or one per feature? (I think the second one is better)\n",
    "- Is it okay to use 8/16 quantization lvl or should I try to find the best one first and then use only that?\n",
    "- How to store the data (i guess it's better to store the encoded data + the huffman code, PCA matrix, scaler matrix and eventually the quantization matrix)\n",
    "\n",
    "TODO:\n",
    "- Improve reconstruction to reduce the amount of data needed\n",
    "- Test 16 lvl\n",
    "- Test diff tol\n",
    "- Find best step_num\n",
    "- all tests with dequantized dataset (in theory it approximates og data more closely than the quantized version)\n",
    "\n",
    "Optimizations:\n",
    "- Is FrameNum necessary if we already have timestamp?\n",
    "- Timestamps decimals can be shortened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/passage.csv')\n",
    "data = data_preprocessing(df, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'csv/step_' + str(method.value)\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv('csv/passage.csv')\n",
    "data = data_preprocessing(df, method)\n",
    "\n",
    "target = 95\n",
    "num_levels = 8\n",
    "\n",
    "#PCA\n",
    "scaled_data, n_components = find_n_components(data, target)\n",
    "reduced_df, pca = analyze_PCA(scaled_data, n_components, target)\n",
    "unused_features = analyze_PC(data, pca, n_components)\n",
    "\n",
    "#LLoyd-Max Quantization\n",
    "quantized_df = apply_quantization(reduced_df, num_levels)\n",
    "\n",
    "#Encode-Decode\n",
    "encoded_df, huffman_codes = apply_encoding(quantized_df, df)\n",
    "decoded_df = apply_decoding(encoded_df, huffman_codes)\n",
    "print(\"Original DataFrame equals Decoded DataFrame:\", quantized_df.equals(decoded_df)) #Correctness check\n",
    "\n",
    "#Reconstruction\n",
    "reconstructed_df = reconstruct_data(decoded_df, encoded_df, pca, scaler, data)\n",
    "load_comparison()\n",
    "\n",
    "columns = [i for i in interestedIndexes if i >= 0]\n",
    "for column in columns:\n",
    "    plot_ampl_comparison(df, reconstructed_df, column, num_levels, target)\n",
    "\n",
    "reconstructed_df.drop(columns=unused_features, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering and Classification\n",
    "reconstructed_filtered = apply_filtering(reconstructed_df)\n",
    "reconstructed_featured, auc = apply_classification(reconstructed_filtered)\n",
    "\n",
    "#Save CSV\n",
    "if(saveCSV):\n",
    "    encoded_df.to_csv(os.path.join(directory, 'encodedQuantizedPCAPassage.csv'), index=False)\n",
    "    #reconstructed_df.set_index('Timestamp', inplace=True)\n",
    "    reconstructed_df.to_csv(os.path.join(directory, 'passage_reconstructed.csv'))\n",
    "    reconstructed_filtered.to_csv(os.path.join(directory, 'filteredPassage_reconstructed.csv'), index=False)\n",
    "    reconstructed_featured.to_csv(os.path.join(directory, 'filteredFeaturesPassage_reconstructed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your targets and num_levels lists\n",
    "targets = [80, 85, 90, 95, 99]\n",
    "num_levels = [4, 8, 16, 32]\n",
    "\n",
    "results = []\n",
    "orig_auc = load_comparison()\n",
    "\n",
    "# Iterate over targets and num_levels\n",
    "for target in targets:\n",
    "    directory = os.path.join(base_directory, f'Target_{target}')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Read and preprocess data\n",
    "    df = pd.read_csv('csv/passage.csv')\n",
    "    data = data_preprocessing(df, method)\n",
    "    \n",
    "    # PCA\n",
    "    scaled_data, n_components = find_n_components(data, target, directory)\n",
    "    reduced_df, pca = analyze_PCA(scaled_data, n_components, directory)\n",
    "    unused_features = analyze_PC(data, pca, n_components)\n",
    "\n",
    "    for level in num_levels:\n",
    "         # Create a directory for each combination of target and num_levels\n",
    "        sub_directory = os.path.join(directory, f'lvls_{level}')\n",
    "        os.makedirs(sub_directory, exist_ok=True)\n",
    "\n",
    "        # Lloyd-Max Quantization\n",
    "        quantized_df = apply_quantization(reduced_df, level)\n",
    "\n",
    "        # Encode-Decode\n",
    "        encoded_df, huffman_codes = apply_encoding(quantized_df, df)\n",
    "        decoded_df = apply_decoding(encoded_df, huffman_codes)\n",
    "        print(\"Original DataFrame equals Decoded DataFrame:\", quantized_df.equals(decoded_df)) #Correctness check\n",
    "\n",
    "        # Reconstruction\n",
    "        reconstructed_df = reconstruct_data(decoded_df, encoded_df, pca, scaler, data)\n",
    "        print(\"original_auc:\",orig_auc)\n",
    "\n",
    "        columns = [i for i in interestedIndexes if i >= 0]\n",
    "        for column in columns:\n",
    "            plot_ampl_comparison(df, reconstructed_df, column, sub_directory)\n",
    "\n",
    "        reconstructed_df.drop(columns=unused_features, inplace=True)\n",
    "\n",
    "        # Filtering and Classification\n",
    "        reconstructed_filtered = apply_filtering(reconstructed_df)\n",
    "        print(f\"----------Results with target {target}%, num_levels {level} ----------\")\n",
    "        reconstructed_featured, auc_value = apply_classification(reconstructed_filtered)\n",
    "\n",
    "        results.append({\n",
    "            'Method': method.name,\n",
    "            'num_levels': level,\n",
    "            'target': target,\n",
    "            'AUC': auc_value\n",
    "        })\n",
    "\n",
    "        # Save CSVs in the specific directory\n",
    "        if saveCSV:\n",
    "            encoded_df.to_csv(os.path.join(sub_directory, 'encodedQuantizedPCAPassage.csv'), index=False)\n",
    "            #reconstructed_df.set_index('Timestamp', inplace=True)\n",
    "            reconstructed_df.to_csv(os.path.join(sub_directory, 'passage_reconstructed.csv'))\n",
    "            reconstructed_filtered.to_csv(os.path.join(sub_directory, 'filteredPassage_reconstructed.csv'), index=False)\n",
    "            reconstructed_featured.to_csv(os.path.join(sub_directory, 'filteredFeaturesPassage_reconstructed.csv'), index=False)\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_file = os.path.join(base_directory, 'classification_results.csv')\n",
    "results_df.to_csv(results_file, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
