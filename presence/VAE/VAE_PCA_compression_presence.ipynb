{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import cmath\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from enum import Enum\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state) # predictable random numbers, for demonstration only\n",
    "tf.random.set_seed(random_state) # reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(random_state) # reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # make operations deterministic\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"]= '1' # Use legacy keras for compatibility\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compression_Method(Enum):\n",
    "    XY = 1               #applies PCA on X and Y then filters (1)\n",
    "    AmpPhase = 2         #applies PCA on Amplitude and Phase then filters (2)\n",
    "    AmpPhaseFiltered = 3 #applies PCA on Amplitude and Phase after filtering (3)\n",
    "\n",
    "#Modify this to change the approach used: XY, AmpPhase, AmpPhaseFiltered\n",
    "method = Compression_Method.AmpPhase\n",
    "scaler = StandardScaler()\n",
    "ignorePhases = True\n",
    "saveCSV = True\n",
    "\n",
    "latent_dim = 1 \n",
    "num_activities = 2 #present or absent\n",
    "\n",
    "folder_name = 'vae_weights'\n",
    "models_directory = './models'\n",
    "results_directory = './results'\n",
    "os.makedirs(os.path.join(models_directory, folder_name), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notInterestedIndexes = list(range(-32,-28)) + list(range(0,1)) + list(range(29,32)) #null columns in the dataset\n",
    "interestedIndexes = list(range(-28,0)) + list(range(1,29)) #non null columns in the dataset\n",
    "\n",
    "w1=5 #for filtering\n",
    "w2=3 #for windows\n",
    "#w2=1 #1 second per window\n",
    "lambda1=3 #threshold\n",
    "\n",
    "#build ground truth\n",
    "t2 = 1205\n",
    "lb1 = [120,360,600,900]\n",
    "ub1 = [240,480,720,1080]\n",
    "lb2 = [t2+l for l in [180,540,990,1500]]\n",
    "ub2 = [t2+u for u in [360,750,1170,1590]]\n",
    "\n",
    "lower_bounds = lb1+lb2\n",
    "upper_bounds = ub1+ub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGT(timestamp,lower_bounds,upper_bounds):\n",
    "    # if I'm in the room in one case, or I'm crossing the entrance put 1\n",
    "    for i in range (0, len(lower_bounds)):\n",
    "        if (timestamp >= lower_bounds[i]) & (timestamp <= upper_bounds[i]):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def classify_presence(df, ycol=\"MuStdAmplPaper\", plot_roc=False, num_iter=1000, thr=None):\n",
    "    # Y are the labels that indicate if i'm passing or not\n",
    "    Y = df[\"Label\"]\n",
    "    # thr is the threshold: if amplitude > thr, then assign to Y_pred 1 (presence), otherwise 0. Every time update the threshold\n",
    "    if thr is None:\n",
    "        thr= df[ycol].min()\n",
    "        tpr = []\n",
    "        fpr= []\n",
    "        thr_list= []\n",
    "        step = (df[ycol].max() - df[ycol].min()) / num_iter\n",
    "        while thr <= df[ycol].max():\n",
    "            # compute the predictions\n",
    "            Y_pred = df.apply(lambda row: 1 if row[ycol] >= thr else 0, axis=1)\n",
    "            tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()\n",
    "            # compute True Positive Rate and False Positive rate to plot the roc curve\n",
    "            tpr.append(tp/(tp+fn))\n",
    "            fpr.append(fp/(fp+tn))\n",
    "            thr_list.append(thr)\n",
    "            thr += step\n",
    "        thr = select_threshold(thr_list, df, ycol) # select the threshold that maximizes the f1 score for class \n",
    "\n",
    "    if plot_roc:\n",
    "        plt.figure(figsize=(3,3),dpi=220)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.plot([0, 1], [0, 1], color = 'green')\n",
    "        plt.xlim(-0.05, 1.05)\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC curve\")\n",
    "        plt.show()\n",
    "\n",
    "    Y_pred_final = (df[ycol] >= thr).astype(int)\n",
    "    f1_result = compute_f1(Y, Y_pred_final)\n",
    "    print(classification_report(Y, Y_pred_final, target_names=[\"absent\", \"present\"]))\n",
    "    #accuracy = accuracy_score(Y, Y_pred)\n",
    "\n",
    "    return f1_result, thr\n",
    "\n",
    "def compute_f1(Y, Y_pred, label=1):\n",
    "    precision = precision_score(Y, Y_pred, pos_label=label)\n",
    "    recall = recall_score(Y, Y_pred, pos_label=label)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "    return f1\n",
    "\n",
    "def select_threshold(thr_list, df, ycol, label=1):\n",
    "    # compute the f1 score for class 1\n",
    "    Y = df[\"Label\"]\n",
    "    f1_scores_class_1 = []\n",
    "    for thr in thr_list:\n",
    "        Y_pred = (df[ycol] >= thr).astype(int)\n",
    "        f1_class_1 = compute_f1(Y, Y_pred, label)\n",
    "        f1_scores_class_1.append(f1_class_1)\n",
    "\n",
    "    # find the optimal threshold for class 1\n",
    "    optimal_idx = np.argmax(f1_scores_class_1)\n",
    "    optimal_thr = thr_list[optimal_idx]\n",
    "\n",
    "    return optimal_thr\n",
    "\n",
    "def extractWindowedFeatures(data, column_indexes = [], w2=3):\n",
    "    data[\"TimeWindow\"] = np.floor(data[\"Timestamp\"] / w2)*w2\n",
    "    #vertical mean/std\n",
    "    dataStd = data.groupby(by=\"TimeWindow\").std().drop([\"Timestamp\",\"Frame_num\"],axis=1)\n",
    "    #dataMean = data.groupby(by=\"TimeWindow\").mean().drop([\"Timestamp\",\"Frame_num\"],axis=1)\n",
    "    \n",
    "    featuredDf = pd.DataFrame()\n",
    "    featuredDf[\"Time\"] = data[\"TimeWindow\"].unique()\n",
    "    #horizontal\n",
    "    featuredDf[\"MuStdAmplPaper\"] = dataStd[[j for j in column_indexes if j.startswith('Ampl')]].mean(axis=1).reset_index(drop=True) #Axis=1: mean over different columns -> into one col\n",
    "    return featuredDf\n",
    "\n",
    "#removes outliers from the data\n",
    "def filterData(df,w1=3,lambda1=3):\n",
    "    data = df.copy()\n",
    "    col_list = [j for j in data.columns if \"Ampl\" in j]\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if index == 0:\n",
    "            prev_row = row\n",
    "            continue\n",
    "        if (index%10000 == 0): print(index)\n",
    "        subDf = data.loc[(data[\"Timestamp\"]<=row['Timestamp']) & (data[\"Timestamp\"]> row['Timestamp'] - w1),col_list]\n",
    "        means = subDf.mean(axis=0)\n",
    "        stds = subDf.std(axis=0)\n",
    "\n",
    "        for c in col_list: \n",
    "            if (abs(row[c] - means[c]) / stds[c]) > lambda1:\n",
    "                data.at[index,c] = prev_row[c]\n",
    "                #row[c] = prev_row[c]\n",
    "\n",
    "        prev_row = row\n",
    "    return data\n",
    "\n",
    "def filterData2(df, w1=3, lambda1=3):\n",
    "    data = df.copy()\n",
    "    col_list = [j for j in data.columns if \"Ampl\" in j]\n",
    "    \n",
    "    # Rolling window to calculate means and std deviations, shifted to exclude the current row\n",
    "    rolling_means = data[col_list].rolling(window=w1, min_periods=1).mean().shift(1)\n",
    "    rolling_stds = data[col_list].rolling(window=w1, min_periods=1).std().shift(1)\n",
    "\n",
    "    # Start at second row since the first row is skipped in original logic\n",
    "    for index in range(1, len(data)):\n",
    "        if index % 10000 == 0: \n",
    "            print(f\"{index}/{len(data)}\")\n",
    "        \n",
    "        for c in col_list:\n",
    "            current_val = data.at[index, c]\n",
    "            mean_val = rolling_means.at[index, c]\n",
    "            std_val = rolling_stds.at[index, c]\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if pd.isna(std_val) or std_val == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check if the current value is out of bounds\n",
    "            if abs(current_val - mean_val) / std_val > lambda1:\n",
    "                # Set value to previous row's value if condition is met\n",
    "                data.at[index, c] = data.at[index-1, c]\n",
    "                rolling_means.at[index, c] = data.at[index, c] # Update mean with corrected value\n",
    "                \n",
    "    return data\n",
    "\n",
    "def complex_real(complex_value):\n",
    "    return complex(complex_value).real\n",
    "\n",
    "def complex_imag(complex_value):\n",
    "    return complex(complex_value).imag\n",
    "\n",
    "def complex_rebuild(real,imag):\n",
    "    return (real + 1j*imag)\n",
    "\n",
    "#Function to get top N features for each principal component\n",
    "def get_top_n_features(loadings_df, n):\n",
    "    top_features = {}\n",
    "    for pc in loadings_df.columns:\n",
    "        top_features[pc] = loadings_df[pc].abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0.65 #max accuracy for f1_loss\n",
    "def f1_loss(data, max_score):\n",
    "    percentage = data['f1_score'].clip(upper=max_score) * 100\n",
    "    loss = (max_score * 100) - percentage\n",
    "    data['f1_loss'] = loss\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf_keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='valid')(encoder_inputs)\n",
    "    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(8, activation='relu')(x)\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(1, 1), padding='valid')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(1, 1), padding='valid')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, out_filter, activation='sigmoid', padding='same')(x) \n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf_keras.Model):\n",
    "    def __init__(self, enc_input_shape=(32, 56, 1), dec_input_shape=(28, 52, 32), latent_dim=2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_csi_encoder(enc_input_shape, latent_dim)\n",
    "        self.decoder = create_csi_decoder(dec_input_shape, latent_dim, enc_input_shape[-1])\n",
    "        self.total_loss_tracker = tf_keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = tf_keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = tf_keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf_keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            beta = 0.001\n",
    "            total_loss = reconstruction_loss + kl_loss * beta\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vae_encoder(vae, source):\n",
    "    #Use the VAE to process CSI data\n",
    "    z_data = np.zeros([0, 4])\n",
    "    z_labels = np.zeros([0])\n",
    "\n",
    "    for (data, labels) in source:\n",
    "        labels = tf.squeeze(labels)\n",
    "        z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "        z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "        z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "        z_labels = np.concatenate([z_labels, labels.numpy().ravel()], axis=0)\n",
    "        \n",
    "    return z_data, z_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./models/{folder_name}/' + 'cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = tf_keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)\n",
    "early_stopping_cb = tf_keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "csv_logger_cb = tf_keras.callbacks.CSVLogger(f'./models/{folder_name}/model_history_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSI Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsiData(tf_keras.utils.Sequence):\n",
    "    def __init__(self, csi, labels, batch_size, window_size):\n",
    "        self.csi = csi\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.indices = tf.range(0, csi.shape[0] - self.window_size, dtype=tf.int32)\n",
    "        self.antennas = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.indices.shape[-1] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        first_idx = batch_idx * self.batch_size\n",
    "        last_idx = (batch_idx + 1) * self.batch_size\n",
    "\n",
    "        data_batch = [self.csi[x:x + self.window_size, ...] for x in range(first_idx, last_idx)]\n",
    "        labels_batch = np.transpose([self.labels[first_idx:last_idx]])\n",
    "        \n",
    "        data_batch = tf.convert_to_tensor(data_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "\n",
    "        if self.antennas == 1:\n",
    "            data_batch = tf.expand_dims(data_batch, 3)\n",
    "            labels_batch = tf.expand_dims(labels_batch, 2)\n",
    "\n",
    "        return data_batch, labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_windowing(data, time_window):\n",
    "    labels = pd.DataFrame()\n",
    "    data[\"TimeWindow\"] = np.floor(data[\"Timestamp\"] / time_window) * time_window # Create time windows\n",
    "    data.drop(columns=[\"Timestamp\"], inplace=True)\n",
    "    labels[\"TimeWindow\"] = data[\"TimeWindow\"].unique() # Get unique time windows\n",
    "    labels[\"Label\"] = labels[\"TimeWindow\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds)) #assign the ground-truth to a label\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_windowing(data, time_window, window_size, step_size, verbose=False):\n",
    "    data, labels = time_windowing(data, time_window)\n",
    "\n",
    "    train_windows, train_labels, test_windows, test_labels = [], [], [], []\n",
    "\n",
    "    # Drop time windows with less than \"window_size\" samples\n",
    "    filtered_df = data.groupby('TimeWindow').filter(lambda x: len(x) >= window_size)\n",
    "    if verbose: print(\"Number of time windows dropped:\", len(data['TimeWindow'].unique()) - len(filtered_df['TimeWindow'].unique()))\n",
    "\n",
    "    for time_window in filtered_df[\"TimeWindow\"].unique():\n",
    "        label = labels[labels[\"TimeWindow\"] == time_window][\"Label\"].values[0]\n",
    "        data_window = filtered_df[filtered_df[\"TimeWindow\"] == time_window].drop(\"TimeWindow\", axis=1)\n",
    "        data_window = np.array(data_window)\n",
    "        train, test = train_test_split(data_window, test_size=0.4, random_state=42, shuffle=False, stratify=None)\n",
    "\n",
    "        # Generate overlapping train window\n",
    "        if verbose: print(f\"Time window: {time_window}, train size: {len(train)}, for_train: {len(train) - window_size + 1} \\t test size: {len(test)}, for test: {len(test) - window_size + 1}\")\n",
    "        for start_idx in range(0, len(train) - window_size + 1, step_size):\n",
    "            #print(f\"Time window Train: {time_window}, start_idx: {start_idx}\")\n",
    "            window = train[start_idx : start_idx + window_size]\n",
    "            train_windows.append(window)\n",
    "            train_labels.extend([label] * window_size)\n",
    "\n",
    "        # Generate overlapping test windows\n",
    "        for start_idx in range(0, len(test) - window_size + 1, step_size):\n",
    "            #print(f\"Time window Train: {time_window}, start_idx: {start_idx}\")\n",
    "            window = test[start_idx : start_idx + window_size]\n",
    "            test_windows.append(window)\n",
    "            test_labels.extend([label] * window_size)\n",
    "\n",
    "    train_windows = np.array(train_windows)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_windows = np.array(test_windows)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Original number of windows:\", len(filtered_df[\"TimeWindow\"].unique()))\n",
    "        print(\"Number of train windows:\", train_windows.shape)   \n",
    "        print(\"Number of train labels:\", train_labels.shape)\n",
    "        print(\"Number of test windows:\", test_windows.shape)\n",
    "        print(\"Number of test labels:\", test_labels.shape)\n",
    "\n",
    "    return train_windows, train_labels, test_windows, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_stats(df):\n",
    "    timewindow_counts = df['TimeWindow'].value_counts(sort=False)\n",
    "    \n",
    "    mean = int(timewindow_counts.mean())\n",
    "    half_mean = mean // 2\n",
    "    double_mean = mean * 2\n",
    "    print(\"average count:\",mean)\n",
    "    print(\"max count:\",timewindow_counts.max())\n",
    "    print(\"min count:\",timewindow_counts.min())\n",
    "\n",
    "    counter = [x for x in timewindow_counts if x <half_mean]\n",
    "    print(f\"number of windows with less than {half_mean} samples: \",len(counter))\n",
    "    print(sorted(counter))\n",
    "\n",
    "    counter = [x for x in timewindow_counts if x >= half_mean and x < mean]\n",
    "    print(f\"number of windows between {half_mean} and {mean}: \",len(counter))\n",
    "    print(sorted(counter))\n",
    "\n",
    "    counter = [x for x in timewindow_counts if x >= mean and x < double_mean]\n",
    "    print(f\"number of windows between {mean} and {double_mean}: \",len(counter))\n",
    "    print(sorted(counter))\n",
    "\n",
    "    counter = [x for x in timewindow_counts if x >= double_mean]\n",
    "    print(f\"number of windows with more than {double_mean} samples: \",len(counter))\n",
    "    print(sorted(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CsiData_generator(train_windows, train_labels, test_windows, test_labels, window_size, batch_size, verbose=False):\n",
    "    num_features = train_windows.shape[2]\n",
    "    \n",
    "    #convert data in tensor\n",
    "    train_data_tf = tf.convert_to_tensor(train_windows, dtype=tf.float32)\n",
    "    test_data_tf = tf.convert_to_tensor(test_windows, dtype=tf.float32)\n",
    "\n",
    "    #reshape\n",
    "    train_data_tf = tf.reshape(train_data_tf, (-1, num_features))\n",
    "    test_data_tf = tf.reshape(test_data_tf, (-1, num_features))\n",
    "\n",
    "    #normalize based on the max value of the train data\n",
    "    max_val = tf.math.reduce_max(train_data_tf, axis=(0, 1))\n",
    "    train_data_tf = tf.math.divide(train_data_tf, max_val)\n",
    "    test_data_tf = tf.math.divide(test_data_tf, max_val)\n",
    "\n",
    "    train_labels_tf = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "    test_labels_tf = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Final train_data size: {train_data_tf.shape}\")\n",
    "        print(f\"Final train_labels size: {train_labels_tf.shape}\")\n",
    "        print(f\"Final test_data size: {test_data_tf.shape}\")\n",
    "        print(f\"Final test_labels size: {test_labels_tf.shape}\")\n",
    "\n",
    "    train_data = CsiData(train_data_tf, train_labels_tf, batch_size, window_size)\n",
    "    test_data = CsiData(test_data_tf, test_labels_tf, batch_size, window_size)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, method, ignorePhases=True, verbose=False):\n",
    "    df['Timestamp'] = round(df['Timestamp'], 4)\n",
    "    data = df.copy()\n",
    "    \n",
    "    columns_to_drop = (['Frame_num', 'Source_address', 'TimeWindow'] + \n",
    "                    [f\"Phase{i}\" for i in notInterestedIndexes] + \n",
    "                    [f\"Ampl{i}\" for i in notInterestedIndexes] + \n",
    "                    [f\"CSI{i}\" for i in notInterestedIndexes])\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    if ignorePhases:\n",
    "        data.drop(columns=[col for col in data.columns if col.startswith('Phase')], inplace=True); #Removes Phase columns\n",
    "\n",
    "    if method == Compression_Method.XY:  \n",
    "        for j in interestedIndexes:\n",
    "            data[f'X{j}'] = data[f\"CSI{j}\"].apply(complex_real)\n",
    "            data[f'Y{j}'] = data[f\"CSI{j}\"].apply(complex_imag)\n",
    "        data.drop(columns=[col for col in data.columns if col.startswith(('Ampl', 'Phase'))], inplace=True); #Removes Ampl and Phase columns\n",
    "    elif method == Compression_Method.AmpPhaseFiltered:\n",
    "        data = filterData(data)\n",
    "\n",
    "    data.drop(columns=[col for col in data.columns if col.startswith('CSI')], inplace=True); #Removes CSI columns\n",
    "    if verbose: print(\"Number of features:\", len(data.columns)) \n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA, check the explained variance ratio and the cumulative explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_PCA(data, n_components, directory, saveGraph=False, plotGraph=True):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    if (plotGraph):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "        plt.title('Cumulative Explained Variance by PCA Components')\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('Cumulative Explained Variance')\n",
    "        plt.grid()\n",
    "        if (saveGraph):\n",
    "            graph_path = os.path.join(directory, 'cumulative_explained_variance.png')\n",
    "            plt.savefig(graph_path)\n",
    "            print(\"Graph saved in: \", graph_path)\n",
    "        plt.show()\n",
    "    \n",
    "    return reduced_df, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Principal Component, find the top \"n\" features that contribute most to the variance of that component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_PC(data, pca, n_components):\n",
    "    loadings = pca.components_\n",
    "    loadings_df = pd.DataFrame(data=loadings.T, index=data.columns, columns=[f'PC{i+1}' for i in range(loadings.shape[0])])\n",
    "    column = []\n",
    "\n",
    "    top_n_features = get_top_n_features(loadings_df, n_components)\n",
    "\n",
    "    for pc, features in top_n_features.items():\n",
    "        #print(f\"Top {n_components} features for {pc}: {features}\") #uncomment to see the top features per PC\n",
    "        for feature in features:\n",
    "            if feature not in column:\n",
    "                column.append(feature)\n",
    "    print(\"available features: \", len(data.columns))\n",
    "    print(\"features used: \", len(column))\n",
    "\n",
    "    difference = set(data.columns) - set(column)\n",
    "    print(\"Unused Features:\", difference)\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    centroids = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (centroids[:-1] + centroids[1:]) / 2 #Defines intervals of centroids\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_centroids = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update centroids to better represent the data\n",
    "        \n",
    "        empty_centroids = np.isnan(new_centroids) #Restore previous cluster if empty\n",
    "        new_centroids[empty_centroids] = centroids[empty_centroids] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_centroids - centroids)) < delta:\n",
    "            break\n",
    "\n",
    "        centroids = new_centroids\n",
    "\n",
    "    quantized_data = centroids[indices]   #Quantize the data based on the final centroids\n",
    "    indices = indices.reshape(data.shape) #Reshape indices to match the original data shape\n",
    "\n",
    "    return quantized_data, centroids, indices\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, clusters, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return clusters[indices]\n",
    "\n",
    "def apply_quantization(reduced_df, lvls):\n",
    "    quantized_data, centroids, indices = lloyd_max_quantization(reduced_df.values, num_levels=lvls)\n",
    "    df_quantized = pd.DataFrame(quantized_data, columns=reduced_df.columns)\n",
    "    return df_quantized, centroids, indices\n",
    "\n",
    "def apply_existing_quantization(data, centroids):\n",
    "    thresholds = (centroids[:-1] + centroids[1:]) / 2\n",
    "    indices = np.digitize(data, thresholds)\n",
    "    quantized_data = centroids[indices] #Quantize the data based on the final centroids\n",
    "    indices = indices.reshape(data.shape) #Reshape indices to match the original data shape\n",
    "\n",
    "    df_quantized = pd.DataFrame(quantized_data, columns=data.columns)\n",
    "    return df_quantized, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self, value=None, frequency=0, left=None, right=None):\n",
    "        self.value = value\n",
    "        self.frequency = frequency\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other): #redefined \"less than\" operator for heapq\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "def build_tree(data):\n",
    "    heap = [Node(value, frequency) for value, frequency in data.items()]  #Init heap\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:  #pop two smallest nodes, merge them and push the merged node back\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = Node(frequency=left.frequency + right.frequency, left=left, right=right)\n",
    "        heapq.heappush(heap, merged) \n",
    "\n",
    "    return heap[0] #root\n",
    "\n",
    "def generate_codes(node, code=\"\", huffman_codes=None):\n",
    "    if huffman_codes is None: \n",
    "        huffman_codes = {}\n",
    "\n",
    "    if node.value is not None: #leaf node case\n",
    "        huffman_codes[node.value] = code\n",
    "    else:\n",
    "        generate_codes(node.left, code + \"0\", huffman_codes)\n",
    "        generate_codes(node.right, code + \"1\", huffman_codes)\n",
    "    return huffman_codes\n",
    "\n",
    "def encode_huffman(data, huffman_codes):\n",
    "    emptyStr = \"\"\n",
    "    return emptyStr.join([huffman_codes[val] for val in data]) \n",
    "\n",
    "def decode_huffman(encoded_data, huffman_codes):\n",
    "    decoded_data = []\n",
    "    code = \"\"\n",
    "    for bit in encoded_data: #traverse the encoded data and searches for the code\n",
    "        code += bit\n",
    "        for key, value in huffman_codes.items():\n",
    "            if value == code: #If found, append the corresponding value to the decoded data, otherwise add another bit to the code\n",
    "                decoded_data.append(key)\n",
    "                code = \"\"\n",
    "                break\n",
    "                \n",
    "    return decoded_data\n",
    "\n",
    "def apply_huffman_encode_per_feature(data):\n",
    "    encoded_df = pd.DataFrame()\n",
    "    huffman_codes = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col]) \n",
    "        if len(freq_per_data) == 1: #If only one unique value, there's no tree, assign it a code of 0\n",
    "            code = {list(freq_per_data.keys())[0]: '0'}\n",
    "        else:\n",
    "            root = build_tree(freq_per_data)\n",
    "            code = generate_codes(root)\n",
    "        encoded_df[col] = data[col].apply(lambda x: encode_huffman([x], code))\n",
    "        huffman_codes[col] = code\n",
    "\n",
    "    return encoded_df, huffman_codes\n",
    "\n",
    "def apply_huffman_decode_per_feature(encoded_data, huffman_codes):\n",
    "    decoded_df = pd.DataFrame()\n",
    "\n",
    "    for col in encoded_data.columns:\n",
    "        decoded_df[col] = decode_huffman(''.join(encoded_data[col]), huffman_codes[col])\n",
    "    return decoded_df\n",
    "\n",
    "def apply_encoding(df_quantized):\n",
    "    encoded_df, huffman_codes = apply_huffman_encode_per_feature(df_quantized)\n",
    "    return encoded_df, huffman_codes\n",
    "\n",
    "def apply_decoding(encoded_df, huffman_codes):\n",
    "    decoded_df = apply_huffman_decode_per_feature(encoded_df.iloc[:, 2:-1], huffman_codes)\n",
    "    return decoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data, verbose=True):\n",
    "    entropy = 0\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col])  # Get frequency of each unique value\n",
    "        total_count = sum(freq_per_data.values())\n",
    "        col_entropy = 0\n",
    "        for count in freq_per_data.values():\n",
    "            p_i = count / total_count  # probability of each unique value\n",
    "            col_entropy += -p_i * np.log2(p_i)  # Entropy formula\n",
    "        if verbose: print(f\"Entropy of column {col}: {col_entropy} bits\")\n",
    "        entropy += col_entropy\n",
    "    return entropy.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the dataset (without CSI components) and save it in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(df, pca, columns):\n",
    "    df_reconstructed = pca.inverse_transform(df.values)\n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=columns)\n",
    "\n",
    "    if method == Compression_Method.XY:\n",
    "        for j in interestedIndexes:\n",
    "            df_reconstructed[f'CSI{j}'] = df_reconstructed.apply(lambda x: complex_rebuild(x[f'X{j}'], x[f'Y{j}']), axis=1)\n",
    "                \n",
    "            #compute back ampl and phases\n",
    "            df_reconstructed[f'Ampl{j}'] = df_reconstructed[f'CSI{j}'].apply(abs)\n",
    "            df_reconstructed[f'Phase{j}'] = df_reconstructed[f'CSI{j}'].apply(cmath.phase)\n",
    "\n",
    "        df_reconstructed.drop(columns=[f'X{j}' for j in interestedIndexes], inplace=True)\n",
    "        df_reconstructed.drop(columns=[f'Y{j}' for j in interestedIndexes], inplace=True)\n",
    "        \n",
    "    return df_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MuStdAmplPaper(data, reconstructed_data, directory, level):\n",
    "    print(\"Plotting MuStdAmplPaper\")\n",
    "    sub_directory = os.path.join(directory, 'MuStdAmplPaper_Comparison')\n",
    "    os.makedirs(sub_directory, exist_ok=True)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    plt.plot(data['Time'], data['MuStdAmplPaper'], label=f'Original MuStdAmplPaper', color=\"blue\" )\n",
    "    plt.plot(reconstructed_data['Time'], reconstructed_data['MuStdAmplPaper'], label=f'Reconstructed MuStdAmplPaper', color=\"green\")\n",
    "    \n",
    "    gt = [min(data['MuStdAmplPaper']) if l == 0 else max(data['MuStdAmplPaper']) for l in reconstructed_data[\"Label\"]]\n",
    "    plt.plot(data['Time'],gt,label=\"Ground-truth\",color=\"r\",ls=\"--\", linewidth=0.5) # per window GT\n",
    "\n",
    "    # Add plot details\n",
    "    plt.title('Amplitude Comparison')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    if (saveCSV): plt.savefig(os.path.join(sub_directory, f'{level}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_needed(source, window_size, num_lvls=-1, verbose=False):\n",
    "    data = source.copy()\n",
    "    num_features = len(data.columns)\n",
    "    number_of_windows = len(data) // window_size\n",
    "    dataset_total_bits = 0 #Sum of the bits needed per frame\n",
    "\n",
    "    #These values will be consistent for every frame and window if num_lvls > 0\n",
    "    bits_per_feature = 0     #Bits needed per feature                                    log2(lvl or num_symbols)\n",
    "    window_frame_bits = 0    #Bits needed per frame (row) inside the window              log2(lvl or num_symbols) * num_features\n",
    "    window_feature_bits = 0  #Bits needed for one feature (col) inside the window        log2(lvl or num_symbols) * window_size\n",
    "    window_total_bits = 0    #Sum of the bits needed per frame (row) inside the window   log2(lvl or num_symbols) * num_features * window_size\n",
    "\n",
    "    if num_lvls > 0:\n",
    "        bits_per_feature = np.ceil(np.log2(num_lvls)).astype(int)\n",
    "        window_frame_bits = bits_per_feature * num_features\n",
    "        window_feature_bits = bits_per_feature * window_size #Bits needed for one feature in the window\n",
    "        window_total_bits = bits_per_feature * num_features * window_size\n",
    "        \n",
    "        dataset_total_bits = window_total_bits * number_of_windows\n",
    "    else: #Calculate the bits needed for each window and average it out if there's no quantization \n",
    "        bits_per_feature, bits_needed_per_frame, window_feature_bits, window_total_bits = {}, {}, {}, {}\n",
    "        for window_idx in range(0, len(data), window_size):\n",
    "            windowed_data = data.iloc[window_idx : window_idx + window_size] #Get the window\n",
    "            bits = {}\n",
    "            \n",
    "            for col in windowed_data.columns:                     \n",
    "                num_symbols = len(windowed_data[col].unique())\n",
    "                bits[col] = np.ceil(np.log2(num_symbols)).astype(int)\n",
    "            avg_bits_per_feature = np.mean(list(bits.values())).round(2)\n",
    "\n",
    "            bits_per_feature[window_idx] = avg_bits_per_feature\n",
    "            bits_needed_per_frame[window_idx] = avg_bits_per_feature * num_features\n",
    "            window_feature_bits[window_idx] = avg_bits_per_feature * window_size #Bits needed for one feature in the window\n",
    "            window_total_bits[window_idx] = avg_bits_per_feature * num_features * window_size\n",
    "\n",
    "        #Average it out for all windows\n",
    "        dataset_total_bits = sum(window_total_bits.values()).round(2)\n",
    "        \n",
    "        bits_per_feature = np.mean(list(bits_per_feature.values())).round(2)\n",
    "        window_frame_bits = np.mean(list(bits_needed_per_frame.values())).round(2) \n",
    "        window_feature_bits = np.mean(list(window_feature_bits.values())).round(2)\n",
    "        window_total_bits = np.mean(list(window_total_bits.values())).round(2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Number of windows: {number_of_windows}\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per feature: {bits_per_feature} bits\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per frame{\"\" if num_lvls>0 else \" per window\"}: {window_frame_bits} bits\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per feature per window: {window_feature_bits} bits\")\n",
    "        print(f\"{\"B\" if num_lvls>0 else \"Average b\"}its needed per window: {window_total_bits} bits\")\n",
    "        print(f\"Total bits needed for the dataset: {dataset_total_bits} bits\")\n",
    "\n",
    "    return bits_per_feature, window_frame_bits, window_feature_bits, window_total_bits, dataset_total_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (partially from another notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredFeaturesPresence = pd.read_csv(\"../datasets/filteredFeaturesPresence3s.csv\")\n",
    "def load_comparison():\n",
    "    filteredFeaturesPresence = pd.read_csv(\"../datasets/filteredFeaturesPresence3s.csv\")\n",
    "    #apply labeling based on GT\n",
    "    filteredFeaturesPresence[\"Label\"]= filteredFeaturesPresence[\"Time\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds))\n",
    "    f1_result = classify_presence(filteredFeaturesPresence, plot_roc=False)\n",
    "    return f1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filtering(df_reconstructed):\n",
    "    reconstructedPresence = df_reconstructed\n",
    "\n",
    "    if method == Compression_Method.AmpPhaseFiltered:\n",
    "        reconstructed_filtered = reconstructedPresence\n",
    "    else:\n",
    "        reconstructed_filtered = filterData2(reconstructedPresence) #removes outliers\n",
    "    reconstructed_filtered.drop(columns=[col for col in reconstructed_filtered.columns if col.startswith('CSI')], inplace=True); #Removes CSI columns\n",
    "\n",
    "    return reconstructed_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classification(reconstructed_filtered):\n",
    "    #compute features\n",
    "    reconstructed_featured = extractWindowedFeatures(reconstructed_filtered,column_indexes = reconstructed_filtered.columns,w2=w2)\n",
    "    reconstructed_featured[\"Label\"] = reconstructed_featured[\"Time\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds)) #assign the ground-truth to a label\n",
    "\n",
    "    #classify\n",
    "    f1_result = classify_presence(reconstructed_featured,plot_roc=False)\n",
    "\n",
    "    return reconstructed_featured, f1_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_classes = ['absent', 'present']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment(directory, scaler=None):\n",
    "    data = None\n",
    "    labels = None\n",
    "\n",
    "    with open(directory, 'rb') as f:\n",
    "        data, labels = pickle.load(f)\n",
    "    \n",
    "    fcolumns = ['mu','sigma']\n",
    "    data = data[:labels.shape[0]]\n",
    "\n",
    "    labels = np.asarray(labels, dtype=np.int32)\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=fcolumns)\n",
    "    df['signal'] = labels\n",
    "    \n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler().fit(df[fcolumns])\n",
    "    df[fcolumns] = scaler.transform(df[fcolumns])\n",
    "\n",
    "    X = df[fcolumns]\n",
    "    y = df['signal']\n",
    "\n",
    "    # one-hot-encoding\n",
    "    y_dummy = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, y_dummy, scaler, fcolumns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directory(directory_type, num_components=0, num_levels=0):\n",
    "    #Generate the path to the model or confusion matrix directory.\n",
    "    #directory_type: 'model' or 'CM'\n",
    "\n",
    "    base_dir = 'models/' if directory_type == 'model' else 'CMs/'\n",
    "    file_extension = '.keras' if directory_type == 'model' else '.png'\n",
    "\n",
    "    if num_components == 0 and num_levels == 0:\n",
    "        return f'{base_dir}benchmark{file_extension}'\n",
    "    \n",
    "    base_path = f'{num_components}_components/{base_dir}' if num_components else base_dir\n",
    "    if num_components == 0:\n",
    "        return f'{base_path}{num_components}components_{directory_type.capitalize()}{file_extension}'\n",
    "    elif num_levels == 0:\n",
    "        return f'{base_path}{num_levels}lvls_{directory_type.capitalize()}{file_extension}'\n",
    "    else:\n",
    "        return f'{base_path}{num_components}components_{num_levels}lvls_{directory_type.capitalize()}{file_extension}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs_annealing = 1\n",
    "num_classes = 2\n",
    "\n",
    "ep = 1.0\n",
    "class GetEpochs(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        global ep\n",
    "        ep += 1\n",
    "\n",
    "def res_to_mean(ev, dim = num_classes):\n",
    "    return np.max(dirichlet.mean(ev.reshape(dim,)+1))\n",
    "\n",
    "def res_to_dirichlet(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    S = np.sum(alpha)\n",
    "    K = 2.0\n",
    "    return dirichlet.mean(alpha), K/S\n",
    "\n",
    "def edl_accuracy(yTrue, yPred):\n",
    "    pred = K.argmax(yPred, axis=1)\n",
    "    truth = K.argmax(yTrue, axis=1)\n",
    "    match = K.reshape(K.cast(K.equal(pred, truth), \"float32\"),(-1,1))\n",
    "    return K.mean(match)\n",
    "\n",
    "def plot_res_beta(ev):\n",
    "    alpha = ev.reshape(2,)+1\n",
    "    plt.figure(figsize=(16,9))\n",
    "    x = np.linspace(0,1,1000)\n",
    "    plt.plot(x, beta.pdf(x, alpha[1], alpha[0]))\n",
    "    x1, x2 = beta.interval(0.95, alpha[1], alpha[0])\n",
    "    areaplot = np.multiply(beta.pdf(x, alpha[1],alpha[0]), rect(x,x1, x2))\n",
    "    plt.fill_between(x, 0, areaplot, alpha=0.5)\n",
    "\n",
    "def results_test (train_dir, test_dir, num_components=0, num_levels=0):\n",
    "    X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dir)\n",
    "    X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dir, scaler)\n",
    "    \n",
    "    #Load the model from the right directory\n",
    "    model_directory = get_directory(directory_type='model')\n",
    "    model_directory = os.path.join(\"./results/benchmark\", model_directory)\n",
    "    print(f\"Loading model from {model_directory}\")\n",
    "    mlp_edl = keras.models.load_model(model_directory, compile=False)\n",
    "\n",
    "    #mlp_edl_scores = np.array([res_to_mean(r, dim=num_classes) for r in mlp_edl.predict(X_test)])\n",
    "    y_predictions_edl = np.array(tf.argmax(mlp_edl.predict(X_test), axis=1))\n",
    "\n",
    "    print(classification_report(y_test, y_predictions_edl, target_names=semantic_classes))\n",
    "    cm = confusion_matrix(y_test, y_predictions_edl)\n",
    "    f1 = f1_score(y_test, y_predictions_edl, average=\"binary\", pos_label=1)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=semantic_classes)\n",
    "    cmdisp = disp.plot(cmap=\"cividis\")\n",
    "\n",
    "    model_directory = get_directory(directory_type='CM', num_components=num_components, num_levels=num_levels)\n",
    "    CM_directory = os.path.join(results_directory, model_directory)\n",
    "    os.makedirs(os.path.dirname(CM_directory), exist_ok=True)\n",
    "    cmdisp.figure_.savefig(CM_directory, bbox_inches='tight')\n",
    "\n",
    "    return round(f1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_edl_experiment(_X_train, _y_train_dummy, num_components=0, num_levels=0):\n",
    "    num_classes = 2\n",
    "    num_epochs_annealing = 22\n",
    "\n",
    "    def KL(alpha):\n",
    "        beta=K.constant(np.ones((1,num_classes)),dtype=\"float32\")\n",
    "        S_alpha = K.sum(alpha,axis=1,keepdims=True)\n",
    "        S_beta = K.sum(beta,axis=1,keepdims=True)\n",
    "        lnB = tf.math.lgamma(S_alpha) - K.sum(tf.math.lgamma(alpha),axis=1,keepdims=True)\n",
    "        lnB_uni = K.sum(tf.math.lgamma(beta),axis=1,keepdims=True) - tf.math.lgamma(S_beta)\n",
    "\n",
    "        dg0 = tf.math.digamma(S_alpha)\n",
    "        dg1 = tf.math.digamma(alpha)\n",
    "\n",
    "        return K.sum((alpha - beta)*(dg1-dg0),axis=1,keepdims=True) + lnB + lnB_uni\n",
    "\n",
    "    # Loss function considering the expected squared error and the KL divergence\n",
    "    def mse_loss(yTrue,yPred):\n",
    "        alpha = yPred + 1\n",
    "        S = K.sum(alpha, axis=1, keepdims=True)\n",
    "        m = alpha / S\n",
    "\n",
    "        # A + B minimises the sum of squared loss, see discussion in EDL paper for the derivation\n",
    "        A = K.sum((yTrue-m)**2, axis=1, keepdims=True)\n",
    "        B = K.sum(alpha*(S-alpha)/(S*S*(S+1)), axis=1, keepdims=True)\n",
    "\n",
    "        # the lambda_t parameter, in this case min{1, t/10} with t the number of epochs\n",
    "        ll = min(1.0, float(ep/float(num_epochs_annealing)))\n",
    "        \n",
    "        alp = yPred*(1-yTrue) + 1 \n",
    "        C =  ll * KL(alp)\n",
    "\n",
    "        return A + B + C\n",
    "    \n",
    "    batch_size = 64\n",
    "    lr = 0.001\n",
    "    epochs = 50\n",
    "    model_edl = tf.keras.models.Sequential()\n",
    "    model_edl.add(tf.keras.layers.Input(shape=(2,)))\n",
    "    model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model_edl.add(tf.keras.layers.Dropout(0.4))\n",
    "    model_edl.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model_edl.add(tf.keras.layers.Dropout(0.4))\n",
    "    model_edl.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model_edl.add(tf.keras.layers.Dense(num_classes, activation='softplus'))\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model_edl.compile(loss=mse_loss, optimizer=optimizer, metrics=[edl_accuracy])\n",
    "    \n",
    "    class_weights = {0: 1.0, 1: 1.30} #class weights to balance the dataset\n",
    "\n",
    "    model_edl.fit(_X_train, _y_train_dummy,\n",
    "      batch_size=batch_size,\n",
    "      epochs=epochs,\n",
    "      class_weight=class_weights,\n",
    "      verbose=1,\n",
    "      shuffle=False)\n",
    "\n",
    "    model_directory = get_directory(directory_type='model', num_components=num_components, num_levels=num_levels)\n",
    "    model_directory = os.path.join(results_directory, model_directory)\n",
    "    os.makedirs(os.path.dirname(model_directory), exist_ok=True)\n",
    "    print(f\"Saving model to {model_directory}\")\n",
    "    model_edl.save(model_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 3 #seconds\n",
    "window_size = 32 # for 3 second\n",
    "step_size = window_size\n",
    "\n",
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)\n",
    "num_features = len(data.columns[1:]) #Excludes the timestamp column (it'll get dropped later)\n",
    "#get_windows_stats(data)\n",
    "train_windows, train_labels, test_windows, test_labels = data_windowing(data, time_window, window_size, step_size, verbose=False)\n",
    "train_data, test_data = CsiData_generator(train_windows, train_labels, test_windows, test_labels, batch_size=window_size, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.csi.shape)\n",
    "print(train_data.labels.shape)\n",
    "print(train_data.indices.shape)\n",
    "\n",
    "print(len(np.unique(train_data.csi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "vae.fit(train_data, epochs=, shuffle=True, callbacks=[checkpoint_cb, csv_logger_cb, early_stopping_cb])\n",
    "vae.save_weights(f'./models/{folder_name}/train_weights_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"-------------- Benchmark Run --------------\")\n",
    "dumps_directory = f'./dumps/benchmark'\n",
    "results_directory = f'./results/benchmark'\n",
    "os.makedirs(dumps_directory, exist_ok=True)\n",
    "\n",
    "vae = VAE(enc_input_shape=(window_size, num_features, 1))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./models/{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "print(\"Encoding train data...\")\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "\n",
    "print(\"Encoding test data...\")\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "train_dump_dir=os.path.join(dumps_directory, 'training/benchmark.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(dumps_directory, 'test/benchmark.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_train, z_labels_train], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_test, z_labels_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_data_train.shape)\n",
    "print(z_data_train.shape)\n",
    "print(z_labels_train.shape)\n",
    "\n",
    "print(np.unique(z_data_train))\n",
    "print(len(np.unique(z_data_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (train_dump_dir, 'rb') as f:\n",
    "    z_data_train, z_labels_train= pickle.load(f)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(len(z_data_train)):\n",
    "    if i % 500 == 0:\n",
    "        # blue for absent, green for present\n",
    "        plt.plot(z_data_train[i][0], z_data_train[i][1], marker='o', color='blue' if z_labels_train[i] == 0 else 'green')\n",
    "plt.title(f'Distribution of train data')\n",
    "plt.xlabel('Mean')\n",
    "plt.ylabel('Variance')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps_directory = f'./dumps/benchmark'\n",
    "results_directory = f'./results/benchmark'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "train_dump_dir=os.path.join(dumps_directory, 'training/benchmark.pkl')\n",
    "test_dump_dir=os.path.join(dumps_directory, 'test/benchmark.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler=scaler)\n",
    "\n",
    "run_edl_experiment(X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "f1_result = results_test(train_dump_dir, test_dump_dir)\n",
    "print(f\"F1 Score: {f1_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "dumps_directory = f'./dumps/benchmark'\n",
    "results_directory = f'./results/benchmark'\n",
    "train_dump_dir=os.path.join(dumps_directory, 'training/benchmark.pkl')\n",
    "test_dump_dir=os.path.join(dumps_directory, 'test/benchmark.pkl')\n",
    "f1_result = results_test(train_dump_dir, test_dump_dir)\n",
    "print(f\"F1 Score: {f1_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./results/SQ', exist_ok=True)\n",
    "os.makedirs('./results_csv/SQ', exist_ok=True)\n",
    "os.makedirs('./results_graphs/SQ', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAE Output Quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 3 #seconds\n",
    "window_size = 32 # for 3 second\n",
    "step_size = window_size\n",
    "\n",
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)\n",
    "num_features = len(data.columns[1:]) #Excludes the timestamp column (it'll get dropped later)\n",
    "train_windows, train_labels, test_windows, test_labels = data_windowing(data, time_window, window_size, step_size, verbose=False)\n",
    "train_data, test_data = CsiData_generator(train_windows, train_labels, test_windows, test_labels, batch_size=window_size, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_data_train = np.zeros([0, 2])\n",
    "z_labels_train = np.zeros([0])\n",
    "\n",
    "vae = VAE(enc_input_shape=(window_size, num_features, 1))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./models/{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps_directory = f'./dumps/SQ/VAE_QNTZD'\n",
    "results_directory = f'./results/SQ/VAE_QNTZD'\n",
    "os.makedirs(dumps_directory, exist_ok=True)\n",
    "\n",
    "bit_results = []\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "\n",
    "df_z_data_train = pd.DataFrame(z_data_train, columns=['z_mean', 'z_log_var'])\n",
    "df_z_data_test = pd.DataFrame(z_data_test, columns=['z_mean', 'z_log_var'])\n",
    "\n",
    "for lvl in levels:\n",
    "   print(f\"-------------- {lvl} lvls --------------\")\n",
    "   df_train_quantized, centroids, train_indices = apply_quantization(df_z_data_train, lvl) #LLoyd-Max quantization\n",
    "   df_test_quantized, test_indices = apply_existing_quantization(df_z_data_test, centroids)\n",
    "   \n",
    "   print (f\"DF_QUANTIZED\")\n",
    "   df_test_indices = pd.DataFrame(test_indices, columns=['z_mean', 'z_log_var'])\n",
    "   QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_indices, window_size, lvl, verbose=True)\n",
    "\n",
    "   z_data_train = df_train_quantized.to_numpy()\n",
    "   z_data_test = df_test_quantized.to_numpy()\n",
    "\n",
    "   sub_dir=os.path.join(dumps_directory, f'training/{lvl}_lvls.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_train, z_labels_train], f)\n",
    "\n",
    "   sub_dir=os.path.join(dumps_directory, f'test/{lvl}_lvls.pkl')\n",
    "   os.makedirs(os.path.dirname(sub_dir), exist_ok=True)\n",
    "   with open(sub_dir, 'wb') as f:\n",
    "      pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "   bit_results.append({\n",
    "         'num_levels': lvl,\n",
    "         'QT_feature': QT_feature,\n",
    "         'QT_frame': QT_frame,\n",
    "         'QT_window': QT_window,\n",
    "         'QT_total': QT_total\n",
    "      })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/SQ/VAE_bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps_directory = f'./dumps/SQ/VAE_QNTZD'\n",
    "results_directory = f'./results/SQ/VAE_QNTZD'\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "results = []\n",
    "\n",
    "for lvl in levels:  \n",
    "    print(f\"-------------- {lvl} lvls --------------\")\n",
    "    train_dump_dir = f'./dumps/benchmark/training/benchmark.pkl'\n",
    "    test_dump_dir=os.path.join(dumps_directory, f'test/{lvl}_lvls.pkl')\n",
    "\n",
    "    # Test model\n",
    "    f1_result = results_test(train_dump_dir, test_dump_dir, num_levels=lvl)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_levels\": lvl,\n",
    "            \"f1_score\": f1_result\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/SQ/VAE_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_f1 = pd.read_csv('results_csv/SQ/VAE_accuracy.csv')\n",
    "df_VAE_f1 = f1_loss(df_VAE_f1, max_score)\n",
    "df_VAE_bits = pd.read_csv('results_csv/SQ/VAE_bits.csv')\n",
    "df_VAE_f1_bits = pd.merge(df_VAE_f1, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_f1_bits.to_csv(f'./results_csv/SQ/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_f1_bits['QT_frame'], df_VAE_f1_bits['f1_loss'], marker='o', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized data (post VAE)')\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.xticks(df_VAE_f1_bits['QT_frame'])\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit_POST-VAE[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_VAE_f1_bits['QT_window'], df_VAE_f1_bits['f1_loss'], marker='o', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized data (post VAE)')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(df_VAE_f1_bits['QT_window'])\n",
    "plt.title('Relative F1-Score Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit_POST-VAE[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "\n",
    "time_window = 3 #seconds\n",
    "window_size = 32 # for 3 second\n",
    "step_size = window_size\n",
    "\n",
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)\n",
    "csi_subcarriers = data.columns[1:]\n",
    "num_features = len(data.columns[1:]) #Excludes the timestamp column (it'll get dropped later)\n",
    "\n",
    "train_windows, train_labels, test_windows, test_labels = data_windowing(data, time_window, window_size, step_size, verbose=False)\n",
    "train_data, test_data = CsiData_generator(train_windows, train_labels, test_windows, test_labels, batch_size=window_size, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = './results/SQ'\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "results = []\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    dumps_directory = f'./dumps/SQ/{num_components}_components'\n",
    "    os.makedirs(dumps_directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=dumps_directory, saveGraph=True, plotGraph=True)\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        filename = f'{num_levels}_lvls'\n",
    "        \n",
    "        #Quantize the data\n",
    "        df_train_quantized, centroids, train_indices = apply_quantization(df_train_reduced, num_levels) #LLoyd-Max quantization\n",
    "        df_test_quantized, test_indices = apply_existing_quantization(df_test_reduced, centroids)\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_train_reconstructed = reconstruct_data(df_train_quantized, pca, csi_subcarriers)\n",
    "        df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "        reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "        train_data.csi = reconstructed_train_data\n",
    "\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        vae = VAE(enc_input_shape=(window_size, num_features, 1))\n",
    "        vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "        vae.load_weights(f'./models/{folder_name}/train_weights_vae').expect_partial()\n",
    "        \n",
    "        print(\"Encoding train data...\")\n",
    "        z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "        \n",
    "        print(\"Encoding test data...\")\n",
    "        z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "        train_dump_dir=os.path.join(dumps_directory, f'training/{filename}.pkl')\n",
    "        os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "        test_dump_dir=os.path.join(dumps_directory, f'test/{filename}.pkl')\n",
    "        os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "        print(\"Saving data...\")\n",
    "        with open(train_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_train, z_labels_train], f)\n",
    "        with open(test_dump_dir, 'wb') as f:\n",
    "            pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "components = [40]\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "\n",
    "\n",
    "for num_components in components:\n",
    "    dumps_directory = f'./dumps/SQ/{num_components}_components'\n",
    "    for num_levels in levels:\n",
    "        test_dump_dir=os.path.join(dumps_directory, f'test/{num_levels}_lvls.pkl')\n",
    "        with open (test_dump_dir, 'rb') as f:\n",
    "            z_data_train, z_labels_train= pickle.load(f)\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        for i in range(len(z_data_train)):\n",
    "            plt.plot(z_data_train[i][0], z_data_train[i][1], marker='o', color='blue' if z_labels_train[i] == 0 else 'green')\n",
    "        plt.title(f'Distribution of data with {num_components} components and {num_levels} levels')\n",
    "        plt.xlabel('Mean')\n",
    "        plt.ylabel('Variance')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] #+ [40]\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "results_directory = './results/SQ'\n",
    "results = []\n",
    "for num_components in components:\n",
    "    dumps_directory = f'./dumps/SQ/{num_components}_components'\n",
    "    for num_levels in levels:  \n",
    "        print(f\"-------------- {num_components} components w/ {num_levels} lvls --------------\")\n",
    "        train_dump_dir = f'./dumps/benchmark/training/benchmark.pkl'\n",
    "        test_dump_dir=os.path.join(dumps_directory, f'test/{num_levels}_lvls.pkl')\n",
    "\n",
    "        # Test model\n",
    "        f1_result = results_test(train_dump_dir, test_dump_dir, num_components, num_levels)\n",
    "        results.append(\n",
    "            {\n",
    "                \"num_components\": num_components,\n",
    "                \"num_levels\": num_levels,\n",
    "                \"f1_score\": f1_result\n",
    "            })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('./results_csv/SQ/accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "time_window = 3 #seconds\n",
    "window_size = 64 # for 3 second\n",
    "step_size = window_size // 2\n",
    "\n",
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)\n",
    "csi_subcarriers = data.columns[1:]\n",
    "num_features = len(csi_subcarriers)\n",
    "\n",
    "train_windows, train_labels, test_windows, test_labels = data_windowing(data, time_window, window_size, step_size, verbose=False)\n",
    "train_data, test_data = CsiData_generator(train_windows, train_labels, test_windows, test_labels, window_size, step_size)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "results_directory = './results/SQ'\n",
    "bit_results = []\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    dumps_directory = f'./dumps/SQ/{num_components}_components'\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=dumps_directory, plotGraph=False)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "    \n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_feature, PCA_frame, PCA_window_feature, PCA_window, PCA_total = bits_needed(df_test_reduced, window_size, verbose=True)\n",
    "\n",
    "    for num_levels in levels:\n",
    "        print(f\"-------------- {num_components} components {num_levels} lvls --------------\")\n",
    "\n",
    "        #Quantize the data\n",
    "        df_train_quantized, centroids, train_indices = apply_quantization(df_train_reduced, num_levels) #LLoyd-Max quantization\n",
    "        df_test_quantized, test_indices = apply_existing_quantization(df_test_reduced, centroids)\n",
    "        print (f\"DF_QUANTIZED\")\n",
    "        df_test_indices = pd.DataFrame(test_indices, columns=[f'PC{i}' for i in range(num_components)])\n",
    "        QT_feature, QT_frame, QT_window_feature, QT_window, QT_total = bits_needed(df_test_indices, window_size, num_levels, verbose=True)\n",
    "\n",
    "        #Huffman\n",
    "        df_encoded_test, huffman_codes_test = apply_encoding(df_test_quantized)\n",
    "        print (f\"DF_ENCODED\")\n",
    "        ENC_feature, ENC_frame, ENC_window_feature, ENC_window, ENC_total = bits_needed(df_encoded_test, window_size, verbose=True)\n",
    "        enc_entropy = compute_entropy(df_encoded_test, verbose=False)\n",
    "        print(f\"Entropy of encoded data: {enc_entropy} bits\")\n",
    "\n",
    "        #Reconstruct the data\n",
    "        df_test_reconstructed = reconstruct_data(df_test_quantized, pca, csi_subcarriers)\n",
    "        print (f\"DF_RECONSTRUCTED\")\n",
    "        REC_feature, REC_frame, REC_window_feature, REC_window, REC_total = bits_needed(df_test_reconstructed, window_size, verbose=True)\n",
    "        \n",
    "        df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "        reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "        test_data.csi = reconstructed_test_data\n",
    "\n",
    "        bit_results.append({\n",
    "            'num_components': num_components,\n",
    "            'num_levels': num_levels,\n",
    "            'PCA_feature': PCA_feature,\n",
    "            'QT_feature': QT_feature,\n",
    "            'ENC_feature': ENC_feature,\n",
    "            'REC_feature': REC_feature,\n",
    "            'PCA_frame': PCA_frame,\n",
    "            'QT_frame': QT_frame,\n",
    "            'ENC_frame': ENC_frame,\n",
    "            'REC_frame': REC_frame,\n",
    "            'QT_window': QT_window,\n",
    "            'PCA_window': PCA_window,\n",
    "            'ENC_window': ENC_window,\n",
    "            'REC_window': REC_window,\n",
    "            'PCA_total': PCA_total,\n",
    "            'QT_total': QT_total,\n",
    "            'ENC_total': ENC_total,\n",
    "            'REC_total': REC_total\n",
    "        })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/SQ/bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "levels = [2**i for i in range(1, 9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv\n",
    "df_f1 = pd.read_csv('results_csv/SQ/accuracy.csv')\n",
    "df_f1 = f1_loss(df_f1, max_score)\n",
    "df_bits = pd.read_csv('results_csv/SQ/bits.csv')\n",
    "df_f1_bit = pd.merge(df_f1, df_bits, on=['num_components', 'num_levels'])\n",
    "df_f1_bit.to_csv(f'./results_csv/SQ/results.csv', index=False)\n",
    "\n",
    "df_VAE_f1 = pd.read_csv('results_csv/SQ/VAE_accuracy.csv')\n",
    "df_VAE_f1 = f1_loss(df_VAE_f1, max_score)\n",
    "df_VAE_bits = pd.read_csv('results_csv/SQ/VAE_bits.csv')\n",
    "df_VAE_f1_bits = pd.merge(df_VAE_f1, df_VAE_bits, on=['num_levels'])\n",
    "\n",
    "df_VAE_f1_bits.to_csv(f'./results_csv/SQ/VAE_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_frame'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_frame'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized data and PCA applied')\n",
    "plt.xticks(np.arange(0, 260, 10))\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_window'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_window'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized data and PCA applied')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(np.arange(0, 17000, 1000))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_frame'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_frame'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per frame')\n",
    "plt.xticks(np.arange(0, 100, 5))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit_Encoded[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_window'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_window'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.xticks(np.arange(0, 6500, 500))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit_Encoded[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for lvl in df_f1_bit['num_levels'].unique():\n",
    "    target_data = df_f1_bit[df_f1_bit['num_levels'] == lvl]\n",
    "\n",
    "    original_bits = target_data['QT_window']\n",
    "    huffman_bits = target_data['ENC_window']\n",
    "\n",
    "    saved_space_percentage = [(orig - huff) / orig * 100 for orig, huff in zip(original_bits, huffman_bits)]\n",
    "    # Plotting\n",
    "    plt.plot(components, saved_space_percentage, marker='o', linestyle='--', label=f'{lvl} levels')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks(components)\n",
    "plt.ylabel('Saved Space (%)')\n",
    "plt.title('Percentage of saved space per window with encoded data versus non-encoded data (quantized data with PCA applied)')\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig(os.path.join('results_graphs/SQ/saved_space_percentage[QT-ENC].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "levels = [2**i for i in range(1, 9)]\n",
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for lvl in df_f1_bit['num_levels'].unique():\n",
    "    target_data = df_f1_bit[df_f1_bit['num_levels'] == lvl]\n",
    "\n",
    "    original_bits = target_data['PCA_window']\n",
    "    huffman_bits = target_data['ENC_window']\n",
    "\n",
    "    saved_space_percentage = [(orig - huff) / orig * 100 for orig, huff in zip(original_bits, huffman_bits)]\n",
    "    # Plotting\n",
    "    plt.plot(components, saved_space_percentage, marker='o', linestyle='--', label=f'{lvl} levels')\n",
    "plt.xlabel('Number of components')\n",
    "plt.xticks(components)\n",
    "plt.ylabel('Saved Space (%)')\n",
    "plt.title('Percentage of saved space per window with encoded data versus non-encoded PCA data')\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig(os.path.join('results_graphs/SQ/saved_space_percentage[PCA-ENC].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "\n",
    "time_window = 3 #seconds\n",
    "window_size = 32 # for 3 second\n",
    "step_size = window_size\n",
    "\n",
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)\n",
    "csi_subcarriers = data.columns[1:]\n",
    "num_features = len(data.columns[1:]) #Excludes the timestamp column (it'll get dropped later)\n",
    "\n",
    "train_windows, train_labels, test_windows, test_labels = data_windowing(data, time_window, window_size, step_size, verbose=False)\n",
    "train_data, test_data = CsiData_generator(train_windows, train_labels, test_windows, test_labels, batch_size=window_size, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = './results/SQ/PCA_ONLY'\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    filename = f'{num_components}_components'\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    dumps_directory = f'./dumps/SQ/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(dumps_directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=dumps_directory, saveGraph=True, plotGraph=True)\n",
    "\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_train_reconstructed = reconstruct_data(df_train_reduced, pca, csi_subcarriers)\n",
    "    df_train_reconstructed = df_train_reconstructed.to_numpy()\n",
    "    reconstructed_train_data = tf.convert_to_tensor(df_train_reconstructed, dtype=tf.float32)\n",
    "    train_data.csi = reconstructed_train_data\n",
    "\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed, dtype=tf.float32)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    vae = VAE(enc_input_shape=(window_size, num_features, 1))\n",
    "    vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "    vae.load_weights(f'./models/{folder_name}/train_weights_vae').expect_partial()\n",
    "    \n",
    "    print(\"Encoding train data...\")\n",
    "    z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "    \n",
    "    print(\"Encoding test data...\")\n",
    "    z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "    train_dump_dir=os.path.join(dumps_directory, f'training/{filename}.pkl')\n",
    "    os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "    test_dump_dir=os.path.join(dumps_directory, f'test/{filename}.pkl')\n",
    "    os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "    print(\"Saving data...\")\n",
    "    with open(train_dump_dir, 'wb') as f:\n",
    "        pickle.dump([z_data_train, z_labels_train], f)\n",
    "    with open(test_dump_dir, 'wb') as f:\n",
    "        pickle.dump([z_data_test, z_labels_test], f)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "results_directory = './results/SQ/PCA_ONLY'\n",
    "results = []\n",
    "\n",
    "for num_components in components:\n",
    "    filename = f'{num_components}_components'\n",
    "    dumps_directory = f'./dumps/SQ/PCA_ONLY/{num_components}_components'\n",
    "\n",
    "    print(f\"-------------- {num_components} components --------------\")\n",
    "    train_dump_dir = f'./dumps/benchmark/training/benchmark.pkl'\n",
    "    test_dump_dir = os.path.join(dumps_directory, f'test/{filename}.pkl')\n",
    "\n",
    "    # Test model\n",
    "    f1_result = results_test(train_dump_dir, test_dump_dir, num_components)\n",
    "    results.append(\n",
    "        {\n",
    "            \"num_components\": num_components,\n",
    "            \"f1_score\": f1_result\n",
    "        })\n",
    "        \n",
    "results_df = pd.DataFrame(results)\n",
    "os.makedirs('results_csv', exist_ok=True)\n",
    "results_df.to_csv('results_csv/SQ/PCA_accuracy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]\n",
    "time_window = 3 #seconds\n",
    "window_size = 64 # for 3 second\n",
    "step_size = window_size // 2\n",
    "\n",
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)\n",
    "csi_subcarriers = data.columns[1:]\n",
    "num_features = len(csi_subcarriers)\n",
    "\n",
    "train_windows, train_labels, test_windows, test_labels = data_windowing(data, time_window, window_size, step_size, verbose=False)\n",
    "train_data, test_data = CsiData_generator(train_windows, train_labels, test_windows, test_labels, window_size, step_size)\n",
    "\n",
    "df_csi_train = pd.DataFrame(train_data.csi.numpy(), columns=csi_subcarriers)\n",
    "df_csi_test = pd.DataFrame(test_data.csi.numpy(), columns=csi_subcarriers)\n",
    "\n",
    "results_directory = './results/SQ/PCA_ONLY'\n",
    "bit_results = []\n",
    "\n",
    "for num_components in components:\n",
    "    print(f\"-------------- {num_components} components ----------------------\")\n",
    "    df_train = df_csi_train.copy()\n",
    "    df_test = df_csi_test.copy()\n",
    "    dumps_directory = f'./dumps/SQ/PCA_ONLY/{num_components}_components'\n",
    "    os.makedirs(dumps_directory, exist_ok=True)\n",
    "\n",
    "    #Apply PCA\n",
    "    df_train_reduced, pca = analyze_PCA(df_train, num_components, directory=dumps_directory, plotGraph=False)\n",
    "    test_reduced = pca.transform(df_test)\n",
    "    df_test_reduced = pd.DataFrame(test_reduced, columns=[f'PC{i}' for i in range(num_components)])\n",
    "\n",
    "    print (f\"DF_CSI_DATA\")\n",
    "    PCA_feature, PCA_frame, PCA_window_feature, PCA_window, PCA_total = bits_needed(df_test_reduced, window_size, verbose=True)\n",
    "\n",
    "    #Reconstruct the data\n",
    "    df_test_reconstructed = reconstruct_data(df_test_reduced, pca, csi_subcarriers)\n",
    "\n",
    "    print (f\"DF_RECONSTRUCTED\")\n",
    "    REC_feature, REC_frame, REC_window_feature, REC_window, REC_total = bits_needed(df_test_reconstructed, window_size, verbose=True)\n",
    "    \n",
    "    df_test_reconstructed = df_test_reconstructed.to_numpy()\n",
    "    reconstructed_test_data = tf.convert_to_tensor(df_test_reconstructed)\n",
    "    test_data.csi = reconstructed_test_data\n",
    "\n",
    "    bit_results.append({\n",
    "        'num_components': num_components,\n",
    "        'PCA_feature': PCA_feature,\n",
    "        'REC_feature': REC_feature,\n",
    "        'PCA_frame': PCA_frame,\n",
    "        'REC_frame': REC_frame,\n",
    "        'PCA_window': PCA_window,\n",
    "        'REC_window': REC_window,\n",
    "        'PCA_total': PCA_total,\n",
    "        'REC_total': REC_total\n",
    "    })\n",
    "\n",
    "bit_results = pd.DataFrame(bit_results)\n",
    "bit_results.to_csv(f'./results_csv/SQ/PCA_bits.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 10)] + [i for i in range (10, 20, 2)] + [i for i in range(20, 35, 5)] + [40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge csv\n",
    "df_f1 = pd.read_csv('results_csv/SQ/PCA_accuracy.csv')\n",
    "df_f1 = f1_loss(df_f1, max_score)\n",
    "df_bits = pd.read_csv('results_csv/SQ/PCA_bits.csv')\n",
    "df_f1_bit = pd.merge(df_f1, df_bits, on=['num_components'])\n",
    "df_f1_bit.to_csv(f'./results_csv/SQ/PCA_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/PCA_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_f1_bit['PCA_frame'], df_f1_bit['f1_loss'], marker='o', linestyle='--')\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40]\n",
    "plt.title('Relative F1-Score loss with PCA applied')\n",
    "plt.xlabel('Average bits per frame')\n",
    "plt.xticks(np.arange(0, 260, 50))\n",
    "plt.title('Relative F1-Score Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit_PCA-Only[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/PCA_results.csv')\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(df_f1_bit['PCA_window'], df_f1_bit['f1_loss'], marker='o', linestyle='--')\n",
    "# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30, 40]\n",
    "plt.title('Relative F1-Score loss with PCA applied')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.xticks(np.arange(0, 17000, 1000))\n",
    "plt.title('Relative F1-Score Loss (%)')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/accuracy-bit_PCA-Only[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0\n",
    "print(x/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/filteredPresence.csv\")\n",
    "data = data_preprocessing(data, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 3 #seconds\n",
    "labels = pd.DataFrame()\n",
    "#print(\"TimeWindow:\", df[\"TimeWindow\"].unique())\n",
    "data[\"TimeWindow\"] = np.floor(data[\"Timestamp\"] / time_window) * time_window\n",
    "labels[\"TimeWindow\"] = data[\"TimeWindow\"].unique()\n",
    "labels[\"Label\"] = labels[\"TimeWindow\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds)) #assign the ground-truth to a label\n",
    "\n",
    "df = data.copy()\n",
    "df.drop(columns=[\"Timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"timewindow_counts = df['TimeWindow'].value_counts(sort=False)\n",
    "\n",
    "print(\"average count:\",timewindow_counts.mean())\n",
    "print(\"max count:\",timewindow_counts.max())\n",
    "print(\"min count:\",timewindow_counts.min())\n",
    "\n",
    "counter = [x for x in timewindow_counts if x < 20]\n",
    "print(\"\\nnumber of windows with less than 20 samples:\",len(counter))\n",
    "print(sorted(counter))\n",
    "\n",
    "counter = [x for x in timewindow_counts if x >= 20 and x < 46]\n",
    "print(\"\\nnumber of windows between 20 and 46:\",len(counter))\n",
    "print(sorted(counter))\n",
    "\n",
    "counter = [x for x in timewindow_counts if x >= 46 and x < 100]\n",
    "print(\"\\nnumber of windows between 46 and 100:\",len(counter))\n",
    "print(sorted(counter))\n",
    "\n",
    "counter = [x for x in timewindow_counts if x >= 100]\n",
    "print(\"\\nnumber of windows with more than 100 samples:\",len(counter))\n",
    "print(sorted(counter))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "#window_size = 16 # for 1 second\n",
    "window_size = 64 # for 3 second\n",
    "\n",
    "# Drop time windows with less than \"window_size\" samples\n",
    "filtered_df = df.groupby('TimeWindow').filter(lambda x: len(x) >= window_size)\n",
    "print(\"Number of time windows dropped:\", len(df['TimeWindow'].unique()) - len(filtered_df['TimeWindow'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside each \"second\" first we divide in train/test and then we divide in windows\n",
    "train_windows = []\n",
    "train_labels = []\n",
    "test_windows = []\n",
    "test_labels = []\n",
    "step_size = window_size // 2\n",
    "\n",
    "for time_window in filtered_df[\"TimeWindow\"].unique():\n",
    "    label = labels[labels[\"TimeWindow\"] == time_window][\"Label\"].values[0]\n",
    "    data_window = filtered_df[filtered_df[\"TimeWindow\"] == time_window].drop(\"TimeWindow\", axis=1)\n",
    "    data_window = np.array(data_window)\n",
    "    train, test = train_test_split(data_window, test_size=0.4, random_state=42, shuffle=False, stratify=None)\n",
    "\n",
    "    # Generate overlapping train windows\n",
    "    for start_idx in range(0, len(train) - window_size + 1, step_size):\n",
    "        window = train[start_idx : start_idx + window_size]\n",
    "        train_windows.append(window)\n",
    "        train_labels.extend([label] * window_size)\n",
    "\n",
    "    # Generate overlapping test windows\n",
    "    for start_idx in range(0, len(test) - window_size + 1, step_size):\n",
    "        window = test[start_idx : start_idx + window_size]\n",
    "        test_windows.append(window)\n",
    "        test_labels.extend([label] * window_size)\n",
    "\n",
    "train_windows = np.array(train_windows)\n",
    "train_labels = np.array(train_labels)\n",
    "test_windows = np.array(test_windows)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "print(\"Original number of windows:\", len(filtered_df[\"TimeWindow\"].unique()))\n",
    "print(\"Number of train windows:\", train_windows.shape)   \n",
    "print(\"Number of train labels:\", train_labels.shape)\n",
    "print(\"Number of test windows:\", test_windows.shape)\n",
    "print(\"Number of test labels:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert in tensor\n",
    "train_data_tf = tf.convert_to_tensor(train_windows, dtype=tf.float32)\n",
    "test_data_tf = tf.convert_to_tensor(test_windows, dtype=tf.float32)\n",
    "\n",
    "#reshape\n",
    "train_data_tf = tf.reshape(train_data_tf, (-1, 56))\n",
    "test_data_tf = tf.reshape(test_data_tf, (-1, 56))\n",
    "\n",
    "#normalize based on the max value of the train data\n",
    "max_val = tf.math.reduce_max(train_data_tf, axis=(0, 1))\n",
    "train_data_tf = tf.math.divide(train_data_tf, max_val)\n",
    "test_data_tf = tf.math.divide(test_data_tf, max_val)\n",
    "\n",
    "train_labels_tf = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "test_labels_tf = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
    "\n",
    "train_indices_tf = tf.range(0, train_data_tf.shape[0] - window_size, dtype=tf.int32)\n",
    "test_indices_tf = tf.range(0, train_data_tf.shape[0] - window_size, dtype=tf.int32)\n",
    "\n",
    "print(f\"Final train_data size: {train_data_tf.shape}\")\n",
    "print(f\"Final train_labels size: {train_labels_tf.shape}\")\n",
    "print(f\"Final test_data size: {test_data_tf.shape}\")\n",
    "print(f\"Final test_labels size: {test_labels_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CsiData(train_data_tf, train_labels_tf, batch_size=step_size, window_size=window_size)\n",
    "test_data = CsiData(test_data_tf, test_labels_tf, batch_size=step_size, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.csi.shape)\n",
    "print(train_data.labels.shape)\n",
    "\n",
    "print(len(np.unique(train_data.csi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "vae.fit(train_data, epochs=3, shuffle=True, callbacks=[checkpoint_cb, csv_logger_cb, early_stopping_cb])\n",
    "vae.save_weights(f'./models/{folder_name}/train_weights_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "print(f\"-------------- Benchmark Run --------------\")\n",
    "dumps_directory = f'./dumps/{folder_name}/benchmark'\n",
    "os.makedirs(dumps_directory, exist_ok=True)\n",
    "\n",
    "vae = VAE(enc_input_shape=(64, 56, 1))\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.load_weights(f'./models/{folder_name}/train_weights_vae').expect_partial()\n",
    "\n",
    "print(\"Encoding train data...\")\n",
    "z_data_train, z_labels_train = apply_vae_encoder(vae, train_data)\n",
    "\n",
    "print(\"Encoding test data...\")\n",
    "z_data_test, z_labels_test = apply_vae_encoder(vae, test_data)\n",
    "\n",
    "train_dump_dir=os.path.join(dumps_directory, 'training/benchmark.pkl')\n",
    "os.makedirs(os.path.dirname(train_dump_dir), exist_ok=True)\n",
    "test_dump_dir=os.path.join(dumps_directory, 'test/benchmark_test.pkl')\n",
    "os.makedirs(os.path.dirname(test_dump_dir), exist_ok=True)\n",
    "with open(train_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_train, z_labels_train], f)\n",
    "with open(test_dump_dir, 'wb') as f:\n",
    "    pickle.dump([z_data_test, z_labels_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z_data_train.shape)\n",
    "print(z_data_train.shape)\n",
    "print(z_labels_train.shape)\n",
    "\n",
    "print(np.unique(z_data_train))\n",
    "print(len(np.unique(z_data_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps_directory = f'./dumps/{folder_name}/benchmark'\n",
    "results_directory = f'./results/{folder_name}/benchmark'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "train_dump_dir=os.path.join(dumps_directory, 'training/benchmark.pkl')\n",
    "test_dump_dir=os.path.join(dumps_directory, 'test/benchmark_test.pkl')\n",
    "results = []\n",
    "\n",
    "print(\"-------------- Training and testing DL model --------------\")\n",
    "X_train, y_train, y_train_dummy, scaler, fcolumns = load_experiment(train_dump_dir)\n",
    "X_test, y_test, y_test_dummy, _, fcolumns = load_experiment(test_dump_dir, scaler=scaler)\n",
    "\n",
    "run_edl_experiment(X_train, y_train_dummy)\n",
    "\n",
    "# Test model\n",
    "f1_result = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "results_directory = f'./results/{folder_name}/benchmark'\n",
    "dumps_directory = f'./dumps/{folder_name}/benchmark'\n",
    "train_dump_dir=os.path.join(dumps_directory, 'training/benchmark.pkl')\n",
    "test_dump_dir=os.path.join(dumps_directory, 'test/benchmark_test.pkl')\n",
    "\n",
    "f1_result = results_test(train_dump_dir, test_dump_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "components = [1,2,3,4,5,8,10,12,14,16,18]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_frame'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_frame'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized data and PCA applied')\n",
    "plt.xticks(np.arange(0, 160, 10))\n",
    "plt.xlabel('Bits per frame')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/Comparison/accuracy-bit[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['QT_window'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_window'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized data and PCA applied')\n",
    "plt.xlabel('Bits per window')\n",
    "plt.xticks(np.arange(0, 11000, 1000))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/Comparison/accuracy-bit[BxW].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_frame'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_frame'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per frame')\n",
    "plt.xticks(np.arange(0, 65, 5))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/Comparison/accuracy_bit_Encoded[BxF].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f1_bit = pd.read_csv(f'./results_csv/SQ/results.csv')\n",
    "df_VAE_f1_bits = pd.read_csv(f'./results_csv/SQ/VAE_results.csv')\n",
    "\n",
    "#components = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "#components = [1, 2, 3, 4, 5, 10, 20, 30, 40]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for num_components in df_f1_bit['num_components'].unique():\n",
    "    if num_components not in components:\n",
    "        continue\n",
    "    target_data = df_f1_bit[df_f1_bit['num_components'] == num_components]\n",
    "    plt.plot(target_data['ENC_window'], target_data['f1_loss'], marker='o', linestyle='--', label=f'{num_components} components')\n",
    "plt.plot(df_VAE_f1_bits['QT_window'], target_data['f1_loss'], marker='o', label='POST-VAE', linewidth=2)\n",
    "plt.title('Relative F1-Score loss with quantized and encoded data and PCA applied')\n",
    "plt.xlabel('Average bits per window')\n",
    "plt.xticks(np.arange(0, 4500, 500))\n",
    "plt.ylabel('Relative F1-Score Loss (%)')\n",
    "plt.yticks(np.arange(0, 70, 5))\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('results_graphs/SQ/Comparison/accuracy_bit_Encoded[BxW].png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
