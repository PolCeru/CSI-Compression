{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE training and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import cmath\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from enum import Enum\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc, confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.stats import dirichlet\n",
    "from scipy.cluster import vq\n",
    "\n",
    "\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"]= '1' # Use legacy keras for compatibility\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "random_state = 42\n",
    "random.seed(random_state)\n",
    "np.random.seed(random_state) # predictable random numbers, for demonstration only\n",
    "tf.random.set_seed(random_state) # reproducibility\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # make operations deterministic\n",
    "os.environ['PYTHONHASHSEED'] = str(random_state) # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compression_Method(Enum):\n",
    "    XY = 1               #applies PCA on X and Y then filters (1)\n",
    "    AmpPhase = 2         #applies PCA on Amplitude and Phase then filters (2)\n",
    "    AmpPhaseFiltered = 3 #applies PCA on Amplitude and Phase after filtering (3)\n",
    "\n",
    "#Modify this to change the approach used: XY, AmpPhase, AmpPhaseFiltered\n",
    "method = Compression_Method.AmpPhase\n",
    "scaler = StandardScaler()\n",
    "ignorePhases = True\n",
    "saveCSV = True\n",
    "\n",
    "latent_dim = 1 \n",
    "num_activities = 2 #present or absent\n",
    "\n",
    "folder_name = f'datasets/vae_weights'\n",
    "base_directory = 'results'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "os.makedirs(base_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class Sampling(tf_keras.layers.Layer):\\n    def call(self, inputs):\\n        z_mean, z_log_var = inputs\\n        batch = tf.shape(z_mean)[0]\\n        dim = tf.shape(z_mean)[1]\\n        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\\n    \\ndef create_csi_encoder(input_shape, latent_dim):\\n    encoder_inputs = tf_keras.Input(shape=input_shape)\\n    x = tf.keras.layers.Reshape((*input_shape, 1))(encoder_inputs)  # (H, W) -> (H, W, 1)\\n    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='valid')(x)\\n    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='valid')(x)\\n    x = tf_keras.layers.Flatten()(x)\\n    x = tf_keras.layers.Dense(4, activation='relu')(x)\\n\\n    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\\n    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\\n    z = Sampling()([z_mean, z_log_var])\\n\\n    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\\n\\n\\ndef create_csi_decoder(input_shape, latent_dim, out_filter):\\n    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\\n    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\\n    x = tf_keras.layers.Reshape(input_shape)(x)\\n    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\\n    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\\n    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, (3, 3), activation='sigmoid', padding='same')(x)\\n\\n    decoder_outputs = tf.keras.layers.Reshape(input_shape[:-1])(decoder_outputs)  # (H, W, 1) -> (H, W)\\n\\n    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class Sampling(tf_keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Reshape((*input_shape, 1))(encoder_inputs)  # (H, W) -> (H, W, 1)\n",
    "    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='valid')(x)\n",
    "    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='valid')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(4, activation='relu')(x)\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder_outputs = tf.keras.layers.Reshape(input_shape[:-1])(decoder_outputs)  # (H, W, 1) -> (H, W)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf_keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "def create_csi_encoder(input_shape, latent_dim):\n",
    "    encoder_inputs = tf_keras.Input(shape=input_shape)\n",
    "    x = tf_keras.layers.Conv2D(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(encoder_inputs)\n",
    "    x = tf_keras.layers.Conv2D(64, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2D(128, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = tf_keras.layers.Flatten()(x)\n",
    "    x = tf_keras.layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "    z_mean = tf_keras.layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = tf_keras.layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    return tf_keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "\n",
    "\n",
    "def create_csi_decoder(input_shape, latent_dim, out_filter):\n",
    "    decoder_inputs = tf_keras.Input(shape=(latent_dim,))\n",
    "    x = tf_keras.layers.Dense(math.prod(input_shape), activation='relu')(decoder_inputs)\n",
    "    x = tf_keras.layers.Reshape(input_shape)(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(128, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    x = tf_keras.layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=(2, 2), padding='same')(x)\n",
    "    decoder_outputs = tf_keras.layers.Conv2DTranspose(out_filter, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return tf_keras.Model(decoder_inputs, decoder_outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf_keras.Model):\n",
    "    def __init__(self, enc_input_shape=(16, 56, 1), dec_input_shape=(2, 7, 128), latent_dim=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_csi_encoder(enc_input_shape, latent_dim)\n",
    "        self.decoder = create_csi_decoder(dec_input_shape, latent_dim, enc_input_shape[-1])\n",
    "        self.total_loss_tracker = tf_keras.metrics.Mean(name='total_loss')\n",
    "        self.reconstruction_loss_tracker = tf_keras.metrics.Mean(name='reconstruction_loss')\n",
    "        self.kl_loss_tracker = tf_keras.metrics.Mean(name='kl_loss')\n",
    "\n",
    "        self.encoder.summary()\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data[0])\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf_keras.losses.binary_crossentropy(data[0], reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            'loss': self.total_loss_tracker.result(),\n",
    "            'reconstruction_loss': self.reconstruction_loss_tracker.result(),\n",
    "            'kl_loss': self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_vae_encoder(vae, source):\n",
    "    #Use the VAE to process CSI data\n",
    "    z_data = np.zeros([0, 4])\n",
    "    z_labels = np.zeros([0])\n",
    "\n",
    "    for (data, labels) in source:\n",
    "        labels = tf.squeeze(labels)\n",
    "        z_mean, z_log_var, _ = vae.encoder.predict(data, verbose=0)\n",
    "        z_tmp = np.concatenate([z_mean, z_log_var], axis=1)\n",
    "        z_data = np.concatenate([z_data, z_tmp], axis=0)\n",
    "        z_labels = np.concatenate([z_labels, labels.numpy().ravel()], axis=0)\n",
    "        \n",
    "    return z_data, z_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = f'./{folder_name}/' + 'cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "checkpoint_cb = tf_keras.callbacks.ModelCheckpoint(checkpoint_path, verbose=1, save_weights_only=True)\n",
    "early_stopping_cb = tf_keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
    "csv_logger_cb = tf_keras.callbacks.CSVLogger(f'./{folder_name}/model_history_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notInterestedIndexes = list(range(-32,-28)) + list(range(0,1)) + list(range(29,32)) #null columns in the dataset\n",
    "interestedIndexes = list(range(-28,0)) + list(range(1,29)) #non null columns in the dataset\n",
    "\n",
    "w1=5 #for filtering\n",
    "w2=3 #for windows\n",
    "#w2=1 #1 second per window\n",
    "lambda1=3 #threshold\n",
    "\n",
    "#build ground truth\n",
    "t2 = 1205\n",
    "lb1 = [120,360,600,900]\n",
    "ub1 = [240,480,720,1080]\n",
    "lb2 = [t2+l for l in [180,540,990,1500]]\n",
    "ub2 = [t2+u for u in [360,750,1170,1590]]\n",
    "\n",
    "lower_bounds = lb1+lb2\n",
    "upper_bounds = ub1+ub2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGT(timestamp,lower_bounds,upper_bounds):\n",
    "    # if I'm in the room in one case, or I'm crossing the entrance put 1\n",
    "    for i in range (0, len(lower_bounds)):\n",
    "        if (timestamp >= lower_bounds[i]) & (timestamp <= upper_bounds[i]):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def classify_presence(df,ycol=\"MuStdAmplPaper\",gt=\"Label\",plot_roc=False,num_iter=1000):\n",
    "    # Y are the labels that indicate if i'm passing or not\n",
    "    Y = df[gt]\n",
    "    # thr is the threshold: if amplitude > thr, then assign to Y_pred 1 (presence), otherwise 0. Every time update the threshold\n",
    "    thr= df[ycol].min()\n",
    "    tpr = []\n",
    "    fpr= []\n",
    "    thr_list= []\n",
    "    step = (df[ycol].max() - df[ycol].min()) / num_iter\n",
    "    while thr <= df[ycol].max():\n",
    "        # compute the predictions\n",
    "        Y_pred = df.apply(lambda row: 1 if row[ycol] >= thr else 0, axis=1)\n",
    "        tn, fp, fn, tp = confusion_matrix(Y, Y_pred).ravel()\n",
    "        # compute True Positive Rate and False Positive rate to plot the roc curve\n",
    "        tpr.append(tp/(tp+fn))\n",
    "        fpr.append(fp/(fp+tn))\n",
    "        thr_list.append(thr)\n",
    "        thr += step\n",
    "    \n",
    "    if plot_roc:\n",
    "        plt.figure(figsize=(3,3),dpi=220)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.plot([0, 1], [0, 1], color = 'green')\n",
    "        plt.xlim(-0.05, 1.05)\n",
    "        plt.ylim(-0.05, 1.05)\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.title(\"ROC curve\")\n",
    "        plt.show()\n",
    "    auc_score = auc(fpr,tpr)    \n",
    "    \n",
    "    return auc_score\n",
    "\n",
    "def extractWindowedFeatures(data, column_indexes = [], w2=3):\n",
    "    data[\"TimeWindow\"] = np.floor(data[\"Timestamp\"] / w2)*w2\n",
    "    #vertical mean/std\n",
    "    dataStd = data.groupby(by=\"TimeWindow\").std().drop([\"Timestamp\",\"Frame_num\"],axis=1)\n",
    "    #dataMean = data.groupby(by=\"TimeWindow\").mean().drop([\"Timestamp\",\"Frame_num\"],axis=1)\n",
    "    \n",
    "    featuredDf = pd.DataFrame()\n",
    "    featuredDf[\"Time\"] = data[\"TimeWindow\"].unique()\n",
    "    #horizontal\n",
    "    featuredDf[\"MuStdAmplPaper\"] = dataStd[[j for j in column_indexes if j.startswith('Ampl')]].mean(axis=1).reset_index(drop=True) #Axis=1: mean over different columns -> into one col\n",
    "    return featuredDf\n",
    "\n",
    "#removes outliers from the data\n",
    "def filterData(df,w1=3,lambda1=3):\n",
    "    data = df.copy()\n",
    "    col_list = [j for j in data.columns if \"Ampl\" in j]\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if index == 0:\n",
    "            prev_row = row\n",
    "            continue\n",
    "        if (index%10000 == 0): print(index)\n",
    "        subDf = data.loc[(data[\"Timestamp\"]<=row['Timestamp']) & (data[\"Timestamp\"]> row['Timestamp'] - w1),col_list]\n",
    "        means = subDf.mean(axis=0)\n",
    "        stds = subDf.std(axis=0)\n",
    "\n",
    "        for c in col_list: \n",
    "            if (abs(row[c] - means[c]) / stds[c]) > lambda1:\n",
    "                data.at[index,c] = prev_row[c]\n",
    "                #row[c] = prev_row[c]\n",
    "\n",
    "        prev_row = row\n",
    "    return data\n",
    "\n",
    "def filterData2(df, w1=3, lambda1=3):\n",
    "    data = df.copy()\n",
    "    col_list = [j for j in data.columns if \"Ampl\" in j]\n",
    "    \n",
    "    # Rolling window to calculate means and std deviations, shifted to exclude the current row\n",
    "    rolling_means = data[col_list].rolling(window=w1, min_periods=1).mean().shift(1)\n",
    "    rolling_stds = data[col_list].rolling(window=w1, min_periods=1).std().shift(1)\n",
    "\n",
    "    # Start at second row since the first row is skipped in original logic\n",
    "    for index in range(1, len(data)):\n",
    "        if index % 10000 == 0: \n",
    "            print(index)\n",
    "        \n",
    "        for c in col_list:\n",
    "            current_val = data.at[index, c]\n",
    "            mean_val = rolling_means.at[index, c]\n",
    "            std_val = rolling_stds.at[index, c]\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if pd.isna(std_val) or std_val == 0:\n",
    "                continue\n",
    "            \n",
    "            # Check if the current value is out of bounds\n",
    "            if abs(current_val - mean_val) / std_val > lambda1:\n",
    "                # Set value to previous row's value if condition is met\n",
    "                data.at[index, c] = data.at[index-1, c]\n",
    "                rolling_means.at[index, c] = data.at[index, c] # Update mean with corrected value\n",
    "                \n",
    "    return data\n",
    "\n",
    "def complex_real(complex_value):\n",
    "    return complex(complex_value).real\n",
    "\n",
    "def complex_imag(complex_value):\n",
    "    return complex(complex_value).imag\n",
    "\n",
    "def complex_rebuild(real,imag):\n",
    "    return (real + 1j*imag)\n",
    "\n",
    "#Function to get top N features for each principal component\n",
    "def get_top_n_features(loadings_df, n):\n",
    "    top_features = {}\n",
    "    for pc in loadings_df.columns:\n",
    "        top_features[pc] = loadings_df[pc].abs().sort_values(ascending=False).head(n).index.tolist()\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyd_max_quantization(data, num_levels=16, max_iter=100, delta=1e-6):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    clusters = np.linspace(min_val, max_val, num_levels) #Uniformly spaced \n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        thresholds = (clusters[:-1] + clusters[1:]) / 2 #Defines intervals of clusters\n",
    "        indices = np.digitize(data, thresholds) #Assign each data point to a cluster\n",
    "        \n",
    "        new_clusters = np.array([data[indices == i].mean() for i in range(num_levels)]) #Update clusters to better represent the data\n",
    "        \n",
    "        empty_clusters = np.isnan(new_clusters) #Restore previous cluster if empty\n",
    "        new_clusters[empty_clusters] = clusters[empty_clusters] \n",
    "\n",
    "        #stop if changes between iterations are small\n",
    "        if np.max(np.abs(new_clusters - clusters)) < delta:\n",
    "            break\n",
    "\n",
    "        clusters = new_clusters\n",
    "\n",
    "    #Quantize the data based on the final clusters\n",
    "    quantized_data = clusters[indices]\n",
    "\n",
    "    return quantized_data, clusters, thresholds\n",
    "\n",
    "def dequantize_lloyd_max(quantized_data, clusters, thresholds):\n",
    "    indices = np.digitize(quantized_data, thresholds, right=True)\n",
    "    return clusters[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node: \n",
    "    def __init__(self, value=None, frequency=0, left=None, right=None):\n",
    "        self.value = value\n",
    "        self.frequency = frequency\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, other): #redefined \"less than\" operator for heapq\n",
    "        return self.frequency < other.frequency\n",
    "\n",
    "def build_tree(data):\n",
    "    heap = [Node(value, frequency) for value, frequency in data.items()]  #Init heap\n",
    "    heapq.heapify(heap)\n",
    "\n",
    "    while len(heap) > 1:  #pop two smallest nodes, merge them and push the merged node back\n",
    "        left = heapq.heappop(heap)\n",
    "        right = heapq.heappop(heap)\n",
    "        merged = Node(frequency=left.frequency + right.frequency, left=left, right=right)\n",
    "        heapq.heappush(heap, merged) \n",
    "\n",
    "    return heap[0] #root\n",
    "\n",
    "def generate_codes(node, code=\"\", huffman_codes=None):\n",
    "    if huffman_codes is None: \n",
    "        huffman_codes = {}\n",
    "\n",
    "    if node.value is not None: #leaf node case\n",
    "        huffman_codes[node.value] = code\n",
    "        return\n",
    "    else:\n",
    "        generate_codes(node.left, code + \"0\", huffman_codes)\n",
    "        generate_codes(node.right, code + \"1\", huffman_codes)\n",
    "    return huffman_codes\n",
    "\n",
    "def encode_huffman(data, huffman_codes):\n",
    "    emptyStr = \"\"\n",
    "    return emptyStr.join([huffman_codes[val] for val in data]) \n",
    "\n",
    "def decode_huffman(encoded_data, huffman_codes):\n",
    "    decoded_data = []\n",
    "    code = \"\"\n",
    "    for bit in encoded_data: #traverse the encoded data and searches for the code\n",
    "        code += bit\n",
    "        for key, value in huffman_codes.items():\n",
    "            if value == code: #If found, append the corresponding value to the decoded data, otherwise add another bit to the code\n",
    "                decoded_data.append(key)\n",
    "                code = \"\"\n",
    "                break\n",
    "                \n",
    "    return decoded_data\n",
    "\n",
    "def apply_huffman_encode_per_feature(data):\n",
    "    encoded_df = pd.DataFrame()\n",
    "    huffman_codes = {}\n",
    "\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col]) \n",
    "        root = build_tree(freq_per_data)\n",
    "        code = generate_codes(root)\n",
    "        #print(\"data[\"+ col +\"]:\\n\", data[col])\n",
    "        encoded_df[col] = data[col].apply(lambda x: encode_huffman([x], code))\n",
    "        huffman_codes[col] = code\n",
    "    return encoded_df, huffman_codes\n",
    "\n",
    "def apply_huffman_decode_per_feature(encoded_data, huffman_codes):\n",
    "    decoded_df = pd.DataFrame()\n",
    "\n",
    "    for col in encoded_data.columns:\n",
    "        decoded_df[col] = decode_huffman(''.join(encoded_data[col]), huffman_codes[col])\n",
    "    return decoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bits_needed(source, df, num_lvls=-1, verbose=True):\n",
    "    data = source.copy()\n",
    "    data[\"TimeWindow\"] = df[\"TimeWindow\"]\n",
    "    bits_needed_unique = {}\n",
    "    bits_needed_window = {}\n",
    "    avg_bits_needed = {}\n",
    "    total_bits_needed_dataset = 0\n",
    "    \n",
    "    for window in data[\"TimeWindow\"].unique():\n",
    "        data_window = data[data[\"TimeWindow\"] == window].drop(\"TimeWindow\", axis=1)        \n",
    "        for col in data_window.columns:\n",
    "            num_symbols = len(data_window[col].unique())\n",
    "            if num_lvls > 0:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_lvls)).astype(int) \n",
    "            else:\n",
    "                bits_needed_unique[col] = np.ceil(np.log2(num_symbols)).astype(int)  # Number of bits to represent each symbol\n",
    "            #print(f\"Column: {col}, Bits needed: {bits_needed[col]} bits\")\n",
    "            \n",
    "        avg_bits_needed[window] = np.mean(list(bits_needed_unique.values())).round(2)\n",
    "        bits_needed_window[window] = sum(bits_needed_unique.values())\n",
    "        total_bits_needed_dataset += sum(bits_needed_unique.values())\n",
    "\n",
    "    bits_needed = np.mean(list(avg_bits_needed.values())).round(2)\n",
    "    bits_needed_window = np.mean(list(bits_needed_window.values())).round(2)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nGlobal metrics:\")\n",
    "        print(f\"Average bits: {bits_needed:.2f} bits\")\n",
    "        print(f\"Average bits per window: {bits_needed_window:.2f} bits\")\n",
    "        print(f\"Bits for the whole dataset: {total_bits_needed_dataset:.2f} bits\")\n",
    "\n",
    "    return bits_needed, bits_needed_window, total_bits_needed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSI Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CsiData(tf_keras.utils.Sequence):\n",
    "    def __init__(self, csi, labels, indices, batch_size=7, window_size=14):\n",
    "        self.csi = csi\n",
    "        self.labels = labels\n",
    "        self.indices = indices\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.antennas = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.indices.shape[-1] / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, batch_idx):\n",
    "        first_idx = batch_idx * self.batch_size\n",
    "        last_idx = (batch_idx + 1) * self.batch_size\n",
    "\n",
    "        #print(f'first_idx: {first_idx}, last_idx: {last_idx}')\n",
    "        \n",
    "        data_batch = [self.csi[x:x + self.window_size, ...] for x in range(first_idx, last_idx)]\n",
    "        labels_batch = np.transpose([self.labels[first_idx:last_idx]])\n",
    "\n",
    "        data_batch = tf.convert_to_tensor(data_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "\n",
    "        if self.antennas == 1:\n",
    "            data_batch = tf.expand_dims(data_batch, 3)\n",
    "            labels_batch = tf.expand_dims(labels_batch, 2)\n",
    "\n",
    "        return data_batch, labels_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, method):\n",
    "    df['Timestamp'] = round(df['Timestamp'], 4)\n",
    "    data = df.copy()\n",
    "    \n",
    "    columns_to_drop = (['Frame_num', 'Source_address', 'TimeWindow'] + \n",
    "                    [f\"Phase{i}\" for i in notInterestedIndexes] + \n",
    "                    [f\"Ampl{i}\" for i in notInterestedIndexes] + \n",
    "                    [f\"CSI{i}\" for i in notInterestedIndexes])\n",
    "    data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "    if ignorePhases:\n",
    "        data.drop(columns=[col for col in data.columns if col.startswith('Phase')], inplace=True); #Removes Phase columns\n",
    "\n",
    "    if method == Compression_Method.XY:  \n",
    "        for j in interestedIndexes:\n",
    "            data[f'X{j}'] = data[f\"CSI{j}\"].apply(complex_real)\n",
    "            data[f'Y{j}'] = data[f\"CSI{j}\"].apply(complex_imag)\n",
    "        data.drop(columns=[col for col in data.columns if col.startswith(('Ampl', 'Phase'))], inplace=True); #Removes Ampl and Phase columns\n",
    "    elif method == Compression_Method.AmpPhaseFiltered:\n",
    "        data = filterData(data)\n",
    "\n",
    "    data.drop(columns=[col for col in data.columns if col.startswith('CSI')], inplace=True); #Removes CSI columns\n",
    "    #data.set_index('Timestamp', inplace=True)\n",
    "    print(\"Number of features:\", len(data.columns))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many components are needed to have an explanation of 95% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_n_components(data, target, directory):\n",
    "    #Fit and transform the data\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    #Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_data)\n",
    "\n",
    "    var_cumulative = np.cumsum(pca.explained_variance_ratio_)*100\n",
    "\n",
    "    #finds PCs that explain 95% of the variance\n",
    "    k = np.argmax(var_cumulative > target) + 1\n",
    "    print(f\"Number of components explaining {target}% variance: \"+ str(k))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title('Cumulative Explained Variance explained by the components')\n",
    "    plt.ylabel('Cumulative Explained variance')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.axvline(x=k, color=\"r\", linestyle=\"--\")\n",
    "    plt.axhline(y=target, color=\"r\", linestyle=\"--\")\n",
    "    plt.plot(range(1, pca.n_components_ + 1), var_cumulative, marker='o', linestyle='--')\n",
    "    plt.grid()\n",
    "    if (saveCSV): plt.savefig(os.path.join(directory, 'var_cumulative_x_component.png'))\n",
    "    plt.show()\n",
    "\n",
    "    return scaled_data, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA, check the explained variance ratio and the cumulative explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_PCA(scaled_data, n_components, directory):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "    reduced_df = pd.DataFrame(data=reduced_data, columns=[f'PC{i}' for i in range(n_components)])\n",
    "\n",
    "    #Explained variance ratio\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    print(\"Explained variance ratio:\", explained_variance_ratio)\n",
    "\n",
    "    #Cumulative explained variance\n",
    "    cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
    "    print(\"Final Cumulative Explained Variance:\", cumulative_explained_variance[-1])\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, n_components + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
    "    plt.title('Cumulative Explained Variance by PCA Components')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.grid()\n",
    "    if (saveCSV): plt.savefig(os.path.join(directory, 'zoomed_var_cumulative_x_component.png'))\n",
    "    plt.show()\n",
    "    \n",
    "    return reduced_df, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each Principal Component, find the top \"n\" features that contribute most to the variance of that component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_PC(data, pca, n_components):\n",
    "    loadings = pca.components_\n",
    "    loadings_df = pd.DataFrame(data=loadings.T, index=data.columns, columns=[f'PC{i+1}' for i in range(loadings.shape[0])])\n",
    "    column = []\n",
    "\n",
    "    top_n_features = get_top_n_features(loadings_df, n_components)\n",
    "\n",
    "    for pc, features in top_n_features.items():\n",
    "        #print(f\"Top {n_components} features for {pc}: {features}\") #uncomment to see the top features per PC\n",
    "        for feature in features:\n",
    "            if feature not in column:\n",
    "                column.append(feature)\n",
    "    print(\"available features: \", len(data.columns))\n",
    "    print(\"features used: \", len(column))\n",
    "\n",
    "    difference = set(data.columns) - set(column)\n",
    "    print(\"Unused Features:\", difference)\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lloyd-Max Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quantization(reduced_df, lvls):\n",
    "    df_quantized = reduced_df.apply(lambda col: lloyd_max_quantization(col.values, num_levels=lvls)[0])\n",
    "    return df_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy Coding (Huffman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data, verbose=True):\n",
    "    entropy = 0\n",
    "    for col in data.columns:\n",
    "        freq_per_data = Counter(data[col])  # Get frequency of each unique value\n",
    "        total_count = sum(freq_per_data.values())\n",
    "        col_entropy = 0\n",
    "        for count in freq_per_data.values():\n",
    "            p_i = count / total_count  # probability of each unique value\n",
    "            col_entropy += -p_i * np.log2(p_i)  # Entropy formula\n",
    "        if verbose: print(f\"Entropy of column {col}: {col_entropy} bits\")\n",
    "        entropy += col_entropy\n",
    "    return entropy.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_encoding(df_quantized):\n",
    "    encoded_df, huffman_codes = apply_huffman_encode_per_feature(df_quantized)\n",
    "    return encoded_df, huffman_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_decoding(encoded_df, huffman_codes):\n",
    "    decoded_df = apply_huffman_decode_per_feature(encoded_df.iloc[:, 2:-1], huffman_codes)\n",
    "    return decoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct the dataset (without CSI components) and save it in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_data(decoded_df, encoded_df, pca, scaler, data):\n",
    "\n",
    "    df_scaled_reconstructed = pca.inverse_transform(decoded_df.values)\n",
    "    df_reconstructed = scaler.inverse_transform(df_scaled_reconstructed)\n",
    "\n",
    "    \"\"\"\n",
    "    print('Original data shape:', df.shape)\n",
    "    print('Scaled data shape:', reduced_data.shape)\n",
    "    print('PCA components shape:', reduced_df.shape)\n",
    "    print('Reconstructed scaled data shape:', df_scaled_reconstructed.shape)\n",
    "    print('Reconstructed original data shape:', df_reconstructed.shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    df_reconstructed = pd.DataFrame(df_reconstructed, columns=data.columns)\n",
    "    df_reconstructed = pd.concat([encoded_df.iloc[:, [0, 1, -1]], df_reconstructed], axis=1)\n",
    "\n",
    "    if method == Compression_Method.XY:\n",
    "        for j in interestedIndexes:\n",
    "            df_reconstructed[f'CSI{j}'] = df_reconstructed.apply(lambda x: complex_rebuild(x[f'X{j}'], x[f'Y{j}']), axis=1)\n",
    "                \n",
    "            #compute back ampl and phases\n",
    "            df_reconstructed[f'Ampl{j}'] = df_reconstructed[f'CSI{j}'].apply(abs)\n",
    "            df_reconstructed[f'Phase{j}'] = df_reconstructed[f'CSI{j}'].apply(cmath.phase)\n",
    "\n",
    "        df_reconstructed.drop(columns=[f'X{j}' for j in interestedIndexes], inplace=True)\n",
    "        df_reconstructed.drop(columns=[f'Y{j}' for j in interestedIndexes], inplace=True)\n",
    "        \n",
    "    return df_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ampl_comparison(data, reconstruct_data, column, directory): #USELESS\n",
    "\n",
    "    data = data[(data['Timestamp'] >= 0) & (data['Timestamp'] <= 1200)]\n",
    "    reconstruct_data = reconstruct_data[(reconstruct_data['Timestamp'] >= 0) & (data['Timestamp'] <= 1200)]\n",
    "    \n",
    "    plt.figure(figsize=(20, 6))\n",
    "\n",
    "    columns = [f'Ampl{column}', f'Ampl-{column}'] \n",
    "\n",
    "    for i in range(columns.__len__()):\n",
    "        if (columns[i] not in data.columns) or (columns[i] not in reconstruct_data.columns):\n",
    "            print(f\"Column {columns[i]} not found in the data\")\n",
    "            return\n",
    "\n",
    "        # Plot the original data\n",
    "        plt.plot(data['Timestamp'], data[columns[i]], label=f'Original {columns[i]}', color=\"red\" if i == 0 else \"green\")\n",
    "\n",
    "        # Plot the reconstructed data\n",
    "        plt.plot(reconstruct_data['Timestamp'], reconstruct_data[columns[i]], label=f'Reconstructed {columns[i]}', color=\"blue\" if i == 0 else \"orange\")\n",
    "\n",
    "        plt.ylim(0, 3000)\n",
    "    \n",
    "    # Add plot details\n",
    "    plt.title('Amplitude Comparison')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    if (saveCSV): plt.savefig(os.path.join(directory, f'Ampl{column}_Ampl-{column}_comparison.png'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_MuStdAmplPaper(data, reconstructed_data, directory, level):\n",
    "    print(\"Plotting MuStdAmplPaper\")\n",
    "    sub_directory = os.path.join(directory, 'MuStdAmplPaper_Comparison')\n",
    "    os.makedirs(sub_directory, exist_ok=True)\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    \n",
    "    plt.plot(data['Time'], data['MuStdAmplPaper'], label=f'Original MuStdAmplPaper', color=\"blue\" )\n",
    "    plt.plot(reconstructed_data['Time'], reconstructed_data['MuStdAmplPaper'], label=f'Reconstructed MuStdAmplPaper', color=\"green\")\n",
    "    \n",
    "    gt = [min(data['MuStdAmplPaper']) if l == 0 else max(data['MuStdAmplPaper']) for l in reconstructed_data[\"Label\"]]\n",
    "    plt.plot(data['Time'],gt,label=\"Ground-truth\",color=\"r\",ls=\"--\", linewidth=0.5) # per window GT\n",
    "\n",
    "    \n",
    "    # Add plot details\n",
    "    plt.title('Amplitude Comparison')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    if (saveCSV): plt.savefig(os.path.join(sub_directory, f'{level}.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification (partially from another notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredFeaturesPresence = pd.read_csv(\"datasets/filteredFeaturesPresence3s.csv\")\n",
    "def load_comparison():\n",
    "    filteredFeaturesPresence = pd.read_csv(\"datasets/filteredFeaturesPresence3s.csv\")\n",
    "    #apply labeling based on GT\n",
    "    filteredFeaturesPresence[\"Label\"]= filteredFeaturesPresence[\"Time\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds))\n",
    "    orig_auc = classify_presence(filteredFeaturesPresence, plot_roc=False)\n",
    "    return orig_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filtering(df_reconstructed):\n",
    "    reconstructedPresence = df_reconstructed\n",
    "\n",
    "    if method == Compression_Method.AmpPhaseFiltered:\n",
    "        reconstructed_filtered = reconstructedPresence\n",
    "    else:\n",
    "        reconstructed_filtered = filterData2(reconstructedPresence) #removes outliers\n",
    "    reconstructed_filtered.drop(columns=[col for col in reconstructed_filtered.columns if col.startswith('CSI')], inplace=True); #Removes CSI columns\n",
    "\n",
    "    return reconstructed_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_classification(reconstructed_filtered):\n",
    "    #compute features\n",
    "    reconstructed_featured = extractWindowedFeatures(reconstructed_filtered,column_indexes = reconstructed_filtered.columns,w2=w2)\n",
    "    reconstructed_featured[\"Label\"] = reconstructed_featured[\"Time\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds)) #assign the ground-truth to a label\n",
    "\n",
    "    #classify\n",
    "    auc_value = classify_presence(reconstructed_featured,plot_roc=False)\n",
    "    print(auc_value)\n",
    "\n",
    "    return reconstructed_featured, auc_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/presence.csv')\n",
    "data = data_preprocessing(df, method)\n",
    "\n",
    "target = 95\n",
    "num_levels = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "scaled_data, n_components = find_n_components(data, target, base_directory)\n",
    "reduced_df, pca = analyze_PCA(scaled_data, n_components, base_directory)\n",
    "unused_features = analyze_PC(data, pca, n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLoyd-Max Quantization\n",
    "quantized_df = apply_quantization(reduced_df, num_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode-Decode\n",
    "encoded_df, huffman_codes = apply_encoding(quantized_df, df)\n",
    "decoded_df = apply_decoding(encoded_df, huffman_codes)\n",
    "print(\"Original DataFrame equals Decoded DataFrame:\", quantized_df.equals(decoded_df)) #Correctness check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction\n",
    "reconstructed_df = reconstruct_data(decoded_df, encoded_df, pca, scaler, data)\n",
    "load_comparison()\n",
    "\n",
    "reconstructed_df.drop(columns=unused_features, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering and Classification\n",
    "reconstructed_filtered = apply_filtering(reconstructed_df)\n",
    "reconstructed_featured_labeled, auc_value = apply_classification(reconstructed_filtered)\n",
    "plot_MuStdAmplPaper(filteredFeaturesPresence, reconstructed_featured_labeled, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save CSV\n",
    "if(saveCSV):\n",
    "    encoded_df.to_csv(os.path.join(base_directory, 'encodedQuantizedPCAPresence.csv'), index=False)\n",
    "    reconstructed_df.to_csv(os.path.join(base_directory, 'Presence_reconstructed.csv'), index=False)\n",
    "    reconstructed_filtered.to_csv(os.path.join(base_directory, 'filteredPresence_reconstructed.csv'), index=False)\n",
    "    reconstructed_featured_labeled.to_csv(os.path.join(base_directory, 'filteredFeaturesLabeledPresence_reconstructed.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 20)]\n",
    "num_levels = [2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "results = []\n",
    "orig_auc = load_comparison()\n",
    "\n",
    "for n_components in components:\n",
    "    directory = os.path.join(base_directory, f'{n_components}_components')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Read and preprocess data\n",
    "    df = pd.read_csv('datasets/presence.csv')\n",
    "    data = data_preprocessing(df, method)\n",
    "    \n",
    "    # PCA\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    reduced_df, pca = analyze_PCA(scaled_data, n_components, directory)\n",
    "    unused_features = analyze_PC(data, pca, n_components)\n",
    "\n",
    "    for level in num_levels:\n",
    "        sub_directory = os.path.join(directory, f'lvls_{level}')\n",
    "        os.makedirs(sub_directory, exist_ok=True)\n",
    "\n",
    "        # Lloyd-Max Quantization\n",
    "        quantized_df = apply_quantization(reduced_df, level)\n",
    "\n",
    "        # Encode-Decode\n",
    "        encoded_df, huffman_codes = apply_encoding(quantized_df)\n",
    "        encoded_df = pd.concat([df[['Frame_num', 'Timestamp']], encoded_df, df['TimeWindow']], axis=1)\n",
    "        decoded_df = apply_decoding(encoded_df, huffman_codes)\n",
    "        print(\"Original DataFrame equals Decoded DataFrame:\", quantized_df.equals(decoded_df)) #Correctness check\n",
    "\n",
    "        # Reconstruction\n",
    "        reconstructed_df = reconstruct_data(decoded_df, encoded_df, pca, scaler, data)\n",
    "        print(\"original_auc:\",orig_auc)\n",
    "\n",
    "        reconstructed_df.drop(columns=unused_features, inplace=True)\n",
    "\n",
    "        # Filtering and Classification\n",
    "        reconstructed_filtered = apply_filtering(reconstructed_df)\n",
    "        print(f\"----------Results with {n_components} components, num_levels {level} ----------\")\n",
    "        reconstructed_featured_labeled, auc_value = apply_classification(reconstructed_filtered)\n",
    "\n",
    "        results.append({\n",
    "            'n_components': n_components,\n",
    "            'num_levels': level,\n",
    "            'AUC': auc_value,\n",
    "            'Original AUC': orig_auc\n",
    "        })\n",
    "\n",
    "        plot_MuStdAmplPaper(filteredFeaturesPresence, reconstructed_featured_labeled, sub_directory, level)\n",
    "        \n",
    "        if (saveCSV):\n",
    "            encoded_df.to_csv(os.path.join(sub_directory, 'encodedQuantizedPCAPresence.csv'), index=False)\n",
    "            reconstructed_df.to_csv(os.path.join(sub_directory, 'presence_reconstructed.csv'), index=False)\n",
    "            reconstructed_filtered.to_csv(os.path.join(sub_directory, 'filteredPresence_reconstructed.csv'), index=False)\n",
    "            reconstructed_featured_labeled.to_csv(os.path.join(sub_directory, 'filteredFeaturesLabeledPresence_reconstructed.csv'), index=False)\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_file = os.path.join(base_directory, 'classification_results.csv')\n",
    "results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC/BIT comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your targets and num_levels to iterate over\n",
    "components = [i for i in range(1, 20)]\n",
    "num_levels = [2, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "results = []\n",
    "orig_auc = load_comparison()\n",
    "\n",
    "for n_components in components:\n",
    "    directory = os.path.join(base_directory, f'{n_components}_components')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Read and preprocess data\n",
    "    df = pd.read_csv('datasets/presence.csv')\n",
    "    data = data_preprocessing(df, method)\n",
    "    \n",
    "    # PCA\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    reduced_df, pca = analyze_PCA(scaled_data, n_components, directory)\n",
    "\n",
    "    entropyPCA = compute_entropy(reduced_df, verbose=False)\n",
    "    print(f\"Entropy of the PCA data: {entropyPCA:.2f} bits\")\n",
    "    PCA_bits, PCA_win_bits, total_PCA_bits = bits_needed(reduced_df, df, verbose=False)\n",
    "\n",
    "    for level in num_levels:\n",
    "        sub_directory = os.path.join(directory, f'lvls_{level}')\n",
    "        os.makedirs(sub_directory, exist_ok=True)\n",
    "\n",
    "        # Lloyd-Max Quantization\n",
    "        quantized_df = apply_quantization(reduced_df, level)\n",
    "\n",
    "        # Encode-Decode\n",
    "        encoded_df, huffman_codes = apply_encoding(quantized_df)\n",
    "            \n",
    "        entropyENC = compute_entropy(encoded_df, verbose=False)\n",
    "        print(f\"Entropy of the encoded data: {entropyENC:.2f} bits\")\n",
    "        ENC_bits, ENC_win_bits, total_ENC_bits = bits_needed(encoded_df, df, level, verbose=False)\n",
    "        \n",
    "        # Reconstruction\n",
    "        reconstructed_df = pd.read_csv(os.path.join(sub_directory, 'presence_reconstructed.csv'))\n",
    "\n",
    "        entropyREC = compute_entropy(reconstructed_df, verbose=False)\n",
    "        print(f\"Entropy of the reconstructed data: {entropyREC:.2f} bits\")\n",
    "        REC_bits, REC_win_bits, total_REC_bits = bits_needed(reconstructed_df.iloc[:, 3:], df, verbose=False)\n",
    "\n",
    "        print(f\"----------Results with {n_components} components, num_levels {level} ----------\")\n",
    "        reconstructed_filtered = pd.read_csv(os.path.join(sub_directory, 'filteredPresence_reconstructed.csv'))\n",
    "        reconstructed_featured, auc_value = apply_classification(reconstructed_filtered)\n",
    "\n",
    "        results.append({\n",
    "            'n_components': n_components,\n",
    "            'num_levels': level,\n",
    "            'AUC': auc_value,\n",
    "            'Original AUC': orig_auc,\n",
    "            'PCA_bits': PCA_bits,\n",
    "            'ENC_bits': ENC_bits,\n",
    "            'REC_bits': REC_bits,\n",
    "            'PCA_win_bits': PCA_win_bits,\n",
    "            'ENC_win_bits': ENC_win_bits,\n",
    "            'REC_win_bits': REC_win_bits,\n",
    "            'total_PCA_bits': total_PCA_bits,\n",
    "            'total_ENC_bits': total_ENC_bits,\n",
    "            'total_REC_bits': total_REC_bits,\n",
    "            'entropyPCA': entropyPCA,\n",
    "            'entropyENC': entropyENC,\n",
    "            'entropyREC': entropyREC\n",
    "        })\n",
    "\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_file = os.path.join('./results_csv', 'new_auc_bit_comparison.csv')\n",
    "results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_bit = pd.read_csv(os.path.join('./results_csv', 'new_auc_bit_comparison.csv'))\n",
    "#components = [1,2,3,4,5,6,8,10,12,14,16,18]\n",
    "components = [1,2,3,4,5,8,10,12,14,18]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for n_components in auc_bit['n_components'].unique():\n",
    "    if(n_components not in components): continue\n",
    "    target_data = auc_bit[auc_bit['n_components'] == n_components]\n",
    "    plt.plot(target_data['ENC_win_bits'], target_data['AUC'], marker='o', linestyle='--', label=f'{n_components} components')\n",
    "#plt.ylim(0.7)\n",
    "plt.title('AUC and Bits Comparison')\n",
    "plt.xlabel('Avg bits per window')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('./results_graphs', 'auc_bit_comparison[BxW][filtered].png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_bit = pd.read_csv(os.path.join('./results_csv', 'new_auc_bit_comparison.csv'))\n",
    "#components = [i for i in range(1, 20)]\n",
    "#components = [1,2,3,4,5,6,8,10,12,14,16,18]\n",
    "components = [1,2,3,4,5,8,10,12,14]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for n_components in auc_bit['n_components'].unique():\n",
    "    if (n_components not in components): continue\n",
    "    target_data = auc_bit[auc_bit['n_components'] == n_components]\n",
    "    plt.plot(target_data['ENC_bits'], target_data['AUC'], marker='o', linestyle='--', label=f'{n_components} components')\n",
    "#plt.ylim(0.7)\n",
    "plt.title('AUC with quantized data and PCA applied')\n",
    "plt.xlabel('bits per symbol')\n",
    "#plt.xticks(np.arange(0, 110, 10))\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join('./results_graphs', 'auc_bit_comparison[BxS][filtered].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [i for i in range(1, 20)]\n",
    "\n",
    "results = []\n",
    "orig_auc = load_comparison()\n",
    "\n",
    "for n_components in components:\n",
    "    directory = os.path.join('./results/PCA_ONLY', f'{n_components}_components')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Read and preprocess data\n",
    "    df = pd.read_csv('datasets/presence.csv')\n",
    "    data = data_preprocessing(df, method)\n",
    "    \n",
    "    # PCA\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    reduced_df, pca = analyze_PCA(scaled_data, n_components, directory)\n",
    "    unused_features = analyze_PC(data, pca, n_components)\n",
    "\n",
    "    reconstructed_scaled_df = pca.inverse_transform(reduced_df.values)\n",
    "    reconstructed_df = scaler.inverse_transform(reconstructed_scaled_df)\n",
    "    reconstructed_df = pd.DataFrame(reconstructed_df, columns=data.columns)\n",
    "    reconstructed_df = pd.concat([df[['Frame_num', 'Timestamp']], reconstructed_df, df['TimeWindow']], axis=1)\n",
    "    reconstructed_df.drop(columns=unused_features, inplace=True)\n",
    "\n",
    "    # Filtering and Classification\n",
    "    reconstructed_filtered = apply_filtering(reconstructed_df)\n",
    "    print(f\"--------------- Results with {n_components} ---------------\")\n",
    "    reconstructed_featured, auc_value = apply_classification(reconstructed_filtered)\n",
    "\n",
    "    results.append({\n",
    "        'n_components': n_components,\n",
    "        'AUC': auc_value,\n",
    "        'Original AUC': orig_auc\n",
    "    })\n",
    "    \n",
    "    if (saveCSV):\n",
    "        reconstructed_df.to_csv(os.path.join(directory, 'presence_reconstructed.csv'), index=False)\n",
    "        reconstructed_filtered.to_csv(os.path.join(directory, 'filteredPresence_reconstructed.csv'), index=False)\n",
    "        reconstructed_featured.to_csv(os.path.join(directory, 'filteredFeaturesLabeledPresence_reconstructed.csv'), index=False)\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_file = os.path.join('./results_csv/PCA_ONLY', 'classification_results.csv')\n",
    "os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
    "results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define your targets and num_levels to iterate over\n",
    "components = [i for i in range(1, 20)]\n",
    "\n",
    "results = []\n",
    "orig_auc = load_comparison()\n",
    "\n",
    "for n_components in components:\n",
    "    directory = os.path.join('./results/PCA_ONLY', f'{n_components}_components')\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Read and preprocess data\n",
    "    df = pd.read_csv('datasets/presence.csv')\n",
    "    data = data_preprocessing(df, method)\n",
    "    \n",
    "    # PCA\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    reduced_df, pca = analyze_PCA(scaled_data, n_components, directory)\n",
    "\n",
    "    entropyPCA = compute_entropy(reduced_df, verbose=False)\n",
    "    print(f\"Entropy of the PCA data: {entropyPCA:.2f} bits\")\n",
    "    PCA_bits, PCA_win_bits, total_PCA_bits = bits_needed(reduced_df.round(5), df, verbose=False)\n",
    "\n",
    "    # Reconstruction\n",
    "    reconstructed_df = pd.read_csv(os.path.join(directory, 'presence_reconstructed.csv'))\n",
    "\n",
    "    entropyREC = compute_entropy(reconstructed_df.iloc[:, 3:], verbose=False)\n",
    "    print(f\"Entropy of the reconstructed data: {entropyREC:.2f} bits\")\n",
    "    REC_bits, REC_win_bits, total_REC_bits = bits_needed(reconstructed_df.iloc[:, 3:], df, verbose=False)\n",
    "\n",
    "    print(f\"----------Results with {n_components} components ----------\")\n",
    "    reconstructed_filtered = pd.read_csv(os.path.join(directory, 'filteredPresence_reconstructed.csv'))\n",
    "    reconstructed_featured, auc_value = apply_classification(reconstructed_filtered)\n",
    "\n",
    "    results.append({\n",
    "        'n_components': n_components,\n",
    "        'num_levels': level,\n",
    "        'AUC': auc_value,\n",
    "        'Original AUC': orig_auc,\n",
    "        'PCA_bits': PCA_bits,\n",
    "        'REC_bits': REC_bits,\n",
    "        'PCA_win_bits': PCA_win_bits,\n",
    "        'REC_win_bits': REC_win_bits,\n",
    "        'total_PCA_bits': total_PCA_bits,\n",
    "        'total_REC_bits': total_REC_bits,\n",
    "        'entropyPCA': entropyPCA,\n",
    "        'entropyREC': entropyREC\n",
    "    })\n",
    "\n",
    "# Convert results list to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Save the results DataFrame to a CSV file\n",
    "results_file = os.path.join('./results_csv/PCA_ONLY', 'auc_bit_comparison.csv')\n",
    "os.makedirs(os.path.dirname(results_file), exist_ok=True)\n",
    "results_df.to_csv(results_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_bit = pd.read_csv(os.path.join('./results_csv/PCA_ONLY', 'auc_bit_comparison.csv'))\n",
    "components = [i for i in range(1, 20)]\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(auc_bit['PCA_win_bits'], auc_bit['AUC'], marker='o', linestyle='--')\n",
    "plt.ylim(0.85, 1.01)\n",
    "plt.title('AUC with PCA applied')\n",
    "plt.xlabel('Avg bits per window')\n",
    "plt.xticks(np.arange(0, 150, 10))\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "#plt.savefig(os.path.join('./results_graphs', 'auc_bit_comparison_PCA-Only[BxW].png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 57\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/presence.csv\")\n",
    "data = data_preprocessing(data, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1 #seconds\n",
    "labels = pd.DataFrame()\n",
    "#print(\"TimeWindow:\", df[\"TimeWindow\"].unique())\n",
    "data[\"TimeWindow\"] = np.floor(data[\"Timestamp\"] / time_window) * time_window\n",
    "labels[\"TimeWindow\"] = data[\"TimeWindow\"].unique()\n",
    "labels[\"Label\"] = labels[\"TimeWindow\"].apply(lambda x: getGT(x,lower_bounds,upper_bounds)) #assign the ground-truth to a label\n",
    "\n",
    "df = data.copy()\n",
    "df.drop(columns=[\"Timestamp\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'timewindow_counts = df[\\'TimeWindow\\'].value_counts(sort=False)\\n\\nprint(\"average count:\",timewindow_counts.mean())\\nprint(\"max count:\",timewindow_counts.max())\\nprint(\"min count:\",timewindow_counts.min())\\n\\ncounter = [x for x in timewindow_counts if x < 20]\\nprint(\"\\nnumber of windows with less than 20 samples:\",len(counter))\\nprint(sorted(counter))\\n\\ncounter = [x for x in timewindow_counts if x >= 20 and x < 46]\\nprint(\"\\nnumber of windows between 20 and 46:\",len(counter))\\nprint(sorted(counter))\\n\\ncounter = [x for x in timewindow_counts if x >= 46 and x < 100]\\nprint(\"\\nnumber of windows between 46 and 100:\",len(counter))\\nprint(sorted(counter))\\n\\ncounter = [x for x in timewindow_counts if x >= 100]\\nprint(\"\\nnumber of windows with more than 100 samples:\",len(counter))\\nprint(sorted(counter))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"timewindow_counts = df['TimeWindow'].value_counts(sort=False)\n",
    "\n",
    "print(\"average count:\",timewindow_counts.mean())\n",
    "print(\"max count:\",timewindow_counts.max())\n",
    "print(\"min count:\",timewindow_counts.min())\n",
    "\n",
    "counter = [x for x in timewindow_counts if x < 20]\n",
    "print(\"\\nnumber of windows with less than 20 samples:\",len(counter))\n",
    "print(sorted(counter))\n",
    "\n",
    "counter = [x for x in timewindow_counts if x >= 20 and x < 46]\n",
    "print(\"\\nnumber of windows between 20 and 46:\",len(counter))\n",
    "print(sorted(counter))\n",
    "\n",
    "counter = [x for x in timewindow_counts if x >= 46 and x < 100]\n",
    "print(\"\\nnumber of windows between 46 and 100:\",len(counter))\n",
    "print(sorted(counter))\n",
    "\n",
    "counter = [x for x in timewindow_counts if x >= 100]\n",
    "print(\"\\nnumber of windows with more than 100 samples:\",len(counter))\n",
    "print(sorted(counter))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time windows to drop: 30\n"
     ]
    }
   ],
   "source": [
    "# Window size\n",
    "window_size = 20 # for 1 second\n",
    "#window_size = 75 # for 3 second\n",
    "timewindow_counts = df['TimeWindow'].value_counts(sort=False) # Count the number of samples per time window\n",
    "\n",
    "#Drop time windows with less than \"Window size\" samples\n",
    "timewindow_counts = pd.DataFrame(timewindow_counts)\n",
    "indexes_below_threshold = timewindow_counts[timewindow_counts['count'] < window_size].index.to_numpy()\n",
    "print(\"Number of time windows to drop:\", len(indexes_below_threshold))\n",
    "df = df[~df['TimeWindow'].isin(indexes_below_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sliding windows created: 90326\n",
      "Number of labels created: 1445216\n"
     ]
    }
   ],
   "source": [
    "sliding_windows = []\n",
    "labels = []\n",
    "\n",
    "for time_window in df['TimeWindow'].unique():\n",
    "    window_data = df[df['TimeWindow'] == time_window]\n",
    "    for start_idx in range(len(window_data) - window_size + 1):\n",
    "        window = window_data[start_idx : start_idx + window_size]\n",
    "        labels.extend(window['TimeWindow'].apply(lambda x: getGT(x, lower_bounds, upper_bounds)).tolist())\n",
    "\n",
    "# Create more sliding windows for each TimeWindow\n",
    "for time_window in df['TimeWindow'].unique():\n",
    "    window_data = df[df['TimeWindow'] == time_window]\n",
    "    window_data.drop(columns=['TimeWindow'], inplace=True)\n",
    "    window_data = np.array(window_data)\n",
    "    for start_idx in range(len(window_data) - window_size + 1):\n",
    "        window = window_data[start_idx : start_idx + window_size]\n",
    "        sliding_windows.append(window)\n",
    "\n",
    "\n",
    "print(f\"Number of sliding windows created: {len(sliding_windows)}\")\n",
    "print(f\"Number of labels created: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n",
    "sliding_windows = tf.convert_to_tensor(sliding_windows, dtype=tf.float32)\n",
    "sliding_windows = tf.reshape(sliding_windows, (-1, 56))\n",
    "\n",
    "print(f\"Sliding windows shape: {sliding_windows.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\"\"\"\n",
    "\n",
    "labels = np.array(labels)\n",
    "labels = labels.reshape(len(sliding_windows), window_size) # for 1 second\n",
    "#labels = labels.reshape(62380, 75) # for 3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90326\n",
      "(16, 56)\n",
      "90326\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(sliding_windows))\n",
    "print(sliding_windows[0].shape)\n",
    "print(len(labels))\n",
    "print(len(labels[0]))\n",
    "#print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/90326\n",
      "5000/90326\n",
      "10000/90326\n",
      "15000/90326\n",
      "20000/90326\n",
      "25000/90326\n",
      "30000/90326\n",
      "35000/90326\n",
      "40000/90326\n",
      "45000/90326\n",
      "50000/90326\n",
      "55000/90326\n",
      "60000/90326\n",
      "65000/90326\n",
      "70000/90326\n",
      "75000/90326\n",
      "80000/90326\n",
      "85000/90326\n",
      "90000/90326\n",
      "0/90326\n",
      "10000/90326\n",
      "20000/90326\n",
      "30000/90326\n",
      "40000/90326\n",
      "50000/90326\n",
      "60000/90326\n",
      "70000/90326\n",
      "80000/90326\n",
      "90000/90326\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "# Process sliding windows in train/test\n",
    "for i, item in enumerate(sliding_windows):\n",
    "    if i % 5000 == 0: print(f\"{i}/{len(sliding_windows)}\")\n",
    "    \n",
    "    train, test = train_test_split(item, test_size=0.2, random_state=42, shuffle=False, stratify=None)\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "\n",
    "train_labels_list = []\n",
    "test_labels_list = []\n",
    "\n",
    "# Process sliding windows in train/test\n",
    "for i, item in enumerate(labels):\n",
    "    if i % 10000 == 0: print(f\"{i}/{len(labels)}\")\n",
    "    \n",
    "    train_labels, test_labels = train_test_split(item, test_size=0.2, random_state=42, shuffle=False, stratify=None)\n",
    "    train_labels_list.append(train_labels)\n",
    "    test_labels_list.append(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train_data size: (90326, 12, 56)\n",
      "Final test_data size: (90326, 4, 56)\n",
      "---\n",
      "Final train_data size: (1083912, 56)\n",
      "Final test_data size: (361304, 56)\n",
      "Final train_labels size: (1083912,)\n",
      "Final test_labels size: (361304,)\n",
      "Final train_indices size: (1083912,)\n",
      "Final test_indices size: (361304,)\n"
     ]
    }
   ],
   "source": [
    "#convert in tensor\n",
    "train_data_tf = tf.convert_to_tensor(train_list, dtype=tf.float32)\n",
    "test_data_tf = tf.convert_to_tensor(test_list, dtype=tf.float32)\n",
    "\n",
    "print(f\"Final train_data size: {train_data_tf.shape}\")\n",
    "print(f\"Final test_data size: {test_data_tf.shape}\")\n",
    "print(\"---\")\n",
    "\n",
    "#reshape\n",
    "train_data_tf = tf.reshape(train_data_tf, (-1, 56))\n",
    "test_data_tf = tf.reshape(test_data_tf, (-1, 56))\n",
    "\n",
    "#normalize\n",
    "train_data_tf = tf.math.divide(train_data_tf, tf.math.reduce_max(train_data_tf, axis=(0, 1)))\n",
    "test_data_tf = tf.math.divide(test_data_tf, tf.math.reduce_max(test_data_tf, axis=(0, 1)))\n",
    "\n",
    "train_indices_tf = tf.convert_to_tensor(tf.range(0, train_data_tf.shape[0], dtype=tf.int32))\n",
    "test_indices_tf = tf.convert_to_tensor(tf.range(0, test_data_tf.shape[0], dtype=tf.int32))\n",
    "\n",
    "train_labels_tf = tf.convert_to_tensor(train_labels_list, dtype=tf.int32)\n",
    "test_labels_tf = tf.convert_to_tensor(test_labels_list, dtype=tf.int32)\n",
    "\n",
    "train_labels_tf = tf.reshape(train_labels_tf, (-1))\n",
    "test_labels_tf = tf.reshape(test_labels_tf, (-1))\n",
    "\n",
    "print(f\"Final train_data size: {train_data_tf.shape}\")\n",
    "print(f\"Final test_data size: {test_data_tf.shape}\")\n",
    "print(f\"Final train_labels size: {train_labels_tf.shape}\")\n",
    "print(f\"Final test_labels size: {test_labels_tf.shape}\")\n",
    "print(f\"Final train_indices size: {train_indices_tf.shape}\")\n",
    "print(f\"Final test_indices size: {test_indices_tf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CsiData(train_data_tf, train_labels_tf, train_indices_tf, batch_size=4, window_size=window_size)\n",
    "test_data = CsiData(test_data_tf, test_labels_tf, test_indices_tf, batch_size=4, window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CsiData'>\n",
      "(1083912, 56)\n",
      "(361304, 56)\n",
      "(<tf.Tensor: shape=(4, 16, 56, 1), dtype=float32, numpy=\n",
      "array([[[[0.03037027],\n",
      "         [0.04819191],\n",
      "         [0.05624213],\n",
      "         ...,\n",
      "         [0.04717913],\n",
      "         [0.04208326],\n",
      "         [0.02829105]],\n",
      "\n",
      "        [[0.03160916],\n",
      "         [0.04786843],\n",
      "         [0.05662412],\n",
      "         ...,\n",
      "         [0.04656804],\n",
      "         [0.04147873],\n",
      "         [0.02904194]],\n",
      "\n",
      "        [[0.03126083],\n",
      "         [0.04755161],\n",
      "         [0.05686344],\n",
      "         ...,\n",
      "         [0.04601929],\n",
      "         [0.04127242],\n",
      "         [0.02830232]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.03126083],\n",
      "         [0.04755161],\n",
      "         [0.05686344],\n",
      "         ...,\n",
      "         [0.04601929],\n",
      "         [0.04127242],\n",
      "         [0.02830232]],\n",
      "\n",
      "        [[0.03046657],\n",
      "         [0.04785082],\n",
      "         [0.05682901],\n",
      "         ...,\n",
      "         [0.04711064],\n",
      "         [0.04123797],\n",
      "         [0.02829564]],\n",
      "\n",
      "        [[0.03031837],\n",
      "         [0.04823853],\n",
      "         [0.05517177],\n",
      "         ...,\n",
      "         [0.04617841],\n",
      "         [0.04045137],\n",
      "         [0.02777886]]],\n",
      "\n",
      "\n",
      "       [[[0.03160916],\n",
      "         [0.04786843],\n",
      "         [0.05662412],\n",
      "         ...,\n",
      "         [0.04656804],\n",
      "         [0.04147873],\n",
      "         [0.02904194]],\n",
      "\n",
      "        [[0.03126083],\n",
      "         [0.04755161],\n",
      "         [0.05686344],\n",
      "         ...,\n",
      "         [0.04601929],\n",
      "         [0.04127242],\n",
      "         [0.02830232]],\n",
      "\n",
      "        [[0.03046657],\n",
      "         [0.04785082],\n",
      "         [0.05682901],\n",
      "         ...,\n",
      "         [0.04711064],\n",
      "         [0.04123797],\n",
      "         [0.02829564]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.03046657],\n",
      "         [0.04785082],\n",
      "         [0.05682901],\n",
      "         ...,\n",
      "         [0.04711064],\n",
      "         [0.04123797],\n",
      "         [0.02829564]],\n",
      "\n",
      "        [[0.03031837],\n",
      "         [0.04823853],\n",
      "         [0.05517177],\n",
      "         ...,\n",
      "         [0.04617841],\n",
      "         [0.04045137],\n",
      "         [0.02777886]],\n",
      "\n",
      "        [[0.03166889],\n",
      "         [0.04833809],\n",
      "         [0.05590875],\n",
      "         ...,\n",
      "         [0.04724951],\n",
      "         [0.04175235],\n",
      "         [0.02942605]]],\n",
      "\n",
      "\n",
      "       [[[0.03126083],\n",
      "         [0.04755161],\n",
      "         [0.05686344],\n",
      "         ...,\n",
      "         [0.04601929],\n",
      "         [0.04127242],\n",
      "         [0.02830232]],\n",
      "\n",
      "        [[0.03046657],\n",
      "         [0.04785082],\n",
      "         [0.05682901],\n",
      "         ...,\n",
      "         [0.04711064],\n",
      "         [0.04123797],\n",
      "         [0.02829564]],\n",
      "\n",
      "        [[0.03031837],\n",
      "         [0.04823853],\n",
      "         [0.05517177],\n",
      "         ...,\n",
      "         [0.04617841],\n",
      "         [0.04045137],\n",
      "         [0.02777886]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.03031837],\n",
      "         [0.04823853],\n",
      "         [0.05517177],\n",
      "         ...,\n",
      "         [0.04617841],\n",
      "         [0.04045137],\n",
      "         [0.02777886]],\n",
      "\n",
      "        [[0.03166889],\n",
      "         [0.04833809],\n",
      "         [0.05590875],\n",
      "         ...,\n",
      "         [0.04724951],\n",
      "         [0.04175235],\n",
      "         [0.02942605]],\n",
      "\n",
      "        [[0.03101146],\n",
      "         [0.04833149],\n",
      "         [0.05673173],\n",
      "         ...,\n",
      "         [0.04704758],\n",
      "         [0.04223502],\n",
      "         [0.02845549]]],\n",
      "\n",
      "\n",
      "       [[[0.03046657],\n",
      "         [0.04785082],\n",
      "         [0.05682901],\n",
      "         ...,\n",
      "         [0.04711064],\n",
      "         [0.04123797],\n",
      "         [0.02829564]],\n",
      "\n",
      "        [[0.03031837],\n",
      "         [0.04823853],\n",
      "         [0.05517177],\n",
      "         ...,\n",
      "         [0.04617841],\n",
      "         [0.04045137],\n",
      "         [0.02777886]],\n",
      "\n",
      "        [[0.03166889],\n",
      "         [0.04833809],\n",
      "         [0.05590875],\n",
      "         ...,\n",
      "         [0.04724951],\n",
      "         [0.04175235],\n",
      "         [0.02942605]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.03166889],\n",
      "         [0.04833809],\n",
      "         [0.05590875],\n",
      "         ...,\n",
      "         [0.04724951],\n",
      "         [0.04175235],\n",
      "         [0.02942605]],\n",
      "\n",
      "        [[0.03101146],\n",
      "         [0.04833149],\n",
      "         [0.05673173],\n",
      "         ...,\n",
      "         [0.04704758],\n",
      "         [0.04223502],\n",
      "         [0.02845549]],\n",
      "\n",
      "        [[0.03124634],\n",
      "         [0.04785733],\n",
      "         [0.05625536],\n",
      "         ...,\n",
      "         [0.04667242],\n",
      "         [0.04131857],\n",
      "         [0.02921617]]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 1), dtype=int32, numpy=\n",
      "array([[[0]],\n",
      "\n",
      "       [[0]],\n",
      "\n",
      "       [[0]],\n",
      "\n",
      "       [[0]]])>)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(train_data.csi.shape)\n",
    "print(test_data.csi.shape)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 16, 56, 1)]          0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 8, 28, 32)            320       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 4, 14, 64)            18496     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 2, 7, 128)            73856     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 1792)                 0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 32)                   57376     ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " z_mean (Dense)              (None, 1)                    33        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " z_log_var (Dense)           (None, 1)                    33        ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape (TFOpLa  (2,)                         0         ['z_mean[0][0]']              \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOp  (2,)                         0         ['z_mean[0][0]']              \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLamb  (None, 1)                    0         ['z_log_var[0][0]']           \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  ()                           0         ['tf.compat.v1.shape[0][0]']  \n",
      " SlicingOpLambda)                                                                                 \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  ()                           0         ['tf.compat.v1.shape_1[0][0]']\n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.math.exp (TFOpLambda)    (None, 1)                    0         ['tf.math.multiply[0][0]']    \n",
      "                                                                                                  \n",
      " tf.random.normal (TFOpLamb  (None, 1)                    0         ['tf.__operators__.getitem[0][\n",
      " da)                                                                0]',                          \n",
      "                                                                     'tf.__operators__.getitem_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLa  (None, 1)                    0         ['tf.math.exp[0][0]',         \n",
      " mbda)                                                               'tf.random.normal[0][0]']    \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 1)                    0         ['z_mean[0][0]',              \n",
      " Lambda)                                                             'tf.math.multiply_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 150114 (586.38 KB)\n",
      "Trainable params: 150114 (586.38 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3584)              7168      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTr  (None, 8, 14, 128)        147584    \n",
      " anspose)                                                        \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2D  (None, 16, 28, 64)        73792     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2D  (None, 32, 56, 32)        18464     \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2D  (None, 32, 56, 1)         289       \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 247297 (966.00 KB)\n",
      "Trainable params: 247297 (966.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node logistic_loss/mul defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_15436\\1895457923.py\", line 4, in <module>\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_15436\\2335218863.py\", line 28, in train_step\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 2532, in binary_crossentropy\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py\", line 5824, in binary_crossentropy\n\nIncompatible shapes: [4,16,56,1] vs. [4,32,56,1]\n\t [[{{node logistic_loss/mul}}]] [Op:__inference_train_function_3574]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf_keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam())\n\u001b[0;32m      3\u001b[0m vae\u001b[38;5;241m.\u001b[39msave_weights(checkpoint_path\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_logger_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m vae\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfolder_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_weights_vae\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node logistic_loss/mul defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\events.py\", line 88, in _run\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n\n  File \"C:\\Users\\Paolo\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n\n  File \"C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_15436\\1895457923.py\", line 4, in <module>\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py\", line 65, in error_handler\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1804, in fit\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1398, in train_function\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1381, in step_function\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py\", line 1370, in run_step\n\n  File \"C:\\Users\\Paolo\\AppData\\Local\\Temp\\ipykernel_15436\\2335218863.py\", line 28, in train_step\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py\", line 2532, in binary_crossentropy\n\n  File \"c:\\Users\\Paolo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py\", line 5824, in binary_crossentropy\n\nIncompatible shapes: [4,16,56,1] vs. [4,32,56,1]\n\t [[{{node logistic_loss/mul}}]] [Op:__inference_train_function_3574]"
     ]
    }
   ],
   "source": [
    "vae = VAE()\n",
    "vae.compile(optimizer=tf_keras.optimizers.Adam())\n",
    "vae.save_weights(checkpoint_path.format(epoch=0))\n",
    "vae.fit(train_data, epochs=20, shuffle=True, callbacks=[checkpoint_cb, csv_logger_cb])\n",
    "vae.save_weights(f'./{folder_name}/train_weights_vae')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
